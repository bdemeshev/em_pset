\documentclass[pdftex,11pt,openany]{book}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

%\documentclass[11pt,openany]{book}

% команды АК.
\newcommand{\calL}{\mathcal{L}}
\newcommand{\bs}[1]{\boldsymbol{#1}} 
\newcommand{\hypo}{\mathcal{H}} 
\newcommand{\simhypo}{\ensuremath{\mathrel{\stackrel{\hypo_0}{\sim}}}}

\input{title_bor_utf8_knitr_e}  % no embedfile in _e


% чисто эконометрические сокращения:
\input{emetrix_preamble}


% стандартизация
% эпсилон во временных рядах --- белый шум, а в остальных сюжетах --- остатки, подумать
% транспонирование --- штрих

% задачи типа "воспроизведите тест такой-то ручками в R" -> в тему согласно тесту, а не в доп. задачи по программированию

% идеи задач:
% * Задача на корреляционную матрицу по реальным данным require(quantmod)
% Задача про суеверную Мырли. Можно ли там что-то про se сказать?
% теорему FWL в массы!
% симуляционные задачи на ошибки 1, 2 рода, мощность
% E_t(X) или с указанием сигма-алгебры

% по явному виду процесса определить подходит ли он
% в заданное стохастическое разностное уравнение


% выложить преамбулу


% перегнанные банки:
% 1 ---20

% осталось:
% задача 12, компьютерное про пи (алгоритмы вычисления)
% распределить проверить наличие программистко-сюжетных упражнений


% для отдельной упаковки решений
\ifdef{\JustPlot}{}{\input{prob_and_sol_2chap_e}} % secsolution patch instead of chapter master counter


\title{Эконометрика \\ {\small с Монте-Карло и эконометрессами} \\ в задачах и упражнениях}
\author{Дмитрий Борзых, Борис Демешев}
\date{\today}

\makeindex % команда для создания предметного указателя
\bibliographystyle{plain} % стиль оформления ссылок
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\setcounter{page}{3}

%\maketitle % печатаем заголовок
\tableofcontents




\vspace{30pt}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(}\hlstr{"knitr"}\hlstd{)}
\hlkwd{library}\hlstd{(}\hlstr{"tikzsetup"}\hlstd{)}
\hlkwd{library}\hlstd{(}\hlstr{"tikzDevice"}\hlstd{)}
\hlkwd{tikzsetup}\hlstd{()}

\hlkwd{library}\hlstd{(}\hlstr{"ggplot2"}\hlstd{)}
\hlkwd{library}\hlstd{(}\hlstr{"Hmisc"}\hlstd{)}
\hlkwd{library}\hlstd{(}\hlstr{"lmtest"}\hlstd{)}
\hlkwd{library}\hlstd{(}\hlstr{"apsrtable"}\hlstd{)}
\hlkwd{library}\hlstd{(}\hlstr{"xtable"}\hlstd{)}
\hlkwd{library}\hlstd{(}\hlstr{"MASS"}\hlstd{)}
\hlkwd{library}\hlstd{(}\hlstr{"car"}\hlstd{)}
\hlkwd{library}\hlstd{(}\hlstr{"texreg"}\hlstd{)}
\hlkwd{library}\hlstd{(}\hlstr{"memisc"}\hlstd{)}
\hlkwd{library}\hlstd{(}\hlstr{"sandwich"}\hlstd{)}

\hlkwd{library}\hlstd{(}\hlstr{"econru"}\hlstd{)}

\hlkwd{theme_set}\hlstd{(}\hlkwd{theme_bw}\hlstd{())}

\hlkwd{load}\hlstd{(}\hlstr{'pset_data.Rdata'}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}



\chapter{Решения и ответы к избранным задачам}
\chaptermark{Избранные решения}

\solutiononly


% МНК


\chapter{МНК без матриц и вероятностей}

\begin{problem}
 Верно ли, что для любых векторов $a = (a_1,\dots,a_n)$ и $b = (b_1,\dots,b_n)$ справедливы следующие равенства?
\begin{enumerate}
\item $\sum_{i=1}^n {(a_i-\bar a)} = 0$
\item $\sum_{i=1}^n {(a_i-\bar a)^2} = \sum_{i=1}^n {(a_i-\bar a)a_i}$
\item $\sum_{i=1}^n {(a_i-\bar a)(b_i-\bar b)} = \sum_{i=1}^n {(a_i-\bar a)b_i}$
\item $\sum_{i=1}^n {(a_i-\bar a)(b_i-\bar b)} = \sum_{i=1}^n {a_i b_i}$
\end{enumerate}
\end{problem}

\begin{solution}
 да, да, да, нет
\end{solution}




\begin{problem}
 При помощи метода наименьших квадратов найдите оценку неизвестного параметра $\theta$ в следующих моделях:

\begin{enumerate}
\item $y_i = \theta + \theta x_i + \varepsilon_i$
\item $y_i = \theta - \theta x_i + \e_i$
\item $\text{ln} y_i = \theta + \text{ln} x_i + \e_i$
\item $y_i = \theta + x_i + \e_i$
\item $y_i = 1 + \theta x_i + \e_i$
\item $y_i = \theta / x_i + \e_i$
\item $y_i = \theta x_{i1} + (1-\theta)x_{i2}+\e_i$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
 Покажите, что для моделей $y_i= \alpha + \beta x_i + \e_i$, $z_i = \gamma + \delta x_i + \upsilon_i$ и $y_i + z_i = \mu + \lambda x_i + \xi_i$ МНК-оценки связаны соотношениями $\hat{\mu}=\hat{\alpha}+\hat{\gamma}$ и $\hat{\lambda}=\hat{\beta} + \hat{\delta}$.
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
 Найдите МНК-оценки параметров $\alpha$ и $\beta$ в модели $y_i = \alpha + \beta y_i + \e_i$.
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
 Рассмотрите модели $y_i = \alpha + \beta (y_i + z_i) + \e_i$, $z_i = \gamma + \delta(y_i+z_i) + \e_i$. 
\begin{enumerate}
\item Как связаны между собой $\hat{\alpha}$ и $\hat{\gamma}$?
\item  Как связаны между собой $\hat{\beta}$ и $\hat{\delta}$?
\end{enumerate} 
\end{problem}

\begin{solution}
$\hat{\alpha} + \hat{\gamma} = 0$ и $\hat{\beta} + \hat{\delta} = 1$
\end{solution}



\begin{problem}
 Как связаны МНК-оценки параметров $\alpha, \beta$ и $\gamma, \delta$ в моделях $y_i = \alpha + \beta x_i + \e_i$ и $z_i = \gamma + \delta x_i + \upsilon_i$, если $z_i = 2 y_i$.
\end{problem}

\begin{solution}
\end{solution} 

\begin{problem}
 Для модели $y_i = \beta_1 x_{i1} + \beta_2 x_{i2} + \e_i$ решите условную задачу о наименьших квадратах: $Q(\beta_1, \beta_2) := \sum_{i=1}^n (y_i - \beta_1 x_{i1} - \beta_2 x_{i2})^2 \rightarrow \underset{\beta_1 + \beta_2 = 1}{\min}$
\end{problem}

\begin{solution}
\end{solution} 
 


\begin{problem}
Даны $n$ пар чисел: $(x_1, y_1)$, \ldots, $(x_n,y_n)$. Мы прогнозируем $y_i$ по формуле $\hy_i=\hb x_i$. Найдите $\hb$ методом наименьших квадратов. 
\end{problem} 

\begin{solution}
$\hb=\sum x_i y_i/\sum x_i^2$
\end{solution}

\begin{problem}
Даны $n$ чисел: $y_1$, \ldots, $y_n$. Мы прогнозируем $y_i$ по формуле $\hy_i=\hb$. Найдите $\hb$ методом наименьших квадратов. 
\end{problem}
\begin{solution}
$\hb=\bar{y}$
\end{solution}

\begin{problem}
Даны $n$ пар чисел: $(x_1, y_1)$, \ldots, $(x_n,y_n)$. Мы прогнозируем $y_i$ по формуле $\hy_i=\hb_1+\hb_2 x_i$. Найдите $\hb_1$ и $\hb_2$ методом наименьших квадратов. 
\end{problem}
\begin{solution}
$\hb_2=\sum (x_i-\bar{x})(y_i-\bar{y})/\sum(x_i-\bar{x})^2$, $\hb_1=\bar{y}-\hb_2\bar{x}$
\end{solution}

\begin{problem}
Даны $n$ пар чисел: $(x_1, y_1)$, \ldots, $(x_n,y_n)$. Мы прогнозируем $y_i$ по формуле $\hy_i=1+\hb x_i$. Найдите $\hb$ методом наименьших квадратов. 
\end{problem}
\begin{solution}
$\hb=\sum x_i (y_i-1)/\sum x_i^2$
\end{solution}

\begin{problem}
 Перед нами два золотых слитка и весы, производящие взвешивания с ошибками. Взвесив первый слиток, мы получили результат $300$ грамм, взвесив второй слиток --- $200$ грамм, взвесив оба слитка --- $400$ грамм. Оцените вес каждого слитка методом наименьших квадратов.
 \end{problem}
\begin{solution}
 $(300-\hb_1)^2+(200-\hb_2)^2+(400-\hb_1-\hb_2)^2\to\min$ 
\end{solution}


\begin{problem}
 Аня и Настя утверждают, что лектор опоздал на 10 минут. Таня считает, что лектор опоздал на 3 минуты. С помощью мнк оцените на сколько опоздал лектор. 
\end{problem}
\begin{solution}
 $2\cdot (10-\hb)^2+(3-\hb)^2\to\min$ 
\end{solution}


\begin{problem}
 Функция $f(x)$ непрерывна на отрезке $[0;1]$. Найдите аналог МНК-оценок для регрессии без свободного члена в непрерывном случае. Более подробно: найдите минимум по $\hb$ для функции
\begin{equation}
Q(\hb)= \int_0^1 (f(x)-\hb x)^2\,dx
\end{equation}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 Есть двести наблюдений. Вовочка оценил модель $\hy=\hb_1+\hb_2 x$ по первой сотне наблюдений. Петечка оценил модель $\hy=\hat{\gamma}_1+\hat{\gamma}_2 x$ по второй сотне наблюдений. Машенька оценила модель $\hy=\hat{m}_1+\hat{m}_2 x$ по всем наблюдениям.
\begin{enumerate}
\item Возможно ли, что $\hb_2>0$, $\hat{\gamma}_2>0$, но $\hat{m}_2<0$?
\item Возможно ли, что $\hb_1>0$, $\hat{\gamma}_1>0$, но $\hat{m}_1<0$?
\item Возможно ли одновременное выполнение всех упомянутых условий?
\end{enumerate}
\end{problem}

\begin{solution}
да, возможно. Два вытянутых облачка точек. Первое облачко даёт первую регрессию, второе --- вторую. Прямая, соединяющая центры облачков, --- общую.
\end{solution}



\begin{problem}
 Вася оценил модель $y=\beta_1+\beta_2 d+\beta_3 x+\varepsilon$. Дамми-переменная $d$ обозначает пол, 1 для мужчин и 0 для женщин. Оказалось, что $\hat{\beta}_2>0$. Означает ли это, что для мужчин $\bar{y}$ больше, чем $\bar{y}$ для женщин?
\end{problem}

\begin{solution}
Нет. Коэффициенты можно интерпретировать только <<при прочих равных>>, т.е. при равных $x$. Из-за разных $x$ может оказаться, что у мужчин $\bar{y}$ меньше, чем $\bar{y}$ для женщин.
\end{solution}



\begin{problem}
 Какие из указанные моделей можно представить в линейном виде?
\begin{enumerate}
\item $y_i=\beta_1+\frac{\beta_2}{x_i}+\e_i$
\item $y_i=\exp(\beta_1+\beta_2 x_i+\e_i)$
\item $y_i=1+\frac{1}{\exp(\beta_1+\beta_2 x_i+\e_i)}$
\item $y_i=\frac{1}{1+\exp(\beta_1+\beta_2 x_i+\e_i)}$
\item $y_i=x_i^{\beta_2}e^{\beta_1+\e_i}$
\item $y_i=\beta_1\exp(\beta_2 x_i + \e_i)$
\end{enumerate}
\end{problem}

\begin{solution}
\newpage
\end{solution}



\begin{problem}
 У эконометриста Вовочки есть переменная $1_f$, которая равна 1, если $i$-ый человек в выборке --- женщина, и 0, если мужчина. Есть переменная $1_m$, которая равна 1, если $i$-ый человек в выборке --- мужчина, и 0, если женщина. Какие $\hy$ получатся, если Вовочка попытается построить регрессии:
\begin{enumerate}
\item $y$ на константу и $1_f$
\item $y$ на константу и $1_m$
\item $y$ на $1_f$ и $1_m$ без константы
\item $y$ на константу, $1_f$ и $1_m$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}
 


\begin{problem}
 У эконометриста Вовочки есть три переменных: $r_i$ --- доход $i$-го человека в выборке, $m_i$ --- пол (1 --- мальчик, 0 --- девочка) и $f_i$ --- пол (1 --- девочка, 0 --- мальчик). Вовочка оценил две модели
\begin{enumerate}
\item[] Модель A: $m_i=\beta_1+\beta_2 r_i+\ve_i$
\item[] Модель B: $f_i=\gamma_1+\gamma_2 r_i+u_i$
\end{enumerate}
\begin{enumerate}
\item Как связаны между собой оценки $\hb_1$ и $\hat{\gamma}_1$?
\item Как связаны между собой оценки $\hb_2$ и $\hat{\gamma}_2$? 
\end{enumerate}
\end{problem}

\begin{solution}
 Оценки МНК линейны по объясняемой переменной. Если сложить объясняемые переменные в этих двух моделях, то получится вектор из единичек. Если строить регрессию вектора из единичек на константу и $r$, то получатся оценки коэффициентов 1 и 0. Значит, $\hb_1+\hat{\gamma}_1=1$, $\hb_2+\hat{\gamma}_2=0$ 
\end{solution}



\begin{problem}
 Эконометрист Вовочка оценил линейную регрессионную модель, где $y$ измерялся в тугриках. Затем он оценил ту же модель, но измерял $y$ в мунгу (1 тугрик = 100 мунгу). Как изменятся оценки коэффициентов?
\end{problem}

\begin{solution}
 Увеличатся в 100 раз
\end{solution}


\begin{problem}
 Возможно ли, что при оценке парной регрессии $y=\beta_1+\beta_2 x+\e$ оказывается, что $\hb_2>0$, а при оценке регрессии без константы, $y=\gamma x+\e$, оказывается, что $\hat{\gamma}<0$?
\end{problem}

\begin{solution}
 да
\end{solution}



\begin{problem}
 Эконометрист Вовочка оценил регрессию $y$ только на константу. Какой коэффициент $R^2$ он получит?
\end{problem}

\begin{solution}
$R^2=0$
\end{solution}



\begin{problem}
 Эконометрист Вовочка оценил методом наименьших квадратов модель 1, $y=\b_1+\b_2 x+\b_3 z+\e$, а затем модель 2, $y=\b_1+\b_2 x+\b_3 z+\b_4 w+\e$. Сравните полученные $ESS$, $RSS$, $TSS$ и $R^2$.
\end{problem}

\begin{solution}
 $TSS_1=TSS_2$, $R_2^2\geq R_2^1$, $ESS_2\geq ESS_1$, $RSS_2\leq RSS_1$
\end{solution}




\begin{problem}
  Создайте набор данных с тремя переменными $y$, $x$ и $z$ со следующими свойствами. При оценке модели $\hy=\hb_1+\hb_2 x$ получается $\hb_2>0$. При оценке модели $\hy=\hat{\gamma}_1+ \hat{\gamma}_2 x+\hat{\gamma}_3 z$ получается $\hat{\gamma}_2<0$. Объясните принцип, руководствуясь которым легко создать такой набор данных.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 У меня есть набор данных с выборочным средним $\bar{y}$ и выборочной дисперсией $s_y^2$. Как нужно преобразовать данные, чтобы выборочное среднее равнялось $7$, а выборочная дисперсия --- $9$? 
\end{problem}

\begin{solution}
 $y_i^*=7+3(y_i-\bar{y})/s_y$ 
% эта задача не использует понятия вероятностей, хотя близка. Пусть будет в невероятностной секции.
\end{solution}



\chapter{Парный МНК без матриц}


\begin{problem}
 Рассмотрим модель $y_t=\b_1+\b_2 \cdot t + \e_t$, где ошибки $\e_t$ независимы и равномерны на $[-1;1]$. С помощью симуляций на компьютере оцените и постройте график функции плотности для $\hb_1$, $\hb_2$, $\hs^2$, $\hVar(\hb_1)$, $\hVar(\hb_2)$ и $\hCov(\hb_1,\hb_2)$.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 Пусть $y_i=\mu+\e_i$, где $\E(\e_i)=0$, $\Var(\e_i)=\sigma^2$, $\Cov(\e_i,\e_j)=0$ при $i \ne j$. Найдите:
\begin{enumerate}
\item $\E(\overline{y})$
\item $\Var(\overline{y})$
\item $\E(\frac{1}{n}\sum_{i=1}^n {(y_i-\overline{y})}^2)$
\item $\Var(\frac{1}{n}\sum_{i=1}^n {(y_i-\overline{y})}^2)$, если дополнительно известно, что $\e_i$ нормально распределены
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 Рассматривается модель $y_i=\beta x_i+\e_i$, $\E(\e_i)=0$, $\Var(\e_i)=\sigma^2$, $\Cov(\e_i,\e_j)=0$ при $i \ne j$.
При каких значениях параметров $c_i$ несмещённая оценка $\hb=\frac{\sum_{i=1}^n {c_i y_i}}{\sum_{i=1}^n {c_i x_i}}$ имеет наименьшую дисперсию?
\end{problem}

\begin{solution}
$c_i=c\cdot x_i$, где $c\neq 0$
\end{solution}


\begin{problem}
 Пусть $y_i = \beta_1 + \beta_2 x_i + \e_i$ и $i = 1, \dots, 5$ --- классическая регрессионная модель. Также имеются следующие данные: $\sum_{i=1}^5 y_i^2 = 55, \sum_{i=1}^5 x_i^2 = 3, \sum_{i=1}^5 x_iy_i = 12, \sum_{i=1}^5 y_i = 15, \sum_{i=1}^5 x_i = 3.$ Используя их, найдите:

\begin{enumerate}
\item $\hat{\beta_1}$ и $\hat{\beta_2}$
\item $\Corr(\hat{\beta_1}, \hat{\beta_2})$
\item $TSS$
\item $ESS$
\item $RSS$
\item $R^2$
\item $\hs^2$
\end{enumerate}

Проверьте следующие гипотезы:
\begin{enumerate}
\item $\begin{cases}  H_0: \beta_2 = 2  \\ H_a: \beta_2 \not= 2 \end{cases}$
\item $\begin{cases}  H_0: \beta_1 + \beta_2 = 1  \\ H_a: \beta_1 + \beta_2 \not= 1 \end{cases}$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 Пусть $y_i = \beta_1 + \beta_2 x_i + \e_i$ и $i = 1, \dots, 5$ --- классическая регрессионная модель. Также имеются следующие данные: $\sum_{i=1}^5 y_i^2 = 55, \sum_{i=1}^5 x_i^2 = 2, \sum_{i=1}^5 x_iy_i = 9, \sum_{i=1}^5 y_i = 15, \sum_{i=1}^5 x_i = 2.$ Используя их, найдите:

\begin{enumerate}
\item $\hat{\beta_1}$ и $\hat{\beta_2}$
\item $\Corr(\hat{\beta_1}, \hat{\beta_2})$
\item $TSS$
\item $ESS$
\item $RSS$
\item $R^2$
\item $\hs^2$
\end{enumerate}

Проверьте следующие гипотезы:
\begin{enumerate}
\item $\begin{cases}  H_0: \beta_2 = 2  \\ H_a: \beta_2 \not= 2 \end{cases}$
\item $\begin{cases}  H_0: \beta_1 + \beta_2 = 1  \\ H_a: \beta_1 + \beta_2 \not= 1 \end{cases}$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
 Рассмотрите классическую линейную регрессионную модель $y_i = \beta x_i + \e_i$. Найдите $\E \hb$. Какие из следующих оценок параметра $\beta$ являются несмещенными:

\begin{enumerate}
\item $\hb = \frac{y_1}{x_1}$
\item $\hb = \frac{1}{2} \frac{y_1}{x_1} + \frac{1}{2} \frac{y_n}{x_n}$
\item $\hb = \frac{1}{n}  \frac{y_1}{x_1} + \ldots + \frac{y_n}{x_n} $
\item $\hb = \frac{\overline{y}}{\overline{x}}$
\item $\hb = \frac{y_n - y_1}{x_n - x_1}$
\item $\hb = \frac{1}{2} \frac{y_2 - y_1}{x_2 - x_1} + \frac{1}{2} \frac{y_n - y_{n-1}}{x_n - x_{n-1}}$
\item $\hb = \frac{1}{n} \frac{y_2 - y_1}{x_2 - x_1} + \frac{1}{n} \frac{y_3 - y_2}{x_3 - x_2} + \ldots + \frac{1}{n} \frac{y_n - y_{n-1}}{x_n - x_{n-1}}$
\item $\hb = \frac{1}{n-1}  \frac{y_2 - y_1}{x_2 - x_1} + \frac{y_3 - y_2}{x_3 - x_2} + \ldots + \frac{y_n - y_{n-1}}{x_n - x_{n-1}} $
\item $\hb = \frac{x_1 y_1 + \ldots + x_n y_n}{x_1^2 + \ldots + x_n^2}$
\item $\hb = \frac{1}{2} \frac{y_n - y_1}{x_n - x_1} + \frac{1}{2n}  \left( \frac{y_1}{x_1} + \ldots + \frac{y_n}{x_n} \right) $
\item $\hb =  \frac{1}{2} \frac{y_n - y_1}{x_n - x_1} + \frac{1}{2} \frac{x_1 y_1 + \ldots + x_n y_n}{x_1^2 + \ldots + x_n^2}$
\item $\hb = \frac{\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}{\sum_{i=1}^n (x_i - \overline{x})^2}$
\item $\hb = \frac{\sum_{i=1}^n (x_i - \overline{x})(\overline{y} - y_i)}{\sum_{i=1}^n (x_i - \overline{x})^2}$
\item $\hb = \frac{y_1 + 2 y_2 + \ldots + n y_n}{x_1 + 2 x_2 + \ldots + n x_n}$
\item $\hb = \frac{\sum_{i=1}^n i(y_i - \overline{y})}{\sum_{i=1}^n i(x_i - \overline{x})}$
\item $\hb = \frac{1}{n} \sum_{i=1}^n \frac{y_i}{x_i}$
\item $\hb = \frac{1}{n} \sum_{i=1}^n \frac{y_i - \overline{y}}{x_i - \overline{x}}$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
 Рассмотрите классическую линейную регрессионную модель $y_i = \beta x_i + \e_i$. Найдите $\Var(\hb)$.

\begin{enumerate}
\item $\hb = \frac{y_1}{x_1}$
\item $\hb = \frac{1}{2} \frac{y_1}{x_1} + \frac{1}{2} \frac{y_n}{x_n}$
\item $\hb = \frac{1}{n} \left( \frac{y_1}{x_1} + \ldots + \frac{y_n}{x_n} \right)$
\item $\hb = \frac{\overline{y}}{\overline{x}}$
\item $\hb = \frac{y_n - y_1}{x_n - x_1}$
\item $\hb = \frac{1}{2} \frac{y_2 - y_1}{x_2 - x_1} + \frac{1}{2} \frac{y_n - y_{n-1}}{x_n - x_{n-1}}$
\item $\hb = \frac{x_1 y_1 + \ldots + x_n y_n}{x_1^2 + \ldots + x_n^2}$
\item $\hb = \frac{\sum_{i=1}^n (x_i - \overline{x})(y_i - \overline{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}$
\item $\hb = \frac{\sum_{i=1}^n (x_i - \overline{x})(\overline{y} - y_i)}{\sum_{i=1}^n (x_i - \bar{x})^2}$
\item $\hb = \frac{y_1 + 2 y_2 + \ldots + n y_n}{x_1 + 2 x_2 + \ldots + n x_n}$
\item $\hb = \frac{\sum_{i=1}^n i(y_i - \overline{y})}{\sum_{i=1}^n i(x_i - \overline{x})}$
\item $\hb = \frac{1}{n} \sum_{i=1}^n \frac{y_i}{x_i}$
\item $\hb = \frac{1}{n} \sum_{i=1}^n \frac{y_i - \overline{y}}{x_i - \overline{x}}$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
 Рассмотрите классическую линейную регрессионную модель $y_i = \beta \cdot i + \e_i$, $i=1, \ldots, n$. Какая из оценок $\hb$ и $\tilde{\beta}$ является более эффективной?

\begin{enumerate}
\item $\hb = y_1$ и $\tilde{\beta} = y_2/2$
\item $\hb = y_1$ и $\tilde{\beta} = \frac{1}{2} y_1 + \frac{1}{2} \frac{y_2}{2}$
\item $\hb = \frac{1}{n} \left(  \frac{y_1}{1} + \ldots + \frac{y_n}{n} \right) $ и $\tilde{\beta} = \frac{1 \cdot y_1 + \ldots + n \cdot y_n}{1^2 + \ldots + n^2}$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem} На основе 100 наблюдений была оценена функция спроса:
\[
\underset{(s.e.)}{\widehat{\ln Q}} = \underset{(0.04)}{0.87} - \underset{(0.02)}{1.23}\ln P
\]
Значимо ли коэффициент эластичности спроса по цене отличается от $-1$? Рассмотрите уровень значимости $5\%$.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 На основе 100 наблюдений была оценена функция спроса:
\[
\underset{(s.e.)}{\widehat{\ln Q}} = \underset{(0.04)}{2.87} - \underset{(0.02)}{1.12}\ln P
\]
На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_{\ln P} = - 1$ против альтернативной $H_a: \beta_{\ln P} < -1$. Дайте экономическую интерпретацию проверяемой гипотезе и альтернативе.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 Используя годовые данные с 1960 по 2005 г., была построена кривая Филлипса, связывающая уровень инфляции $Inf$ и уровень безработицы $Unem$:
\[
\widehat{Inf} = 2.34 - 0.23Unem
\]
\[
\sqrt{\widehat{Var}(\hb_{Unem})} = 0.04, R^2 = 0.12
\]
На уровне значимости $1\%$ проверьте гипотезу  $H_0: \beta_{Unem} = 0$ против альтернативной $H_a: \beta_{Unem} \not= 0$.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 Пусть $y_i = \beta_1 + \beta_2 x_i + \e_i$ и $i = 1, \dots, 18$ --- классическая регрессионная модель, где $\E(\e_i)$ = 0, $Var(\e_i) = \sigma^2$. Также имеются следующие данные: $\sum_{i=1}^{18} y_i^2 = 4256, \sum_{i=1}^{18} x_i^2 = 185, \sum_{i=1}^{18} x_iy_i = 814.25, \sum_{i=1}^{18} y_i = 225, \sum_{i=1}^{18} x_i = 49.5.$ Используя эти данные, оцените эту регрессию и на уровне значимости $5\%$ проверьте гипотезу $H_0: \beta_1 = 3.5$ против альтернативной $H_a: \beta_1 > 3.5$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики
\item Укажите распределение тестовой статистики
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 Рассматривается модель $y_i=\mu+\e_i$, где $\E(\e_i)=0$, $\Var(\e_i)=\sigma^2$ и $\Cov(\e_i,\e_j)=0$ при $i\neq j$. При каких $c_i$ несмещенная оцека
\[
\hat{\mu}=\sum_{i=1}^{n} c_i y_i
\]
имеет наименьшую дисперсию?
\end{problem}

\begin{solution}
Через теорему Гаусса--Маркова или через условную минимизацию, $c_i=1/n$
\end{solution}

\begin{problem}
 Рассмотрим классическую линейную регрессионную модель, $y_t=\b\cdot t+\e_t$. Какая из оценок, $\hb$ или $\hb'$ является более эффективной?
\begin{enumerate}
\item $\hb=y_1$, $\hb'=y_2/2$
\item $\hb=y_1$, $\hb'=0.5y_1+0.5\frac{y_2}{2}$
\item $\hb=\frac{1}{n}\left(y_1+\frac{y_2}{2}+\frac{y_3}{3}+\ldots+\frac{y_n}{n}\right)$, $\hb'=\frac{y_1+2y_2+\ldots+ny_n}{1^2+2^2+\ldots+n^2}$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 Ошибки регрессии $\e_i$ независимы и равновероятно принимают значения $+1$ и $-1$. Также известно, что $y_i=\beta \cdot i +\e_i$. Модель оценивается всего по двум наблюдениям.
\begin{enumerate}
\item Найдите закон распределения $\hb$, $RSS$, $ESS$, $TSS$, $R^2$
\item Найдите $\E(\hb)$, $\Var(\hb)$, $\E(RSS)$, $\E(ESS)$, $\E(R^2)$
\item При каком $\beta$ величина $\E(R^2)$ достигает максимума?
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
 Рассмотрим модель с линейным трендом без свободного члена, $y_t=\beta t +\e_t$.
\begin{enumerate}
\item Найдите МНК оценку коэффициента $\beta$
\item Рассчитайте $\E(\hb)$ и $\Var(\hb)$ в предположениях теоремы Гаусса-Маркова
\item Верно ли, что оценка $\hb$ состоятельна?
\end{enumerate}
\end{problem}


\begin{solution}
\begin{enumerate}
\item $\hb=\frac{\sum y_t t}{\sum t^2}$
\item $\E(\hb)=\beta$ и $\Var(\hb)=\frac{\sigma^2}{\sum_{t=1}^{T} t^2}$
\item Да, состоятельна
\end{enumerate}
\end{solution}


\begin{problem}
 В модели $y_t=\beta_1+\beta_2 x_t+\e_t$, где
$x_t=\left\{
\begin{array}{l}
2,\, t=1 \\
1,\, t>1
\end{array}
\right.
$:
\begin{enumerate}
\item Найдите мнк-оценку $\hb_2$
\item Рассчитайте $\E(\hb_2)$ и $\Var(\hb_2)$ в предположениях теоремы Гаусса-Маркова
\item Верно ли, что оценка $\hb_2$ состоятельна?
\end{enumerate}
\end{problem}

\begin{solution}
несостоятельна
\end{solution}


\begin{problem}
 В модели $y_t=\beta_1+\beta_2 x_t$, где
$x_t=\left\{
\begin{array}{l}
1,\, t=2k+1 \\
0,\, t=2k
\end{array}
\right.
$:
\begin{enumerate}
\item Найдите мнк-оценку $\hb_2$
\item Рассчитайте $\E(\hb_2)$ и $\Var(\hb_2)$ в предположениях теоремы Гаусса-Маркова
\item Верно ли, что оценка $\hb_2$ состоятельна?
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
 Априори известно, что парная регрессия должна проходить через точку $(x_{0},y_{0})$.
\begin{enumerate}
\item  Выведите формулы МНК оценок;
\item В предположениях теоремы Гаусса-Маркова найдите дисперсии и средние оценок
\end{enumerate}
\end{problem}

\begin{solution}
Вроде бы равносильно переносу начала координат и применению результата для регрессии без свободного члена. Должна остаться несмещенность.
\end{solution}



\begin{problem}
 Мы предполагаем, что $y_t$ растёт с линейным трендом, т.е. $y_t=\b_1+\b_2 t+\e_t$. Все предпосылки теоремы Гаусса-Маркова выполнены. В качестве оценки $\hb_2$ предлагается $\hb_2=\frac{Y_T-Y_1}{T-1}$, где $T$ --- общее количество наблюдений.
\begin{enumerate}
\item Найдите $\E(\hb_2)$ и $\Var(\hb_2)$
\item Совпадает ли оценка $\hb_2$ с классической мнк-оценкой?
\item У какой оценки дисперсия выше, у $\hb_2$ или классической мнк-оценки?
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 Вася считает, что выборочная ковариация $\sCov(y,\hy)=\frac{\sum (y_i-\bar{y})(\hy_i-\bar{y})}{n-1}$ это неплохая оценка для $\Cov(y_i,\hy_i)$. Прав ли он?
\end{problem}

\begin{solution}
Не прав. Ковариация $\Cov(y_i,\hy_i)$ зависит от $i$, это не одно неизвестное число, для которого можно предложить одну оценку.
\end{solution}


\begin{problem}
 В классической линейной регрессионной модели $y_i=\beta_1+\beta_2 x_i+\e_i$, дисперсия зависимой переменной не зависит от номера наблюдения, $\Var(y_i)=\sigma^2$. Почему для оценки $\sigma^2$ вместо известной из курса математической статистики формулы $\sum (y_i-\bar{y})^2/(n-1)$ используют $\sum \he_i^2/(n-2)$?
\end{problem}

\begin{solution}
формула $\sum (y_i-\bar{y})^2/(n-1)$ неприменима так как $\E(y_i)$ не является константой
\end{solution}



\begin{problem}
 Оценка регрессии имеет вид $\hy_i=3-2x_i$. Выборочная дисперсия $x$ равна $9$, выборочная дисперсия $y$ равна $40$. Найдите $R^2$ и выборочные корреляции $\sCorr(x,y)$, $\sCorr(y,\hy)$.
\end{problem}

\begin{solution}
$R^2$ --- это отношение выборочных дисперсий $\hy$ и $y$.
\end{solution}



\begin{problem}
 Слитки-вариант. Перед нами два золотых слитка и весы, производящие взвешивания с ошибками. Взвесив первый слиток, мы получили результат $300$ грамм, взвесив второй слиток --- $200$ грамм, взвесив оба слитка --- $400$ грамм. Предположим, что ошибки взвешивания --- независимые одинаково распределенные случайные величины с нулевым средним.
\begin{enumerate}
\item Найдите несмещеную оценку веса первого слитка, обладающую наименьшей дисперсией.
\item Как можно проинтерпретировать нулевое математическое ожидание ошибки взвешивания?
\end{enumerate}
\end{problem}

\begin{solution}
 Как отсутствие систематической ошибки.
\end{solution}



\begin{problem}
 Рассмотрим линейную модель $y_i=\beta_1+\beta_2 x_i +\e_i$, где ошибки $\e_i$ нормальны $N(0;\sigma^2)$ и независимы.
\begin{enumerate}
\item Верно ли, что $y_i$ одинаково распределены?
\item Верно ли, что $\bar{y}$ --- это несмещенная оценка для $\E(y_i)$?
\item Верно ли, что $\sum (y_i-\bar{y})^2/(n-1)$ --- несмещенная оценка для $\sigma^2$? Если да, то докажите, если нет, то определите величину смещения
\end{enumerate}
\end{problem}

\begin{solution}
нет, нет, нет
\end{solution}



\begin{problem}
 Модель регрессии $y_i = \beta_1 + \beta_2 x_i + \e_i$, в которой ошибки
$\e_i$ независимы и нормальны $N(0;\sigma^2)$, оценивается по 22 наблюдениям. Найдите $\E(RSS)$, $\Var(RSS)$, $\P(10\sigma^2<RSS<30\sigma^2)$, $\P(10\hs^2<RSS<30\hs^2)$
\end{problem}

\begin{solution}
 $RSS/\sigma^2\sim\chi^2_{n-k}$, $\E(RSS)=(n-k)\sigma^2$, $\Var(RSS)=2(n-k)\sigma^4$, $\P(10\sigma^2<RSS<30\sigma^2)\approx 0.898$
\end{solution}


\begin{problem}
 Модель регрессии $y_i = \beta_1 + \beta_2 x_i + \e_i$, в которой ошибки
$\e_i$ независимы и нормальны $N(0;\sigma^2)$, оценивается по 12 наблюдениям. Найдите
\begin{enumerate}
\item $\P(\hb_1>\beta_1)$, $\P(\beta_1>0)$, $\P(|\hb_1-\beta_1|<se(\hb_1))$, $\P(\hb_2>\beta_2+se(\hb_2))$, $\P(\hb_2>\beta_2-se(\hb_2))$
\item $\E(\hb_1)$, $\E(\hb_2)$, $\E(\beta_2)$
\item Закон распределения, математическое ожидание и дисперсию величин $\frac{\hb_2-\beta_2}{\sqrt{\Var(\hb_2)}}$, $\frac{\hb_2-\beta_2}{\sqrt{\widehat{\Var}(\hb_2)}}$, $\frac{\hb_1+\hb_2-\beta_1-\beta_2}{\sqrt{\widehat{\Var}(\hb_1+\hb_2)}}$
\item $\P(\hs>\sigma)$, $\P(\hs>2\sigma)$
\end{enumerate}

\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
 Для модели парной регрессии известны $y=(1, 2, 3, 4, 5)'$ и $\hy=(2, 2, 2, 4, 5)'$. Найдите $RSS$, $TSS$, $R^2$, $\hs^2$.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 В классической парной регрессионной модели $y_i = \beta_1 + \beta_2 x_i + \e_i$ с нормально распределенными ошибками, оцениваемой по 30 наблюдениям, дополнительно известно, что $\Var(\e_7)=9$. Найдите
\begin{enumerate}
\item $\E(\e_2)$, $\Cov(\e_1,\e_3)$, $\E(\e_3^5)$, $\E(\he_5^3)$, $\Var(\he_5)$, $\Var(y_3)$
\item $\P(\he_2>\e_3)$, $\P(\he_1>0)$, $\P(\he_1>3)$
\item $\E(RSS)$, $\Var(RSS)$, $\P(RSS>200)$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
В модели парной регрессии придумайте такие наблюдения, чтобы:
\begin{itemize}
\item $R^2=0.9$
\item $R^2=0.8$ и регрессия имела вид $\hy=2+3x$
\end{itemize}
\end{problem}

\begin{solution}
 Можно взять четыре наблюдения равноотстоящих по вертикали от данной прямой. Подбирая остатки, добиваемся нужного $R^2$.
 \end{solution}



\begin{problem}
Оцененная с помощью линейной модели $y_t = \beta_1 + \beta_2 t + \e_t$ методом наименьших квадратов зависимость расходов на питание $y$ от времени, определённого как $t = 1$ для 1995 г., $t = 2$ для 1996 г., \ldots, $t = 12$ для 2006 г., задана уравнением $\hat{y}_t = 95 + 2.5 t$.

Чему были бы равны оценки коэффициентов $\beta_1$ и $\beta_2$, если бы в качестве $t$ использовались фактические даты (1995 – 2006), а не числа от 1 до 12?
\end{problem}

\begin{solution}
$\hat{\beta_1} = -4890$ и $\hat{\beta_2} = 2.5$

$X = \begin{bmatrix}
1 & 1 \\
1 & 2 \\
\ldots & \ldots \\
1 & 12 \\
\end{bmatrix}$ --- матрица исходных регрессоров; $\tilde{X} = \begin{bmatrix}
1 & 1+1994\\
1 & 2+1994 \\
\ldots & \ldots \\
1 & 12+1994 \\
\end{bmatrix}$ --- матрица новых регрессоров.

$\tilde{X} = X \cdot D$, где $D = \begin{bmatrix}
1 & 1994 \\
0 & 1 \\
\end{bmatrix}$.

Итак, уравнение регрессии с новыми регрессорами имеет вид $y = \tilde{X}\beta + \e$ и МНК-оценки коэффициентов равны:
\begin{multline}
\hb = \left( \tilde{X}^T \tilde{X} \right)^{-1} \tilde{X}^T y = \left( [XD]^T [XD] \right)^{-1} [XD]^T y = \\
D^{-1} (X^T X)^{-1} (D^T)^{-1} D^T X^T y = D^{-1} (X^T X)^{-1}X^T y
\end{multline}
\[
\hb = D^{-1}\hb_{old} = \begin{bmatrix}
1 & -1994 \\
0 & 1 \\
\end{bmatrix} \begin{bmatrix}
95 \\
2.5 \\
\end{bmatrix} = \begin{bmatrix}
-4890 \\
2.5 \\
\end{bmatrix}
\]
\end{solution}

\begin{problem}
Пусть есть набор данных $(x_i, y_i)$, $i = 1, \ldots, n$, $(x_i>0, y_i>0)$, порожденных уравнением $y_i = \beta_1 + \beta_2 x_i + \e_i$, удовлетворяющих условиям стандартной модели парной регрессии.
Рассматриваются следующие оценки параметра $\beta_2$:
$$\tilde{\beta}_2^a = \frac{1}{n}\sum_{i=1}^n \frac{y_i}{x_i}, \text{ } \tilde{\beta}_2^b = \frac{\overline{y}}{\overline{x}}$$
Найти дисперсию и смещение каждой из оценок.
\end{problem}

\begin{solution}
Мы можем существенно упростить решение, воспользовавшись матричным представлением:

\begin{multline}
\tilde{\beta}_2^a = \frac{1}{n}\sum_{i=1}^n \frac{y_i}{x_i} = \frac{1}{n} \begin{bmatrix}
\frac{1}{x_1} & \frac{1}{x_2} & \ldots & \frac{1}{x_n}
\end{bmatrix} \begin{bmatrix}
y_1\\
y_2\\
\vdots\\
y_n\\
\end{bmatrix} = \\
 \frac{1}{n} \begin{bmatrix}
\frac{1}{x_1} & \frac{1}{x_2} & \ldots & \frac{1}{x_n}
\end{bmatrix} y
\end{multline}

\begin{multline}
\E\tilde{\beta}_2^a = \frac{1}{n}\sum_{i=1}^n \frac{\E y_i}{x_i} = \frac{1}{n} \begin{bmatrix}
\frac{1}{x_1} & \frac{1}{x_2} & \ldots & \frac{1}{x_n}
\end{bmatrix} \begin{bmatrix}
\E y_1\\
\E y_2\\
\vdots\\
\E y_n\\
\end{bmatrix} = \\
 \frac{1}{n} \begin{bmatrix}
\frac{1}{x_1} & \frac{1}{x_2} & \ldots & \frac{1}{x_n}
\end{bmatrix} \begin{bmatrix}
\beta_1 + \beta_2 x_1\\
\beta_1 + \beta_2 x_2\\
\vdots\\
\beta_1 + \beta_2 x_n\\
\end{bmatrix} = \\
\frac{1}{n} \begin{bmatrix}
\frac{1}{x_1} & \frac{1}{x_2} & \ldots & \frac{1}{x_n}
\end{bmatrix} \left[ \beta_1 \begin{bmatrix}
1\\
1\\
\vdots\\
1\\
\end{bmatrix} + \beta_2 \begin{bmatrix}
x_1\\
x_2\\
\vdots\\
x_n\\
\end{bmatrix} \right] = \frac{\beta_1}{n}\sum_{k=1}^n \frac{1}{x_k} + \beta_2
\end{multline}

Значит, смещение для первой оценки равно $\frac{\beta_1}{n}\sum_{k=1}^n \frac{1}{x_k}$.

\begin{multline}
\Var(\tilde{\beta}_2^a) = \Var\left(\frac{1}{n} \begin{bmatrix}
\frac{1}{x_1} & \frac{1}{x_2} & \ldots & \frac{1}{x_n}
\end{bmatrix} y\right) =\\
 \frac{1}{n^2} \begin{bmatrix}
\frac{1}{x_1} & \frac{1}{x_2} & \ldots & \frac{1}{x_n}
\end{bmatrix} \Var(y) \begin{bmatrix}
\frac{1}{x_1} & \frac{1}{x_2} & \ldots & \frac{1}{x_n}
\end{bmatrix}^T =\\
\frac{1}{n^2} \begin{bmatrix}
\frac{1}{x_1} & \frac{1}{x_2} & \ldots & \frac{1}{x_n}
\end{bmatrix} \Var(\e) \begin{bmatrix}
\frac{1}{x_1} & \frac{1}{x_2} & \ldots & \frac{1}{x_n}
\end{bmatrix}^T = \\
 \frac{1}{n^2} \begin{bmatrix}
\frac{1}{x_1} & \frac{1}{x_2} & \ldots & \frac{1}{x_n}
\end{bmatrix} \sigma_{\e}^2 I \begin{bmatrix}
\frac{1}{x_1} & \frac{1}{x_2} & \ldots & \frac{1}{x_n}
\end{bmatrix}^T =\\
\frac{\sigma^2_{\e}}{n^2}\sum_{k=1}^n \frac{1}{x_k^2}
\end{multline}

Перейдём ко второй оценке.

$\tilde{\beta}_2^b = \frac{\overline{y}}{\overline{x}} = \frac{1}{\overline{x}} \frac{1}{n} \begin{bmatrix}
1 & 1 & \ldots & 1
\end{bmatrix} y$


\begin{multline}
\E\tilde{\beta}_2^b = \frac{\overline{y}}{\overline{x}} = \frac{1}{\overline{x}} \frac{1}{n} \begin{bmatrix}
1 & 1 & \ldots & 1
\end{bmatrix} \E y = \frac{1}{\overline{x}} \frac{1}{n} \begin{bmatrix}
1 & 1 & \ldots & 1
\end{bmatrix} \begin{bmatrix}
\beta_1 + \beta_2 x_1\\
\beta_1 + \beta_2 x_2\\
\vdots\\
\beta_1 + \beta_2 x_n\\
\end{bmatrix} =\\
\frac{1}{\overline{x}} \frac{1}{n} \begin{bmatrix}
1 & 1 & \ldots & 1
\end{bmatrix} \left[ \beta_1 \begin{bmatrix}
1\\
1\\
\vdots\\
1\\
\end{bmatrix} + \beta_2 \begin{bmatrix}
x_1\\
x_2\\
\vdots\\
x_n\\
\end{bmatrix} \right] = \\
\frac{1}{n} \frac{\beta_1 n}{\overline{x}} + \frac{1}{n} \frac{\beta_2 \sum x_i}{\overline{x}} = \frac{\beta_1}{\overline{x}} + \beta_2
\end{multline}

Значит, смещение равно $\frac{\beta_1}{\overline{x}}$.

\begin{multline}
\Var(\tilde{\beta}_2^b) = \frac{1}{\overline{x}^2} \frac{1}{n^2} \begin{bmatrix}
1 & 1 & \ldots & 1
\end{bmatrix} \Var(y) \begin{bmatrix}
1\\
1\\
\vdots\\
1\\
\end{bmatrix} = \\
\frac{1}{\overline{x}^2} \frac{1}{n^2} \begin{bmatrix}
1 & 1 & \ldots & 1
\end{bmatrix} \Var(\e) \begin{bmatrix}
1\\
1\\
\vdots\\
1\\
\end{bmatrix} = \frac{\sigma_{\e}^2}{\overline{x}^2 n}
\end{multline}
\end{solution}



\begin{problem}
Уравнение $y_t = \beta_1 + \beta_2 x_t + \e_t$ оценивается по МНК. Может ли коэффициент детерминации быть малым (<0.05), а статистика $t_{\hb_2}$ большой (>10)?
\end{problem}

\begin{solution}
Известно, что для парной регрессии $t_{\hb_2}^2 = \frac{R^2}{(1 - R^2)/(n-2)}$. Поэтому из выражения $t_{\hb_2}^2 = \frac{0.05^2}{(1 - 0.05^2)/(n-2)} = \frac{0.05^2 (n-2)}{1 - 0.05^2}$ становится очевидным, что при надлежащем выборе числа наблюдений можно сделать величину $t_{\hb_2}$ сколь угодно большой.
\end{solution}


\begin{problem}
 Докажите, что в случае, когда $|\sCorr(x, y)| = 1$, линия парной регрессии $y$ на $x$ совпадает с линией парной регрессии $x$ на $y$.
\end{problem}

\begin{solution}
Пусть $Y_i = \beta_1 + \beta_2 X_i + \e_i$, $i = 1, \ldots, n$.

Тогда $Y_i = \hb_1 + \hb_2 X_i + \hat{\e}_i$

$Y_i -  \overline{Y} + \overline{Y} = \hb_1 + \hb_2 (X_i -  \overline{X} + \overline{X}) + \hat{\e}_i$

$Y_i -  \overline{Y}  = \underbrace{\hb_1 - \overline{Y} + \hb_2 \overline{X}}_{=0} + \hb_2 (X_i -  \overline{X}) + \hat{\e}_i$

$Y_i -  \overline{Y} = \hb_2 (X_i -  \overline{X}) + \hat{\e}_i$

$y_i \equiv Y_i -  \overline{Y}$, $i = 1, \ldots, n$

$x_i \equiv X_i -  \overline{X}$, $i = 1, \ldots, n$

$y_i = \hb_2 x_i + \hat{\e}_i$

$\textbf{y} = \hb_2 \textbf{x} + \hat{\e}$, где $\textbf{y} = \begin{bmatrix}
y_1 & \ldots & y_n
\end{bmatrix}^T$, $\textbf{x} = \begin{bmatrix}
x_1 & \ldots & x_n
\end{bmatrix}^T$, $\e = \begin{bmatrix}
\e_1 & \ldots & \e_n
\end{bmatrix}^T$

$\textbf{x}^T \textbf{y} = \hb_2 \textbf{x}^T \textbf{x} + \underbrace{\textbf{x}^T \hat{\e}}_{=0}$

\begin{equation}
\label{task20:direct_ols}\hb_2 = \frac{\textbf{x}^T \textbf{y}}{\textbf{x}^T \textbf{x}}
\end{equation}

Аналогично получаем, что в обратной регрессии $X_i = \beta_3 + \beta_4 Y_i + \xi_i$, $i = 1, \ldots, n$

\begin{equation}
\label{task20:reverse_ols}\hb_4 = \frac{\textbf{y}^T \textbf{y}}{\textbf{y}^T \textbf{y}}
\end{equation}

$ESS = (\hat{Y} - \overline{Y}_i)^T(\hat{Y} - \overline{Y}_i)$

Заметим, что $\hat{Y} - \overline{Y}_i = (I - \pi)(\hat{Y} - \overline{Y}_i)$.

Действительно, $(I - \pi)(P - \pi) = P - \pi$, следовательно,

$\hat{Y} - \overline{Y}_i = (P - \pi)Y = (I - \pi)(P - \pi)Y = (I-\pi)(\hat{Y} - \overline{Y}_i)$.

Далее, $\hat{Y} - \overline{Y}_i = (I - \pi)(\hat{Y} - \overline{Y}_i) = (I - \pi)(\hb_1 + \hb_2 X - \overline{Y}_i) = \hb_2 \textbf{x}$

Значит, $ESS = \hb_2^2 \textbf{x}^T \textbf{x}$.

Получаем:
\begin{equation}
\label{task20:corr}R^2 = \frac{ESS}{TSS} = \frac{\hb_2^2 \textbf{x}^T \textbf{x}^{(2)}}{\textbf{y}^T \textbf{y}} = \frac{\textbf{x}^T \textbf{y}^{(2)}}{(\textbf{x}^T \textbf{x})(\textbf{y}^T \textbf{y})} = \Corr^2(X, Y)
\end{equation}

Заметим также, что из формул (\ref{task20:direct_ols}), (\ref{task20:reverse_ols}) и (\ref{task20:corr}) следует, что $R^2 = \hb_2 \hb_4$.

Если $\Corr^2(X, Y) = 1$, то $R^2 = \hb_2 \hb_4 = 1$.

Отметим также, что из $R^2 = 1$ следует, что $\hat{\e}_1 = \ldots = \hat{\e}_n = 0$ и $\hat{\xi}_1 = \ldots = \hat{\xi}_n = 0$.

Тогда $Y_i = \hb_1 + \hb_2 X_i + \underbrace{\hat{\e}_i}_{=0}$ и $X_i = \hat{\beta_3} + \hat{\beta_4} Y_i + \underbrace{\hat{\xi}_i}_{=0}$, $i = 1, \ldots, n$.

$X_i = \hat{\beta_3} + \hat{\beta_4} Y_i = (\overline{X} - \hat{\beta_4}\overline{Y}) + \hat{\beta_4} Y_i = \left( \overline{X} - \frac{1}{\hb_2} \overline{Y} \right) + \frac{1}{\hb_2} Y_i$

$\hb_2 X_i = (\hb_2 \overline{X} - \overline{Y}) + Y_i$

$Y_i = (\overline{Y} - \hb_2 \overline{X}) + \hb_2 X_i = \hb_1 + \hb_2 X_i$

Следовательно, в случае когда $\Corr^2(X, Y) = 1$, линия парной регрессии $Y$ на $X$ совпадает с линией парной регрессии $X$ на $Y$.
\end{solution}



\begin{problem}
Сгенерите выборку из двух зависимых но некоррелированных случайных величин. Можно ли <<поймать>> зависимость используя парную регрессию?
\end{problem}

\begin{solution}
Да, если строить регрессию функции от $y$ на функцию от $x$. А если строить регрессию просто $y$ на $x$, то оценка наклона будет распределена симметрично около нуля.
\end{solution}


\begin{problem}
Все предпосылки классической линейной модели выполнены, $y=\beta_1+\beta_2 x+\e$. Рассмотрим альтернативную оценку коэффициента $\beta_2$,
\begin{equation}
\hb_{2,IV}=\frac{\sum z_i(y_i-\bar{y})}{\sum z_i(x_i-\bar{x})}
\end{equation}
\begin{enumerate}
\item Является ли оценка несмещенной?
\item Любые ли $z_i$ можно брать?
\item Найдите $\Var(\hb_{2,IV})$
\end{enumerate}
\end{problem}

\begin{solution}
Да, является. Любые, кроме констант. $\Var(\hb_{2,IV})=\sigma^2 \sum (z_i-\bar{z})^2/ \left(\sum (z_i-\bar{z})x_i \right)^2 $.
\end{solution}

\begin{problem}
Напишите формулу для оценок коэффициентов в парной регрессии без матриц. Напишите формулу для дисперсий оценок коэффициентов.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Рассматривается модель линейной регрессии $y_i = \beta_1 + \beta_2x_i + \e_i$, в которой ошибки $\e_1, \ldots, \e_n$ являются независимыми нормально распределенными случайными величинами с математическим ожиданием $0$ и дисперсией $\sigma^2$. Найдите

\begin{enumerate}
  \item $\mathbb{P}\{\e_1 > 0\}$,
  \item $\mathbb{P}\{\e_1^2 + \e_2^2 > 2\sigma^2\}$,
  \item $\mathbb{P}\left\{\frac{\e_1}{\sqrt{\e_2^2 + \e_3^2}} > 2 \right\}$,
  \item $\mathbb{P}\left\{\frac{\e_1}{\sqrt{\e_2^2 + \e_3^2 + \e_4^2}} > \frac{5}{4\sqrt{3}} \right\}$,
  \item $\mathbb{P}\left\{\frac{\e_1 + 2\e_2}{\sqrt{\e_3^2 + \e_4^2 + \e_5^2}} < \frac{9}{2} \right\}$,
  \item $\mathbb{P}\left\{\frac{\e_1^2}{\e_2^2 + \e_3^2} > 17 \right\}$.
\end{enumerate}
\end{problem}

\begin{solution}
Вспомните про $t$, $\chi^2$, $F$ распределения
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% новые задачи

\begin{problem}
В модели парной регрессии $y_i=\beta_1+\beta_2 x_i +\e_i$ ошибки $\e_i$ независимы и имеют пуассоновское распределение с параметром $\lambda$.
\begin{enumerate}
\item Предложите способ несмещенно оценить $\lambda$.
\item Являются ли МНК-оценки $\hb_1$ и $\hb_2$ несмещенными? Если оценки являются смещенными, то предложите несмещенные оценки
\end{enumerate}
\end{problem}


\begin{solution}
$\hat{\lambda}=RSS/(n-2)$ т.к. $\Var(\e_i)=\lambda$. Оценка $\hb_2$ является несмещенной, но $\E(\hb_1)=\beta_1+\lambda$. Можно предложить несмещенную оценку $\hb'_1=\hb_1-RSS/(n-2)$.
\end{solution}



\begin{problem}
У Эконометрессы Глафиры было четыре наблюдения и она решила оценить модель парной регрессии:

\begin{tabular}{cc}
$y$ & $x$ \\
\hline
5 & 1 \\
4 & 2 \\
4 & 3 \\
3 & 4 \\
\end{tabular}

Эконометресса Анжелла решила, что четыре наблюдения --- мало, и поэтому учла каждое наблюдение 10 раз, так что в результате у неё вышло 40 наблюдений.

\begin{enumerate}
\item Какие оценки коэффициентов получат Анжелла и Глафира? Будут ли значимы оценки коэффициентов в предположении нормальности ошибок?
\item Во сколько раз у Анжеллы и Глафиры отличаются: коэффициенты детерминации, коэффициенты выборочной корреляции между $x$ и $y$, $RSS$?
\end{enumerate}

\end{problem}


\begin{solution}
\begin{kframe}
\begin{alltt}
\hlstd{df1} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{x} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{,}\hlnum{3}\hlstd{,}\hlnum{4}\hlstd{),} \hlkwc{y} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{5}\hlstd{,}\hlnum{3}\hlstd{,}\hlnum{3}\hlstd{,}\hlnum{4}\hlstd{) )}
\hlstd{df2} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{y} \hlstd{=} \hlkwd{rep}\hlstd{(df1}\hlopt{$}\hlstd{y,}\hlnum{10}\hlstd{),} \hlkwc{x} \hlstd{=} \hlkwd{rep}\hlstd{(df1}\hlopt{$}\hlstd{x,}\hlnum{10}\hlstd{))}
\hlstd{m1} \hlkwb{<-} \hlkwd{lm}\hlstd{(}\hlkwc{data}\hlstd{=df1, y}\hlopt{~}\hlstd{x)}
\hlstd{m2} \hlkwb{<-} \hlkwd{lm}\hlstd{(}\hlkwc{data}\hlstd{=df2, y}\hlopt{~}\hlstd{x)}
\hlkwd{library}\hlstd{(memisc)}
\hlstd{mt} \hlkwb{<-} \hlkwd{mtable}\hlstd{(m1,m2,}
  \hlkwc{summary.stats}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"N"}\hlstd{,}
    \hlstr{"Deviance"}\hlstd{,}\hlstr{"R-squared"}\hlstd{,} \hlstr{"sigma"}\hlstd{,} \hlstr{"F"}\hlstd{,} \hlstr{"p"}\hlstd{))}
\hlkwd{write.mtable}\hlstd{(mt,} \hlkwc{forLaTeX}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\end{alltt}
\end{kframe}%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Calls:
% m1:  lm(formula = y ~ x, data = df1) 
% m2:  lm(formula = y ~ x, data = df2) 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{tabular}{lcD{.}{.}{7}cD{.}{.}{7}}
\toprule
&&\multicolumn{1}{c}{m1} && \multicolumn{1}{c}{m2}\\
\midrule
(Intercept) &  &    4.500     &&  4.500^{***}\\
            &  &   (1.313)    &&  (0.301)    \\
x           &  &   -0.300     && -0.300^{**} \\
            &  &   (0.480)    &&  (0.110)    \\
\midrule
N           &  &    4         &&    40       \\
Deviance    &  &    2.300     &&    23.000   \\
R-squared   &  &    0.164     &&     0.164   \\
sigma       &  &    1.072     &&     0.778   \\
F           &  &    0.391     &&     7.435   \\
p           &  &    0.595     &&     0.010   \\
\bottomrule
\end{tabular}


\end{solution}






\chapter{Многомерный МНК без матриц}



\begin{problem} % 3.1
 Эконометрэсса Ширли зашла в пустую аудиторию, где царил приятный полумрак, и увидела на доске до боли знакомую надпись:
\[
\hat{y}=\underset{(2.37)}{1.1}-\underset{(-0.4)}{0.7}\cdot x_2+\underset{(3.15)}{0.9}\cdot x_3-\underset{(-0.67)}{19}\cdot x_4
\]

Помогите эконометрэссе Ширли определить, что находится в скобках 
\begin{enumerate}
\item $P$-значения
\item $t$-статистики
\item стандартные ошибки коэффициентов
\item $R^2$ скорректированный на номер коэффициента
\item показатели $VIF$ для каждого коэффициента
\end{enumerate}
\end{problem}

\begin{solution}
$t$-статистики
\end{solution}

\begin{problem}  % 3.2
 Для нормальной регрессии с 5-ю факторами (включая свободный член) известны границы симметричного по вероятности 80$\%$ доверительного интервала для дисперсии $\sigma_{\e}^2$: $[45; 87.942]$.

\begin{enumerate}
\item Определите количество наблюдений в выборке
\item Вычислите $\hs_{\e}^2$
\end{enumerate}
\end{problem}
 
\begin{solution}
\begin{enumerate}
\item Поскольку $\frac{\hs_{\e}^2(n-k)}{\sigma_{\e}^2} \sim \chi ^2(n-k)$, где $\hs_{\e}^2 = \frac{RSS}{n-k}$, $k$ = 5. $P(\chi_{l}^2 < \frac{\hs_{\e}^2}{\sigma_{\e}^2} < \chi_{u}^2) = 0.8$. Преобразовав, получим $P(\frac{\hs_{\e}^2(n-5)}{\chi_{u}^2} < \sigma_{\e}^2 < \frac{\hs_{\e}^2(n-5)}{\chi_{l}^2}) = 0.8$, где $\chi_{u}^2 = \chi_{n-5; 0.1} ^2$, $\chi_{l}^2 = \chi_{n-5; 0.9} ^2$ --- соответствующие квантили. По условию $\frac{\hs_{\e}^2(n-5)}{\chi_{l}^2} = A = 45, \frac{\hs_{\e}^2(n-5)}{\chi_{u}^2} = B = 87.942.$ Поделим $B$ на $A$, отсюда следует $\frac{\chi_{u}^2}{\chi_{l}^2} = 1.95426.$ Перебором квантилей в таблице для хи-квадрат распределения мы находим, что $\frac{\chi_{30; 0.1}^2}{\chi_{30; 0.9}^2} = \frac{40.256}{20.599} = 1.95426.$ Значит, $n - 5 = 30$, отсюда следует, что $n = 35.$
\item $\hs_{\e}^2 = 45 \frac{\chi_{u}^2}{n-5} = 45 \frac{40.256}{30} = 60.384$ 
\end{enumerate}

Решение в R:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{df} \hlkwb{<-} \hlnum{1}\hlopt{:}\hlnum{200}
\hlstd{a} \hlkwb{<-} \hlkwd{qchisq}\hlstd{(}\hlnum{0.1}\hlstd{,df)}
\hlstd{b} \hlkwb{<-} \hlkwd{qchisq}\hlstd{(}\hlnum{0.9}\hlstd{,df)}
\hlstd{c} \hlkwb{<-} \hlstd{b}\hlopt{/}\hlstd{a}
\hlstd{d} \hlkwb{<-} \hlnum{87.942}\hlopt{/}\hlnum{45}
\hlstd{penalty} \hlkwb{<-} \hlstd{(c}\hlopt{-}\hlstd{d)}\hlopt{^}\hlnum{2}
\hlstd{df.ans} \hlkwb{<-} \hlstd{df[}\hlkwd{which}\hlstd{(penalty}\hlopt{==}\hlkwd{min}\hlstd{(penalty))]}
\end{alltt}
\end{kframe}
\end{knitrout}
Количество степеней свободы $n-5$ должно быть равно $\verb|df.ans|=30$.

\end{solution}

\begin{problem}  % 3.3
 Рассмотрим следующую регрессионную модель зависимости логарифма заработной платы индивида $\ln W$ от его уровня образования $Edu$, опыта работы $Exp$, $Exp^2$, уровня образования его отца $Fedu$, и уровня образования его матери $Medu$:
\[
\widehat{\ln W}=\hat{\beta}_1+\hat{\beta}_2Edu+\hat{\beta}_3Exp+\hat{\beta}_4Exp^2+\hat{\beta}_5Fedu+\hat{\beta}_6Medu
\]

Модель регрессии была отдельно оценена по выборкам из 35 мужчин и 23 женщин, и были получены остаточные суммы квадратов $RSS_1$ = 34.4 и $RSS_2$ = 23.4 соответственно. Остаточная сумма квадратов в регрессии, оценённой по объединённой выборке, равна 70.3. На уровне значимости $5\%$ проверьте гипотезу об отсутствии дискриминации в оплате труда между мужчинами и женщинами.
\end{problem}

\begin{solution}

Упорядочим нашу выборку таким образом, чтобы наблюдения с номерами с 1 по 35 относились к мужчинам, а наблюдения с номерами с 36 по 58 относились к женщинам.
Тогда уравнение
\begin{multline}
\ln W_i=\beta_1+\beta_2 Edu_i+\beta_3 Exp_i+\beta_4 Exp_i^2+\\
\beta_5 Fedu_i+\beta_6 Medu_i+\e_i, i=1,...,35
\end{multline}

соответствует регрессии, построенной для подвыборки из мужчин, а уравнение
\begin{multline}
\ln W_i=\gamma_1+\gamma_2 Edu_i+\gamma_3 Exp_i+\gamma_4 Exp_i^2+\\
\gamma_5 Fedu_i+\gamma_6 Medu_i+\e_i, i=36,...,58
\end{multline}
соответствует регрессии, построенной для подвыборки из женщин. Введем следующие переменные:

\[
d_i =
\begin{cases}
    1, & \text{если $i$--ое наблюдение соответствует мужчине,} \\
    0, & \text{в противном случае;}
\end{cases}
\]

\[
dum_i =
\begin{cases}
    1, & \text{если $i$--ое наблюдение соответствует женщине,} \\
    0, & \text{в противном случае.}
\end{cases}
\]

Рассмотрим следующее уравнение регрессии:
\begin{multline}
\ln W_i=\beta_1 d_i+\gamma_1 dum_i+\beta_2 Edu_i d_i+\gamma_2
Edu_i dum_i+\beta_3 Exp_i d_i+\\
\gamma_3 Exp_i dum_i+\beta_4 Exp_i^2 d_i + \gamma_4 Exp_i^2 dum_i + \beta_5 Fedu_i d_i +\gamma_5 Fedu_i dum_i+\\
\beta_6 Medu_i d_i+\gamma_6 Medu_i dum_i+\e_i, i=1,...,58
\end{multline}
Гипотеза, которую требуется проверить в данной задаче, имеет вид

\[
H_0: 
  \begin{cases}
    \beta_1 =\gamma_1, \\
    \beta_2 =\gamma_2 , & H_1:|\beta_1-\gamma_1|+|\beta_2-\gamma_2|+\dots+|\beta_6-\gamma_6| > 0.\\
    \dots   \\
    \beta_6=\gamma_6 \\
 \end{cases}
\]

Тогда регрессия
\begin{multline}
\ln W_i=\beta_1 d_i+\gamma_1 dum_i+\beta_2 Edu_i d_i+\gamma_2 Edu_i dum_i+\beta_3 Exp_i d_i+\\
\gamma_3 Exp_i dum_i+\beta_4 Exp_i^2 d_i+
\gamma_4 Exp_i^2 dum_i+\beta_5 Fedu_i d_i +\\
\gamma_5 Fedu_i dum_i+\beta_6 Medu_i d_i+\gamma_6 Medu_i dum_i+\e_i, i=1,\ldots,58
\end{multline}
по отношению к основной гипотезе $H_0$ является регрессией без ограничений, а регрессия 
\begin{multline}
\ln W_i=\beta_1 +\beta_2 Edu_i+\beta_3 Exp_i+\beta_4 Exp_i^2+\\
\beta_5 Fedu_i+\beta_6 Medu_i+\e_i, i=1,...,58 
\end{multline}

является регрессией с ограничениями.

Кроме того, для решения задачи должен быть известен следующий факт:

$RSS_{UR}=RSS_1+RSS_2$, где $RSS_{UR}$ --- это сумма квадратов остатков в модели:
\begin{multline}
\ln W_i=\beta_1  d_i+\gamma_1 dum_i+\beta_2 Edu_i d_i+\gamma_2 Edu_i dum_i+\beta_3 Exp_i d_i+\\
\gamma_3 Exp_i dum_i+\beta_4 Exp_i^2 d_i
+ \gamma_4 Exp_i^2 dum_i+\beta_5 Fedu_i d _i +\\
\gamma_5 Fedu_i dum_i+\beta_6 Medu_i d_i+\gamma_6 Medu_i dum_i+\e_i, i=1,...,58
\end{multline}

$RSS_1$ --- это сумма квадратов остатков в модели:
\begin{multline}
\ln W_i=\beta_1 +\beta_2 Edu_i+\beta_3 Exp_i+\beta_4 Exp_i^2+\\
\beta_5 Fedu_i+\beta_6 Medu_i+\e_i, i=1,...,35
\end{multline}

$RSS_2$ --- это сумма квадратов остатков в модели:
\begin{multline}
\ln W_i=\gamma_1 +\gamma_2 Edu_i+\gamma_3 Exp_i+\gamma_4 Exp_i^2+\\
\gamma_5 Fedu_i+\gamma_6 Medu_i+\e_i, i=36,...,58
\end{multline}


\begin{enumerate}
\item Тестовая статистика:

\[
T = \frac{(RSS_R-RSS_{UR})/q}{RSS_{UR}/(n-m)},
\]

где $RSS_R$ -- сумма квадратов остатков в модели с ограничениями;

$RSS_{UR}$ -- сумма квадратов остатков в модели без ограничений;

$q$ -- число линейно независимых уравнений в основной гипотезе $H_0$;

$n$ -- общее число наблюдений;

$m$ -- число коэффициентов в модели без ограничений

\item Распределение тестовой статистики при верной $H_0$:

\[
T \sim F(q,n-m)
\]

\item Наблюдаемое значение тестовой статистики:

\[
T_{obs} = \frac{(70.3-(34.4+23.4))/6}{(34.4+23.4)/(58-12)}=1.66
\]

\item Область, в которой $H_0$ не отвергается:

\[
[0;T_{cr}]=[0;2.3]
\]

\item Статистический вывод:

Поскольку $T_{obs} \in [0;T_{cr}]$, то на основе имеющихся данных мы не можем отвергнуть гипотезу $H_0$ в пользу альтернативной $H_1$. Следовательно, имеющиеся данные не противоречат гипотезе об отсутствии дискриминации на рынке труда между мужчинами и женщинами.

\end{enumerate}
\end{solution}

\begin{problem}  % 3.4
 Рассмотрим следующую регрессионную модель зависимости логарифма заработной платы $\ln W$ от уровня образования $Edu$, опыта работы $Exp$, $Exp^2$:
\[
\widehat{\ln W}=\hat{\beta}_1+\hat{\beta}_2Edu+\hat{\beta}_3Exp+\hat{\beta}_4Exp^2
\]
Модель регрессии была отдельно оценена по выборкам из 20 мужчин и 20 женщин, и были получены остаточные суммы квадратов $RSS_1$ = 49.4 и $RSS_2$ = 44.1 соответственно. Остаточная сумма квадратов в регрессии, оценённой по объединённой выборке, равна 105.5. На уровне $ 5\%$ проверьте гипотезу об отсутствии дискриминации в оплате труда между мужчинами и женщинами.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}  % 3.5
 Ниже приведены результаты оценивания спроса на молоко для модели $y_i=\beta_1 + \beta_2 I_i + \beta_3 P_i + \e_i$, где $y_i$ -- стоимость молока, купленного $i$--ой семьёй за последние $7$ дней (в руб.), $I_i$ -- месячный доход $i$--ой семьи (в руб.), $P_i$ -- цена 1 литра молока (в руб.). Вычисления для общей выборки, состоящей из $2127$ семей, дали $RSS = 8841601$. Для двух подвыборок, состоящих из $348$ городских и $1779$ сельских семей, соответствующие суммы квадратов остатков оказались следующими: $RSS_1$ = 1720236 и $RSS_2$ = 7099423. Можно ли считать зависимость спроса на молоко от его цены и дохода единой для городской и сельской местности? Ответ обоснуйте подходящим тестом.
\end{problem}

\begin{solution}
Для ответа на вопрос задачи, а именно, можно или нет считать зависимость спроса на молоко от его цены и дохода единой для городской и сельской местностей, воспользуемся гипотезой о нескольких ограничениях. Тогда:
\begin{itemize}
\item Ограниченная (<<короткая>>) модель, то есть та модель, которая предполагает выполнение нулевой гипотезы, имеет вид :
\[
R: y_i = \beta_1 + \beta_2I_i + \beta_3P_i + \epsilon_i
\]
\[
RSS_R = RSS = 8841601
\]
\item Для того чтобы записать спецификацию неограниченной (<<длинной>>) модели, которая пердполагает разные $\beta_i$ для городской и сельской местностей, введем дополнительную переменную $d_i$, такую что:
\[
d_i=
\begin{cases}
1, \text{город;}\\
0, \text{село}\\
\end{cases}
\]
Пусть коэффициенты для городской местности отличаются на некоторое $\Delta_i$, тогда неограниченная модель имеет вид:
\[
UR: y_i = \beta_1+\Delta_1 d_i + (\beta_2+\Delta_2 d_i)I_i + (\beta_3+\Delta_3 d_i)P_i + \epsilon_i
\]
\[
RSS_{UR} = RSS_1 + RSS_2 = 1720236 + 7099423 = 8819659
\]
\item Гипотезы:
\[
H_0=
\begin{cases}
\Delta_1=0\\
\Delta_2=0\\
\Delta_3=0\\
\end{cases} \;
H_a:\Delta_1^2+\Delta_2^2+\Delta_3^2>0 
\]
\item Тестовая статистика имеет вид:
\[ 
F = \frac{(RSS_R-RSS_{UR})/q}{RSS_{UR}/(n-m)}
\]
где $q$ --- число линейно независимых уранений в нулевой гипотезе $H_0$;\\
$n$ --- общее число наблюдений;\\
$m$ --- число коэффициентов в неограниченной модели
\item Распределение тестовой статистики при верной $H_0$:
\[
F_{\text{кр}}\sim(F_{\alpha,q,n-m})
\]
\item Расчётное значение тестовой статистики $F_{\text{рас}}=17.58$, $F_{\text{кр}}\approx 2.61$
\item Так как $F_{\text{рас}}>F_{\text{кр}}$ следовательно гипотеза $H_0$ --- отвергается.
\end{itemize}
Вывод: зависимость спроса на молоко от его цены и дохода  для городской и сельской местностей нельзя считать единой.
\end{solution}


\begin{problem} % 3.6
 По 52 наблюдениям была оценена следующая зависимость цены квадратного метра квартиры $Price$ (в долларах) от площади кухни $K$ (в квадратных метрах), времени в пути пешком до ближайшего метро $M$ (в минутах), расстояния до центра города $C$ (в км) и наличия рядом с домом лесопарковой зоны $P$ (1 --- есть, 0 --- нет).
\[
\underset{(s.e.)}{\widehat{Price}}=\underset{(3.73)}{16.12}+\underset{(0.14)}{1.7}K-\underset{(0.03)}{0.35}M-\underset{(0.12)}{0.46}C+\underset{(0.98)}{2.22}P
\]
\[
R^2=0.78, \sum_{i=1}^{52} {(Price_i-\overline{Price})^2}=278
\]
Предположим, что все квартиры в выборке можно отнести к двум категориям: квартиры на севере города (28 наблюдений) и квартиры на юге города (24 наблюдения). Модель регрессии была оценена отдельно только по квартирам на севере и только по квартирам на юге. Ниже приведены результаты оценивания.

Для квартир на севере:
\[
\underset{(s.e.)}{\widehat{Price}}=\underset{(3.3)}{14}+\underset{(0.23)}{1.6}K-\underset{(0.04)}{0.33}M-\underset{(0.22)}{0.4}C+\underset{(0.78)}{2.1}P, RSS=21.8
\]
Для квартир на юге:
\[
\underset{(s.e.)}{\widehat{Price}}=\underset{(3.9)}{16.8}+\underset{(0.4)}{1.62}K-\underset{(0.12)}{0.29}M-\underset{(0.23)}{0.51}C+\underset{(1.28)}{1.98}P, RSS=19.2
\]

На уровне значимости $5\%$ проверьте гипотезу о различии в ценообразовании квартир на севере и на юге.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem} % 3.7
 По 52 наблюдениям была оценена следующая зависимость цены квадратного метра квартиры $Price$ (в долларах) от площади кухни $K$ (в квадратных метрах), времени в пути пешком до ближайшего метро $M$ (в минутах), расстояния до центра города $C$ (в км) и наличия рядом с домом лесопарковой зоны $P$ (1 --- есть, 0 --- нет).
\[
\underset{(s.e.)}{\widehat{Price}}=\underset{(3.73)}{16.12}+\underset{(0.14)}{1.7}K-\underset{(0.03)}{0.35}M-\underset{(0.12)}{0.46}C+\underset{(0.98)}{2.22}P
\]
\[
R^2=0.78, \sum_{i=1}^{52} {(Price_i-\overline{Price})^2}=278
\]
Предположим, что все квартиры в выборке можно отнести к двум категориям: квартиры на севере города (28 наблюдений) и квартиры на юге города (24 наблюдения). Пусть
$S$ --- это фиктивная переменная, равная 1 для домов в южной части города и 0 для домов в северной части города. Используя эту переменную, была оценена следующая регрессия:
\begin{multline}
\underset{(s.e.)}{\widehat{Price}} =\underset{(3.13)}{14.12}+\underset{(0.11)}{0.25}S+\underset{(0.13)}{1.65}K+\underset{(0.14)}{0.17}K\cdot{S}-\underset{(0.039)}{0.37}M+\\
+\underset{(0.0012)}{0.05}M\cdot{S}-\underset{(0.13)}{0.44}C-\underset{(0.18)}{0.06}C\cdot{S}+\underset{(0.88)}{2.27}P-\underset{(0.08)}{0.23}P\cdot{S}
\end{multline}
\[
R^2 = 0.85
\]
На уровне значимости $5\%$ проверьте гипотезу о различии в ценообразовании квартир на севере и на юге.
\end{problem}

\begin{solution}
Задача решается аналогично предыдущем задачам, к примеру, 3.3, 3.5. 

Главное отличие заключается в том, что вместо значений $RSS_{R}$ и $RSS_{UR}$ даются значения соответствующих $R^2$, также следует вспомнить, что $\sum_{i=1}^{n=52}(Price_i-\overline{Price})^2=278$ ни что иное, как $TSS$, которое, в свою очередь, не зависит от спецификации модели, то есть $TSS_R=TSS_{UR}=TSS$. Тогда можно выразить$RSS$ моделей:
\[
\begin{cases}
R^2=\frac{ESS}{TSS} \\
TSS=ESS+RSS
\end{cases}
\to
\begin{cases}
RSS_R=TSS(1-R^2_R)=278(1-0.78)\approx 61.16\\ 
RSS_{UR}=TSS(1-R^2_{UR})=278(1-0.85)\approx 41.7
\end{cases} 
\]

Находим расчётное значение $F$-статистики
\[
F_{\text{рас}}=\frac{(61.16-41.7)/5}{41.7/(52-10)}\approx 3.92
\]

Находим критическое значение $F$-статистики
\[
F_{\text{кр}}\sim F_{0.05,5,42}\approx 2.44
\]

Получаем, что $F_{\text{рас}}>F_{\text{кр}}$, и, следовательно, $H_0$ отвергается в пользу альтернативной гипотезы на уровне значимости 5\%.


Вывод: гипотеза об одинаковом ценообразовании квартир на севере и на юге отвергается на уровне значимости 5\%.
\end{solution}


\begin{problem} % 3.8
 На основе квартальных данных с 2003 по 2008 год было получено следующее уравнение регрессии, описывающее зависимость цены на товар P от нескольких факторов:
\[
P=3.5+0.4X+1.1W, ESS=70.4, RSS=40.5
\]
Когда в уравнение были добавлены фиктивные переменные, соответствующие первым
трем кварталам года $Q_1, Q_2, Q_3$, оцениваемая модель приобрела вид:
\[
P_t=\beta+\beta_X X_t+\beta_W W_t+\beta_{Q_{1t}} Q_{1t}+\beta_{Q_{2t}} Q_{2t}+\beta_{Q_{3t}} Q_{3t}+\e_t
\]
При этом величина $ESS$ выросла до 86.4. Сформулируйте и на уровне значимости $5\%$ проверьте гипотезу о наличии сезонности.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem} % 3.9
 Рассмотрим следующую функцию спроса с сезонными переменными $SPRING$ (весна), $SUMMER$ (лето), $FALL$ (осень):
\[
\widehat{\ln Q}=\hat{\beta}_1+\hat{\beta}_2\cdot{\ln P}+\hat{\beta}_3\cdot{SPRING}+\hat{\beta}_4\cdot{SUMMER}+\hat{\beta}_5\cdot{FALL}
\]
\[
R^2=0.37,n=20
\]
Напишите спецификацию регрессии с ограничениями для проверки статистической гипотезы $H_0: \beta_3 = \beta_5$. Дайте интерпретацию проверяемой гипотезе. Пусть для регрессии с ограничениями был вычислен коэффициент $R_{R}^2=0.23$. На уровне значимости $5\%$ проверьте нулевую гипотезу.
\end{problem}

\begin{solution}
Спецификация модели : 
\[
\widehat{\ln{Q}} = \hb_1+\hb_2 \ln{P}+\hb_3(SPRING+SUMMER)+\hb_5 FALL 
\]
Интерпретация: осень так же влияет на логарифм величины спросы, как и весна. Задача решается аналогично задачам 3.7, 3.5
\[
\begin{cases}
R^2=\frac{ESS}{TSS}\\
TSS&=ESS+RSS\\
TSS_R=TSS_{UR}=TSS\\
\end{cases}
\]

Находим расчётное и наблюдаемое значение $F$-статистики
\[
\begin{cases}
F_{\text{рас}}=\frac{(R_{UR}^2-R_{R}^2)/q}{(1-R^2_{UR})/(n-m)}\approx3.3\\
F_{\text{кр}}= F_{0.05,1,15}\approx 4.54
\end{cases}
\]

Следовательно, $F_{\text{рас}}<F_{\text{кр}}$ и $H_0$ не отвергается на уровне значимости 5\%.

Вывод:гипотеза $H_0$ о равном влиянии осени и весны на логарифм спроса не отвергается на уровне значимости 5\% .
\end{solution}


\begin{problem} % 3.10
 Рассмотрим следующую функцию спроса с сезонными переменными $SPRING$ (весна), $SUMMER$ (лето), $FALL$ (осень):
\[
\widehat{\ln Q}=\hat{\beta}_1+\hat{\beta}_2\cdot{\ln P}+\hat{\beta}_3\cdot{SPRING}+\hat{\beta}_4\cdot{SUMMER}+\hat{\beta}_5\cdot{FALL}
\]
\[
R^2=0.24,n=24
\]
Напишите спецификацию регрессии с ограничениями для проверки статистической гипотезы 
$H_0: 
  \begin{cases}
    \beta_3=0, \\
    \beta_4=\beta_5 
 \end{cases}.$
Дайте интерпретацию проверяемой гипотезе. Пусть для регрессии с ограничениями был вычислен коэффициент $R_{R}^2=0.13$. На уровне значимости $5\%$ проверьте нулевую гипотезу.
\end{problem}

\begin{solution}
Смысл гипотезы: летом и осенью одинаковая зависимость и одинаковая зависимость зимой и весной. Ограниченная модель: $\widehat{\ln Q}=\hat{\beta}_1+\hat{\beta}_2\cdot{\ln P}+\hat{\beta}_3 d$, где $d$ равна 1 для лета и осени. Наблюдаемое значение статистики $F_{obs}=1.375$, критическое, $F_{cr}=3.5218933$. Гипотеза не отвергается.
\end{solution}


\begin{problem}
 Исследователь собирается по выборке, содержащей данные за 2 года,
построить модель линейной регрессии с константой и 3-мя объясняющими переменными. В модель предполагается ввести 3 фиктивные сезонные переменные $SPRING$ (весна), $SUMMER$ (лето) и $FALL$ (осень) на все коэффициенты регрессии. Однако в процессе оценивания статистический пакет вывел на экран компьютера следующее сообщение <<insufficient number of observations>>. Объясните, почему имеющегося числа наблюдений не хватило для оценивания параметров модели.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 По данным для 57 индивидов оценили зависимость длительности обучения индивида $S$ от способностей индивида, описываемых обобщённой переменной $IQ$, и пола индивида, описываемого с помощью фиктивной переменной $MALE$ (равной 1 для мужчин и 0 для женщин), с помощью двух регрессий (в скобках под коэффициентами указаны оценки стандартных отклонений):
\[
\underset{(s.e.)}{\hat{S}}=\underset{(0.44)}{6.12}+\underset{(0.088)}{0.147}\cdot{IQ}, RSS=2758.6
\]
\begin{multline}
\underset{(s.e.)}{\hat{S}}=\underset{(0.73)}{6.12}+\underset{(0.014)}{0.147}\cdot{IQ}-\underset{(0.933)}{1.035}\cdot{MALE}+\underset{(0.018)}{0.0166}\cdot{(MALE}\cdot{IQ)}
\end{multline}
Во второй регрессии сумма квадратов остатков равна $RSS=2090.98$
Зависит ли длительность обучения от пола индивида и почему?
\end{problem}

\begin{solution} 
\end{solution}


\begin{problem}
 По данным, содержащим 30 наблюдений, построена регрессия:
\[
\hat{y}=1.3870+5.2587\cdot{x}+2.6259\cdot{d}+2.5955 \cdot{x} \cdot{d},
\]
где фиктивная переменная $d$ определяется следующим образом:
\[
d_i =
  \begin{cases}
    1 & \text{при $i$ $\in \bigl\{ 1,\dots,20 \bigr\} $}, \\
    0 & \text{при $i$ $\in \bigl\{ 21,\dots,30 \bigr\} $}. 
 \end{cases}
\]
Найдите оценки коэффициентов в модели $y_i=\beta_1+\beta_2 x_i+\e_i$, построенной по первым 20-ти наблюдениям, т.е. при $i \in \bigl\{1,\dots,20 \bigr\}$.
\end{problem}

\begin{solution}
$\hb_1=1.3870+2.6259=4.0129$, $\hb_2=5.2587+2.5955=7.8542$
\end{solution}


\begin{problem} % 3.14
 Выборка содержит 30 наблюдений зависимой переменной $y$ и независимой переменной $x$. Ниже приведены результаты оценивания уравнения регрессии $y_i=\beta_1+\beta_2 x_i+\e_i$ по первым 20-ти и последним 10-ти наблюдениям соответственно:
\[
\hat{y}=4.0039+2.6632\cdot{x}
\]
\[
\hat{y}=1.3780+5.2587\cdot{x}
\]
По имеющимся данным найдите оценки коэффициентов  модели, рассчитанной по 30-ти наблюдениям $y_i=\beta_1+\beta_2 x_i+\Delta{\beta_1} \cdot{d_i} + \Delta{\beta_2} \cdot{x_i} \cdot{d_i}+\e_i$, где фиктивная переменная $d$ определяется следующим образом:
\[
d_i =
  \begin{cases}
    1 & \text{при } i \in \bigl\{ 1,\dots,20 \bigr\} , \\
    0 & \text{при } i \in \bigl\{ 21,\dots,30 \bigr\} . 
 \end{cases}
\]
\end{problem}

\begin{solution}
$y_i=\beta_1+\beta_2(x_{i1}+x_{i2}+x_{i3})+\epsilon_{i}$
\end{solution}


\begin{problem} % 3.15
 Пусть регрессионная модель имеет вид $y_i=\beta_1+\beta_2 x_{i1}+\beta_3 x_{i2}+\beta_4 x_{i3}+\e_i, i=1,\dots,n.$ Тестируемая гипотеза $H_0: \beta_2=\beta_3=\beta_4.$ Запишите, какой вид имеет модель <<с ограничением>> для тестирования указанной гипотезы.
\end{problem}

\begin{solution}
$y_i=\beta_1+\beta_2(x_{i1}+x_{i2}+x_{i3})+\epsilon_{i}$
\end{solution}


\begin{problem}
 Пусть регрессионная модель имеет вид $y_i= \beta_1+ \beta_2 x_{i1}+ \beta_3 x_{i2}+ \beta_4 x_{i3}+\e_i, i=1,\dots,n.$ Тестируемая гипотеза $H_0: \beta_3= \beta_4=1.$ Какая модель из приведённых ниже может выступать в качестве модели <<с ограничением>> для тестирования указанной гипотезы? Если ни одна из них, то запишите свою.
\begin{enumerate}
\item $y_i-(x_{i2}+x_{i3}) = \beta_1+ \beta_2 x_{i1}+ \e_i$
\item $y_i+(x_{i2}-x_{i3}) = \beta_1+ \beta_2 x_{i1}+ \e_i$
\item $y_i+x_{i2}+x_{i3} = \beta_1+ \beta_2 x_{i1}+ \e_i$
\item $y_i= \beta_1+ \beta_2 x_{i1}+ \beta_3 + \beta_4 + \e_i$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 Пусть регрессионная модель имеет вид $y_i= \beta_1+ \beta_2 x_{i1}+ \beta_3 x_{i2}+ \beta_4 x_{i3}+\e_i, i=1,\dots,n.$ Тестируемая гипотеза 
$H_0: 
  \begin{cases}
    \beta_2+ \beta_3+ \beta_4=1, \\
    \beta_3+ \beta_4=0. 
 \end{cases}$
Какая модель из приведённых ниже может выступать в качестве модели <<с ограничением>> для тестирования указанной гипотезы? Если ни одна из них, то запишите свою.
\begin{enumerate}
\item $y_i-x_{i1} = \beta_1+ \beta_3 (x_{i2}-x_{i3})+ \e_i$
\item $y_i-x_{i1} = \beta_1+ \beta_4 (x_{i3}-x_{i2})+ \e_i$
\item $y_i+x_{i1} = \beta_1+ \beta_3 (x_{i2}+x_{i3})+ \e_i$
\item $y_i+x_{i1} = \beta_1+ \beta_3 (x_{i2}-x_{i3})+ \e_i$
\end{enumerate}
\end{problem}

\begin{solution}
1,2
\end{solution}


\begin{problem} % 3.18
 Пусть регрессионная модель имеет вид $y_i= \beta_1+ \beta_2 x_{i1}+ \beta_3 x_{i2}+ \beta_4 x_{i3}+\e_i, i=1,\dots,n.$ Тестируемая гипотеза 
$H_0:
  \begin{cases}
    \beta_2 - \beta_3=0, \\
    \beta_3 + \beta_4=0. 
 \end{cases}$
Какая модель из приведённых ниже может выступать в качестве модели <<с ограничением>> для тестирования указанной гипотезы? Если ни одна из них, то запишите свою.
\begin{enumerate}
\item $y_i = \beta_1 + \beta_3 (x_{i2}-x_{i1}-x_{i3})+ \e_i$
\item $y_i-x_{i1} = \beta_1+ \beta_4 (x_{i3}-x_{i2})+ \e_i$
\item $y_i = \beta_1+ \beta_3 (x_{i1}+x_{i2}+x_{i3})+ \e_i$
\item $y_i = \beta_1+ \beta_3 (x_{i1}+x_{i2}-x_{i3})+ \e_i$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem} % 3.19
  Известно, что $P$-значение для коэффициента регрессии равно 0.087, а уровень значимости 0.1. Является ли значимым данный коэффициент в регрессии?
\end{problem}

\begin{solution}
значим
\end{solution}


\begin{problem} % 3.20
 Известно, что $P$-значение для коэффициента регрессии равно 0.078, а уровень значимости 0.05. Является ли значимым данный коэффициент в регрессии?
\end{problem}

\begin{solution}
не значим
\end{solution}


\begin{problem}
  Известно, что $P$-значение для коэффициента регрессии равно 0.09. На каком
уровне значимости данный коэффициент в регрессии будет признан значимым?
\end{problem}

\begin{solution}
$\alpha>0.09$
\end{solution}


\begin{problem}
 Ниже приведены результаты оценивания уравнения линейной регрессии зависимости количества смертей в автомобильных катастрофах от различных характеристик:
\[
deaths_i = \beta_1 + \beta_2 drivers_i + \beta_3 popden_i + \beta_4  temp + \beta_5 fuel + \e_i
\]
\ensuremath{\widehat{deaths}_i=-\underset{(222.8803)}{27.1}+\underset{(  0.3767)}{4.64}\cdot drivers_i-\underset{(  0.0239)}{0.0228}\cdot popden_i+\underset{(  4.6016)}{5.3}\cdot temp_i-\underset{(  0.8679)}{0.663}\cdot fuel_i}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:02 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & St.Error & t value & P-value \\ 
  \hline
Intercept & -27.10 & 222.88 & -0.12 & 0.90 \\ 
  Drivers & 4.64 & 0.38 & 12.30 & 0.00 \\ 
  Popden & -0.02 & 0.02 & -0.95 & 0.35 \\ 
  Temp & 5.30 & 4.60 & 1.15 & 0.26 \\ 
  Fuel & -0.66 & 0.87 & -0.76 & 0.45 \\ 
   \hline
\end{tabular}
\end{table}

Перечислите, какие из переменных в регрессии являются значимыми и на каком уровне значимости.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 Была оценена функция Кобба-Дугласа с учётом человеческого капитала $H$ ($K$ --- физический капитал, $L$ --- труд): 
\[ 
\widehat{\ln Q} = 1.4 + 0.46\ln L + 0.27\ln H + 0.23\ln K
\] 
\[
ESS = 170.4, RSS = 80.3, n = 21
\]
\begin{enumerate}
\item Чему равен коэффициент $R^2$?
\item На уровне значимости $1\%$ проверьте гипотезу о значимости регрессии <<в целом>>
\end{enumerate}
\end{problem}

\begin{solution}
Из формул
\[
\begin{cases}
R^2=\frac{ESS}{TSS}\\
TSS=ESS+RSS\\
\end{cases}
\]
получаем $R^2=\frac{170.4}{(170.4+80.3)}\approx=0.68$

Тестируемые гипотезы:
\[
H_0=
\begin{cases}
\beta_2=0\\
\beta_3=0\\
\beta_4=0\\
\end{cases}
\;
H_a:\beta_2^2+\beta_3^2+\beta_4^2>0 
\]

Так как по условию задачи проверяем значимость модели в целом, следовательно ограниченная модель --- регрессия на константу, таким образом:
\[
\begin{cases}
\widehat{y_i}=\overline{y}\\
RSS_{R}=\sum_{i=1}^{n}(y_i-\widehat{y_i})^2=\sum_{i=1}^{n}(y_i-\overline{y_i})^2=TSS\\
RSS_{UR}=TSS(1-R^2_{UR})\\
TSS_{UR}=TSS_{R}=TSS\\
\end{cases}
\]

Получаем, $F_{\text{рас}}=\frac{R_{UR}^2/q}{(1-R^2_{UR})/(n-m)}$

Значения статистик:
\[
\begin{cases}
F_{\text{рас}}\approx 12.04\\
F_{\text{кр}}=F_(0.01,3,17)\approx 5.185
\end{cases}
\]

Отсюда, 
$F_{\text{рас}}>F_{\text{кр}}$, и  $H_0$ отвергается на уровне значимости 1\%.

Вывод: гипотеза $H_0$ отвергается на уровне значимости 1\%, 
следовательно модель <<в целом>> значима.
\end{solution}




\begin{problem}
 На основе опроса 25 человек была оценена следующая модель зависимости логарифма зарплаты $\ln W$ от уровня образования $Edu$ (в годах) и возраста $Age$: 
\[
\widehat{\ln W} = 1.7 + 0.5Edu + 0.06Age - 0.0004Age^2
\]
\[
ESS = 90.3, RSS = 60.4
\]
Когда в модель были введены переменные $Fedu$ и $Medu$, учитывающие уровень образования родителей, величина $ESS$ увеличилась до $110.3.$
\begin{enumerate}
\item Напишите спецификацию уравнения регрессии с учётом образования родителей
\item Сформулируйте и на уровне значимости $5\%$ проверьте гипотезу о значимом влиянии уровня образования родителей на заработную плату:
\begin{enumerate}
\item Сформулируйте гипотезу
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\end{enumerate}
\end{problem}

\begin{solution}

Ограниченная модель (Restricted model):
\[
\ln W_i = \beta + \beta_{Edu}Edu_i + \beta_{Age}Age_i + \beta_{Age^2}Age^2_i + \e_i
\]
Неограниченная модель (Unrestricted model):
\begin{multline}
\ln W_i = \beta + \beta_{Edu}Edu_i + \beta_{Age}Age_i + \beta_{Age^2}Age^2_i + \\
\beta_{Fedu}Fedu_i + \beta_{Medu}Medu_i + \e_i
\end{multline}

По условию $ESS_R = 90.3$, $RSS_R = 60.4$, $TSS = ESS_R + RSS_R = 90.3 + 60.4 = 150.7.$ Также сказано, что $ESS_{UR} = 110.3$. Значит, $RSS_{UR} = TSS - ESS_{UR} = 150.7 - 110.3 = 40.4$
\begin{enumerate}
\item Cпецификация:
\begin{multline}
\ln W_i = \beta + \beta_{Edu}Edu_i + \beta_{Age}Age_i + \beta_{Age^2}Age^2_i + \\
\beta_{Fedu}Fedu_i + \beta_{Medu}Medu_i + \e_i
\end{multline}
\item Проверка гипотезы
\begin{enumerate}
\item $H_0: \begin{cases} \beta_{Fedu} = 0  \\  \beta_{Medu} = 0 \end{cases}$
$H_a: |\beta_{Fedu}| + |\beta_{Medu}| > 0$
\item $T = \frac{(RSS_{R} - RSS_{UR})/q}{RSS_{UR}/(n - k)}$, где $q = 2$ --- число линейно независимых уравнений в основной гипотезе $H_0$, $n = 25$ --- число наблюдений, $k = 6$ --- число коэффициентов в модели без ограничения
\item $T \sim F(q; n - k)$
\item $T_{obs} = \frac{(RSS_{R} - RSS_{UR})/q}{RSS_{UR}/(n - k)} = \frac{(60.4 - 40.4)/2}{40.4/(25 - 6)} = 4.70$
\item Нижняя граница $= 0$, верхняя граница $= 3.52$
\item Поскольку $T_{obs} = 4.70$, что не принадлежит промежутку от $0$ до $3.52$, то на основе имеющихся данных можно отвергнуть основную гипотезу на уровне значимости $5\%$. Таким образом, образование родителей существенно влияет на заработную плату.
\end{enumerate}
\end{enumerate}

\end{solution}



\begin{problem}
 Рассмотрим следующую модель зависимости цены дома $Price$ (в тысячах долларов) от его площади $Hsize$ (в квадратных метрах), площади участка $Lsize$ (в квадратных метрах), числа ванных комнат $Bath$ и числа спален $BDR$:
\[
\widehat{Price} = \hat{\beta}_1 + \hat{\beta}_2 Hsize + \hat{\beta}_3 Lsize + \hat{\beta}_4 Bath + \hat{\beta}_5 BDR
\]
\[
R^2 = 0.218, n = 23
\]
Напишите спецификацию регрессии с ограничениями для проверки статистистической гипотезы $H_0: \beta_3 = 20\beta_4$. Дайте интерпретацию проверяемой гипотезе. Для регрессии с ограничением был вычислен коэффициент $R_{R}^2 = 0.136$. На уровне значимости $5\%$ проверьте нулевую гипотезу.
\end{problem}

\begin{solution}
\[
\widehat{Price}= \hb_1+\hb_2 Hsize+20\hb_4 Lsize+\hb_4 Bath + \hb_5 BDR
\]

Размер участка в 20 раз сильнее влияет на цену дома, чем число ванных комнат.
\[
\begin{cases}
R^2=\frac{ESS}{TSS}\\
TSS=ESS+RSS\\
TSS_R=TSS_{UR}=TSS\\
\end{cases}
\]

\[
\begin{cases}
RSS_{R}=TSS(1-R^2_{R})\\
RSS_{UR}=TSS(1-R^2_{UR})\\
\end{cases} \to
\]

\[
\begin{cases}
F_{\text{рас}}=\frac{(R_{UR}^2-R_{R}^2)/q}{(1-R^2_{UR})/(n-m)}=\frac{(0.218-0.136)/1}{(1-0.218)/15}\approx 1.573\\
F_{\text{кр}}= F_{0.05,1,15}\approx 4.54
\end{cases}
\]

$F_{\text{рас}}<F_{\text{кр}}$ и, следовательно, $H_0$ не отвергается на уровне значимости 5\%.

Вывод: гипотеза $H_0$ о том, что размер участка в 20 раз сильнее влияет на цену дома, чем число ванных комнат, не отвергается на уровне значимости 5\%.
\end{solution}


\begin{problem}
 Рассмотрим следующую модель зависимости почасовой оплаты труда $W$ от уровня образования $Educ$, возраста $Age$, уровня образования родителей $Fathedu$ и $Mothedu$:
\[
\widehat{\ln W} = \hat{\beta}_1 + \hat{\beta}_2 Educ + \hat{\beta}_3 Age + \hat{\beta}_4 Age^2+ \hat{\beta}_5 Fathedu + \hat{\beta}_6 Mothedu
\]
\[
R^2 = 0.341, n = 27
\]
Напишите спецификацию регрессии с ограничениями для проверки статистистической гипотезы $H_0: \beta_5 = 2\beta_4$. Дайте интерпретацию проверяемой гипотезе. Для регрессии с ограничением был вычислен коэффициент $R_{R}^2 = 0.296$. На уровне значимости $5\%$ проверьте нулевую гипотезу.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem} % 3.27
 По данным для 27 фирм исследователь оценил зависимость объёма выпуска $y$ от труда $l$ и капитала $k$ с помощью двух моделей:
\[
\ln y_i = \beta_1 + \beta_2 \ln l_i + \beta_3 \ln k_i + \e_i
\]
\[
\ln y_i = \beta_1 + \beta_2 \ln (l_i \cdot k_i) + \e_i
\]
Он получил для этих двух моделей суммы квадратов остатков $RSS_1 = 0.851$ и $RSS_2 = 0.894$ соответственно. Сформулируйте гипотезу, которую хотел проверить исследователь. На уровне значимости $5\%$ проверьте эту гипотезу и дайте экономическую интерпретацию.
\end{problem}

\begin{solution}
$H_0: \beta_2=\beta_3$ --- труд и капитал вносят одинаковый вклад в выпуск фирмы.

\[
\begin{cases}
F_{\text{рас}}=\frac{(RSS_R-RSS_{UR})/q}{RSS_{UR}/(n-m)}=\frac{(0.894-0.851)/1}{0.851/(27-3)}\approx 1.213\\
F_{\text{кр}}= F_{0.05,1,24}\approx 4.26
\end{cases}
\]

Получаем, что $F_{\text{рас}}<F_{\text{кр}}$, и, следовательно, $H_0$ не отвергается на уровне значимости 5\%

Вывод: гипотеза $H_0$, предполагающая, что труд и капитал вносят одинаковый вклад в выпуск фирмы, не отвергается на уровне значимости 5\%.
\end{solution}


\begin{problem} % 3.28
 Пусть задана линейная регрессионная модель:
\[
y_i = \beta_1 + \beta_2 x_{1i} + \beta_3 x_{2i} + \beta_4 x_{3i} + \beta_5 x_{4i} + \e_i, i = 1, \dots, 20
\]
По имеющимся данным оценены следующие регрессии:
\[
\underset{(s.e.)}{\hat{y_i}} = \underset{(0.15)}{10.01} + \underset{(0.06)}{1.05}x_1 + \underset{(0.04)}{2.06}x_2 + \underset{(0.06)}{0.49}x_3 - \underset{(0.06)}{1.31}x_4, RSS = 6.85
\]
\[
\underset{(s.e.)}{\widehat{y_i- x_1 - 2x_2}} = \underset{(0.15)}{10.00} + \underset{(0.07)}{0.50}x_3 - \underset{(0.06)}{1.32}x_4, RSS = 8.31 
\]
\[
\underset{(s.e.)}{\widehat{y_i + x_1 + 2x_2}} = \underset{(3.62)}{9.93} + \underset{(1.48)}{0.56}x_3 - \underset{(1.42)}{1.50}x_4, RSS = 4310.62 
\]
\[
\underset{(s.e.)}{\widehat{y_i - x_1 + 2x_2}} = \underset{(3.26)}{10.71} + \underset{(1.33)}{0.09}x_3 - \underset{(1.28)}{1.28}x_4, RSS = 3496.85
\]
\[
\underset{(s.e.)}{\widehat{y_i + x_1 - 2x_2}} = \underset{(1.25)}{9.22} + \underset{(0.51)}{0.97}x_3 - \underset{(0.49)}{1.54}x_4, RSS = 516.23
\]
На уровне значимости $5\%$ проверьте гипотезу $H_0: \begin{cases} \beta_2 = 1 \\ \beta_3 = 2 \end{cases}$ против альтернативной гипотезы $H_a: |\beta_2 - 1| + |\beta_3 - 2| \not= 0$.
\end{problem}

\begin{solution}
\end{solution}




\begin{problem} % 3.29
 Рассмотрим следующую модель зависимости расходов на образование на душу населения от дохода на душу населения, доли населения в возрасте до 18 лет, а также доли городского населения: 
\[
education_i = \beta_1 + \beta_2 income_i + \beta_3 young_i + \beta_4 urban_i + \e_i
\]
Ниже приведены результаты оценивания уравнения этой линейной регрессии:
\begin{center}
\ensuremath{\widehat{education}_i=-\underset{(64.9199)}{287}+\underset{( 0.0093)}{0.0807}\cdot income_i+\underset{( 0.1598)}{0.817}\cdot young_i-\underset{( 0.0343)}{0.106}\cdot urban_i}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:02 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & St.Error & t value & P-value \\ 
  \hline
Intercept & -286.84 & 64.92 & -4.42 & 0.00 \\ 
  Income & 0.08 & 0.01 & 8.67 & 0.00 \\ 
  Young & 0.82 & 0.16 & 5.12 & 0.00 \\ 
  Urban & -0.11 & 0.03 & -3.09 & 0.00 \\ 
   \hline
\end{tabular}
\end{table}

\end{center}

Модель оценивается по 51 наблюдению.

\begin{enumerate}
\item Сформулируйте основную и альтернативую гипотезы, которые соответствуют тесту на значимость коэффициента при переменной доход на душу населения в уравнении регрессии
\item На уровне значимости $10\%$ проверьте гипотезу о значимости коэффициента при переменной доход на душу населения в уравнении регрессии:
\begin{enumerate}
\item Приведите формулу для тестовой статистики
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item На уровне значимости $5\%$ проверьте гипотезу $H_0: \beta_1 = 1$ против альтернативной $H_a: \beta_1 > 1$ :
\begin{enumerate}
\item Приведите формулу для тестовой статистики
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item Сформулируйте основную гипотезу, которая соответствует тесту на значимость регрессии <<в целом>>
\item На уровне значимости $1\%$ проверьте гипотезу о значимости регрессии <<в целом>>,если известно, что $F-$статистика равна $34.81$ со степенями свободы 3 и 47, $P-$значение равно $5.337e^{-12}$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики
\item Укажите распределение тестовой статистики  при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}

Далее приведены результаты оценивания уравнения регрессии без переменной, отражающей долю городского населения:
\begin{center}
\ensuremath{\widehat{education}_i=-\underset{(70.27134)}{301}+\underset{( 0.00741)}{0.0612}\cdot income_i+\underset{( 0.17327)}{0.836}\cdot young_i}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:02 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & St.Error & t value & P-value \\ 
  \hline
Intercept & -301.09 & 70.27 & -4.28 & 0.00 \\ 
  Income & 0.06 & 0.01 & 8.25 & 0.00 \\ 
  Young & 0.84 & 0.17 & 4.83 & 0.00 \\ 
   \hline
\end{tabular}
\end{table}

\end{center}
Также известно, что $RSS$ для первой модели равен $33489.35$, а для второй модели --- $40276.61$. На уровне значимости $5\%$ проверьте гипотезу $H_0: \beta_4 = 0$ против альтернативной $H_0: \beta_4 \not= 0$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики
\item Укажите распределение тестовой статистики  при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\end{enumerate}
\end{problem}

\begin{solution}
\begin{enumerate}
\item
\[
\begin{cases}
H_0: \beta_2=0\\
H_a: \beta_2\neq 0
\end{cases}
\]

\item 
\begin{enumerate}
\item[(a)] $t=\frac{\hb_i-\beta_i}{se(\beta_i)}$
\item[(b)] $t_{\alpha,n-m}=t_{0.05,47}$
\item[(c)] $t=\frac{0.08-0}{0.0093}\approx 8.67$
\item[(d)]$[-t_{\text{кр}},t_{\text{кр}}]$
\item[(e)] гипотеза $H_0$ отвергается, так как $P$-значение равно нулю; можно честно посчитать $t_{\text{кр}}=t_{0.05,47}$ или вспомнить, что при количестве наблюдений больше 30, $t$-распределение похоже на нормальное, для которого квантиль на уровне 5\% примерно равна $1.67$ и $F_{\text{наб}}>F_{\text{кр}}$. Гипотеза $H_0$ отвергается, следовательно коэффициент $\beta_2$ значим на уровне значимости 10\%.
\end{enumerate}

\item 
\begin{enumerate}
\item[(a)] $t=\frac{\hb_i-\beta_i}{se(\beta_i)}$
\item[(b)] $t_{\alpha,n-m}=t_{0.05,47}$
\item[(c)] $t=\frac{-287-1}{64.92}\approx -4.42$
\item[(d)]$(-\infty,t_{\text{кр}}]$
\item[(e)] гипотеза $H_0$ отвергается, так как $P$-значение равно нулю; аналогично 2(e) $t_{\text{кр}}=t_{0.05,47}\approx 1.67$ и $F_{\text{наб}}>F_{\text{кр}}$. Гипотеза $H_0$ отвергается, следовательно коэффициент $\beta_1$ значим на уровне значимости 5\%.
\end{enumerate}

\item 
\[
H_0=
\begin{cases}
\beta_2=0\\
\beta_3=0\\
\beta_4=0\\
\end{cases}
\;
H_a:\beta_2^2+\beta_3^2+\beta_4^2>0 
\]

\item 
\begin{enumerate}
\item[(a)] $F =\frac{(RSS_R-RSS_{UR})/q}{RSS_{UR}/(n-m)}$
\item[(b)] $F_{\alpha,q,n-m}=F_{0.01,3,47}$
\item[(c)] $F=34.81$
\item[(d)]$[0,F_{\text{кр}}]$
\item[(e)] гипотеза $H_0$ отвергается, так как P-value $\approx 0(5.337e^{-12})$; можно вычислить $F_{\text{кр}}=F_{0.01,3,47} \approx 4.23$. Следовательно, $F_{\text{наб}}>F_{\text{кр}}$ и $H_0$ отвергается, и регрессия <<в целом>> значима на уровне значимости 1\%.
\end{enumerate}

\item 
\begin{enumerate}
\item[(a)] $F =\frac{(RSS_R-RSS_{UR})/q}{RSS_{UR}/(n-m)}$
\item[(b)] $F_{\alpha,q,n-m}=F_{0.05,3,47}$
\item[(c)] $F\approx 9.525$
\item[(d)] $[0,F_{\text{кр}}]$
\item[(e)] гипотеза $H_0$ отвергается, так как $F_{\text{кр}}=F_{0.05,3,47} \approx 4.047$ и $F_{\text{наб}}>F_{\text{кр}}$, следовательно коэффициент $\beta_4$ значим на уровне значимости 5\%.
\end{enumerate}
\end{enumerate}
\end{solution}


\begin{problem}
 Рассмотрим следующую модель зависимости расходов на образование на душу населения от дохода на душу населения, доли населения в возрасте до 18 лет, а также доли городского населения: 
\[
education_i = \beta_1 + \beta_2 income_i + \beta_3 young_i + \beta_4 urban_i + \e_i
\]
Модель оценивается по 51 наблюдению. Ниже приведены результаты оценивания уравнения этой линейной регрессии:
\begin{center}
\ensuremath{\widehat{education}_i=-\underset{(64.9199)}{287}+\underset{( 0.0093)}{0.0807}\cdot income_i+\underset{( 0.1598)}{0.817}\cdot young_i-\underset{( 0.0343)}{0.106}\cdot urban_i}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:02 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & St.Error & t value & P-value \\ 
  \hline
Intercept & -286.84 & 64.92 & -4.42 & 0.00 \\ 
  Income & 0.08 & 0.01 & 8.67 & 0.00 \\ 
  Young & 0.82 & 0.16 & 5.12 & 0.00 \\ 
  Urban & -0.11 & 0.03 & -3.09 & 0.00 \\ 
   \hline
\end{tabular}
\end{table}

\end{center}

\begin{enumerate}
\item Сформулируйте основную и альтернативую гипотезы, которые соответствуют тесту на значимость коэффициента при переменной доля населения в возрасте до 18 лет в уравнении регрессии
\item На уровне значимости $10\%$ проверьте гипотезу о значимости коэффициента при переменной доля населения в возрасте до 18 лет в уравнении регрессии:
\begin{enumerate}
\item Приведите формулу для тестовой статистики
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}


Далее приведены результаты оценивания уравнения регрессии без переменной, отражающей долю населения в возрасте до 18 лет:
\begin{center}
\ensuremath{\widehat{education}_i=\underset{(27.3827)}{25.3}+\underset{( 0.0114)}{0.0762}\cdot income_i-\underset{( 0.0423)}{0.112}\cdot urban_i}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:02 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & St.Error & t value & P-value \\ 
  \hline
Intercept & 25.25 & 27.38 & 0.92 & 0.36 \\ 
  Income & 0.08 & 0.01 & 6.67 & 0.00 \\ 
  Urban & -0.11 & 0.04 & -2.66 & 0.01 \\ 
   \hline
\end{tabular}
\end{table}

\end{center}
Также известно, что $RSS$ для первой модели равен $33489.35$, а для второй модели --- $52132.29$. На уровне значимости $5\%$ проверьте гипотезу $H_0: \beta_3 = 0$ против альтернативной $H_0: \beta_3 \not= 0$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
 Вася построил регрессию оценки за первую контрольную работу на константу, рост и вес студента, $\widehat{kr1_i}=\hb_1 + \hb_2 height_i + \hb_3 weight_i$. Затем построил регрессию оценки за вторую контрольную работу на те же объясняющие переменные, $\widehat{kr2_i}=\hb_1' + \hb_2' height_i + \hb_3' weight_i$. Накопленная оценка считается по формуле $nak_i=0.25 \cdot kr1_i + 0.75 \cdot kr2_i$. Чему равны оценки коэффициентов в регрессии накопленной оценки на те же объясняющие переменные? Ответ обоснуйте.
\end{problem}
\begin{solution}
$0.25\hb_1+0.75\hb_1'$, $0.25\hb_2+0.75\hb_2'$ и $0.25\hb_3+0.75\hb_3'$
\end{solution}

\begin{problem}
 Истинная модель имеет вид $y_i=\beta x_i +\e_i$. Вася оценивает модель $\hy_i=\hb x_i$ по первой части выборки, получает $\hb_a$, по второй части выборки --- получает $\hb_b$ и по всей выборке --- $\hb_{tot}$. Как связаны между собой $\hb_{a}$, $\hb_{b}$, $\hb_{tot}$? Как связаны между собой дисперсии $\Var(\hb_a)$,  $\Var(\hb_b)$ и  $\Var(\hb_{tot})$?
\end{problem}

\begin{solution}
Сами оценки коэффициентов никак детерминистически не связаны, но при большом размере подвыборок примерно равны. А дисперсии связаны соотношением $\Var(\hb_a)^{-1}+\Var(\hb_b)^{-1}=\Var(\hb_{tot})^{-1}$ 
\end{solution}

\begin{problem}
 Сгенерируйте вектор $y$ из 300 независимых нормальных $N(10,1)$ случайных величин. Сгенерируйте 40 <<объясняющих>> переменных, по 300 наблюдений в каждой, каждое наблюдение --- независимая нормальная $N(5,1)$ случайная величина. Постройте регрессию $y$ на все 40 регрессоров и константу. 
\begin{enumerate}
\item Сколько регрессоров оказалось значимо на 5\% уровне?
\item Сколько регрессоров в среднем значимо на 5\% уровне?
\item Эконометрист Вовочка всегда использует следующий подход: строит регрессию зависимой переменной на все имеющиеся регрессоры, а затем выкидывает из модели те регрессоры, которые оказались незначимы. Прокомментируйте Вовочкин эконометрический подход.
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 Мы попытаемся понять, как введение в регрессию лишнего регрессора влияет на оценки уже имеющихся. В регрессии будет 100 наблюдений. Возьмем $\rho=0.5$. Сгенерим выборку совместных нормальных $x_i$ и $z_i$ с корреляцией $\rho$. Настоящий $y_i$ задаётся формулой $y_i=5+6x_i+\e_i$. Однако мы будем оценивать модель $\hy_i=\hb_1+\hb_2 x_i+\hb_3 z_i$.

\begin{enumerate}
\item Повторите указанный эксперимент 500 раз и постройте оценку для функции плотности $\hb_1$. 
\item Повторите указанный эксперимент 500 раз для каждого $\rho$ от $-1$ до $1$ с шагом в $0.05$. Каждый раз сохраняйте полученные 500 значений $\hb_1$. В осях $(\rho,\hb_1)$ постройте 95\%-ый предиктивный интервал для $\hb_1$. Прокомментируйте.
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
 Цель задачи --- оценить модель CAPM несколькими способами. 
\begin{enumerate}
\item Соберите подходящие данные для модели CAPM. Нужно найти три временных ряда: ряд цен любой акции, любой рыночный индекс, безрисковый актив. Переведите цены в доходности.
\item Постройте графики
\item Оцените модель CAPM без свободного члена по всем наборам данных. Прокомментируйте смысл оцененного коэффициента
\item Разбейте временной период на два участка и проверьте устойчивость коэффициента бета
\item Добавьте в классическую модель CAPM свободный член и оцените по всему набору данных. Какие выводы можно сделать? 
\item Методом максимального правдоподобия оцените модель с ошибкой измерения $R^m-R^0$, т.е.

истинная зависимость имеет вид 
\begin{equation}
(R^s-R^0)=\b_1+\b_2(R_m^*-R_0^*)+\e
\end{equation}
величины $R_m^*$ и $R_0^*$ не наблюдаемы, но 
\begin{equation}
R_m-R_0=R_m^*-R_0^*+u
\end{equation}

\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}







\begin{problem}
 По 47 наблюдениям оценивается зависимость доли мужчин занятых в сельском хозяйстве от уровня образованности и доли католического населения по Швейцарским кантонам в 1888 году.

\[Agriculture_i=\beta_1+\beta_2 Examination_i+\beta_3 Catholic_i+\varepsilon_i\]

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{h} \hlkwb{<-} \hlstd{swiss}
\hlstd{model1} \hlkwb{<-} \hlkwd{glm}\hlstd{(Agriculture}\hlopt{~}\hlstd{Examination}\hlopt{+}\hlstd{Catholic,}\hlkwc{data}\hlstd{=h)}
\hlstd{coef.t} \hlkwb{<-} \hlkwd{coeftest}\hlstd{(model1)}
\hlkwd{dimnames}\hlstd{(coef.t)[[}\hlnum{2}\hlstd{]]} \hlkwb{<-}
    \hlkwd{c}\hlstd{(}\hlstr{"Оценка"}\hlstd{,}\hlstr{"Ст. ошибка"}\hlstd{,}
    \hlstr{"t-статистика"}\hlstd{,} \hlstr{"P-значение"}\hlstd{)}
\hlstd{coef.t} \hlkwb{<-} \hlstd{coef.t[,}\hlopt{-}\hlnum{4}\hlstd{]}
\hlstd{coef.t[}\hlnum{1}\hlstd{,}\hlnum{1}\hlstd{]} \hlkwb{<-} \hlnum{NA}
\hlstd{coef.t[}\hlnum{2}\hlstd{,}\hlnum{2}\hlstd{]} \hlkwb{<-} \hlnum{NA}
\hlstd{coef.t[}\hlnum{3}\hlstd{,}\hlnum{3}\hlstd{]} \hlkwb{<-} \hlnum{NA}
\end{alltt}
\end{kframe}
\end{knitrout}



\begin{kframe}
\begin{alltt}
\hlkwd{xtable}\hlstd{(coef.t)}
\end{alltt}
\end{kframe}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:02 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
  \hline
 & Оценка & Ст. ошибка & t-статистика \\ 
  \hline
(Intercept) &  & 8.72 & 9.44 \\ 
  Examination & -1.94 &  & -5.08 \\ 
  Catholic & 0.01 & 0.07 &  \\ 
   \hline
\end{tabular}
\end{table}


\begin{enumerate}
\item Заполните пропуски в таблице
\item Укажите коэффициенты, значимые на 10\% уровне значимости.
\item Постройте 99\%-ый доверительный интервал для коэффициента при переменной Catholic 
\end{enumerate}

Набор данных доступен в пакете R:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{h} \hlkwb{<-} \hlstd{swiss}
\end{alltt}
\end{kframe}
\end{knitrout}
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
 Оценивается зависимость уровня фертильности всё тех же швейцарских кантонов в 1888 году от ряда показателей. В таблице представлены результаты оценивания двух моделей.

Модель 1: $Fertility_i=\beta_1+\beta_2 Agriculture_i+\beta_3 Education_i+\beta_4 Examination_i+\beta_5 Catholic_i+\varepsilon_i$

Модель 2: $Fertility_i=\gamma_1+\gamma_2 (Education_i+Examination_i)+\gamma_3 Catholic_i+u_i$

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{m1} \hlkwb{<-} \hlkwd{lm}\hlstd{(Fertility}\hlopt{~}\hlstd{Agriculture}\hlopt{+}\hlstd{Education}\hlopt{+}
  \hlstd{Examination}\hlopt{+}\hlstd{Catholic,}\hlkwc{data}\hlstd{=h)}
\hlstd{m2} \hlkwb{<-} \hlkwd{lm}\hlstd{(Fertility}\hlopt{~}\hlkwd{I}\hlstd{(Education}\hlopt{+}\hlstd{Examination)}\hlopt{+}\hlstd{Catholic,}
  \hlkwc{data}\hlstd{=h)}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{kframe}
\begin{alltt}
\hlkwd{texreg}\hlstd{(}\hlkwd{list}\hlstd{(m1,m2))}
\end{alltt}
\end{kframe}
\begin{table}
\begin{center}
\begin{tabular}{l c c }
\hline
                           & Model 1 & Model 2 \\
\hline
(Intercept)                & $91.06^{***}$ & $80.52^{***}$ \\
                           & $(6.95)$      & $(3.31)$      \\
Agriculture                & $-0.22^{**}$  &               \\
                           & $(0.07)$      &               \\
Education                  & $-0.96^{***}$ &               \\
                           & $(0.19)$      &               \\
Examination                & $-0.26$       &               \\
                           & $(0.27)$      &               \\
Catholic                   & $0.12^{**}$   & $0.07^{*}$    \\
                           & $(0.04)$      & $(0.03)$      \\
I(Education + Examination) &               & $-0.48^{***}$ \\
                           &               & $(0.08)$      \\
\hline
R$^2$                      & 0.65          & 0.55          \\
Adj. R$^2$                 & 0.62          & 0.53          \\
Num. obs.                  & 47            & 47            \\
\hline
\multicolumn{3}{l}{\scriptsize{$^{***}p<0.001$, $^{**}p<0.01$, $^*p<0.05$}}
\end{tabular}
\caption{Statistical models}
\label{table:coefficients}
\end{center}
\end{table}


Набор данных доступен в пакете R:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{h} \hlkwb{<-} \hlstd{swiss}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{enumerate}
\item Проверьте гипотезу о том, что коэффициент при $Education$ в модели 1 равен  $-0.5$.
\item На 5\% уровне значимости проверьте гипотезу о том, что переменные $Education$ и $Examination$ оказывают одинаковое влияние на $Fertility$.
\end{enumerate}
\end{problem}
 
\begin{solution}
\end{solution}



\begin{problem}
 По 2040 наблюдениям оценена модель зависимости стоимости квартиры в Москве (в 1000\$) от общего метража и метража жилой площади.

% данные загружаются одним махом вначале


\begin{kframe}
\begin{alltt}
\hlstd{model1} \hlkwb{<-} \hlkwd{lm}\hlstd{(price}\hlopt{~}\hlstd{totsp}\hlopt{+}\hlstd{livesp,}\hlkwc{data}\hlstd{=flats)}
\hlstd{report} \hlkwb{<-} \hlkwd{summary}\hlstd{(model1)}
\hlstd{coef.table} \hlkwb{<-} \hlstd{report}\hlopt{$}\hlstd{coefficients}
\hlkwd{rownames}\hlstd{(coef.table)} \hlkwb{<-}
  \hlkwd{c}\hlstd{(}\hlstr{"Константа"}\hlstd{,}\hlstr{"Общая площадь"}\hlstd{,} \hlstr{"Жилая площадь"}\hlstd{)}
\hlkwd{xtable}\hlstd{(coef.table)}
\end{alltt}
\end{kframe}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:03 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
Константа & -88.81 & 4.37 & -20.34 & 0.00 \\ 
  Общая площадь & 1.70 & 0.10 & 17.78 & 0.00 \\ 
  Жилая площадь & 1.99 & 0.18 & 10.89 & 0.00 \\ 
   \hline
\end{tabular}
\end{table}


Оценка ковариационной матрицы $\widehat{Var}(\hat{\beta})$ имеет вид
\begin{kframe}
\begin{alltt}
\hlstd{var.hat} \hlkwb{<-} \hlkwd{vcov}\hlstd{(model1)}
\hlkwd{xtable}\hlstd{(var.hat,} \hlkwc{digits}\hlstd{=}\hlnum{4}\hlstd{)}
\end{alltt}
\end{kframe}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:03 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
  \hline
 & (Intercept) & totsp & livesp \\ 
  \hline
(Intercept) & 19.0726 & 0.0315 & -0.4498 \\ 
  totsp & 0.0315 & 0.0091 & -0.0151 \\ 
  livesp & -0.4498 & -0.0151 & 0.0335 \\ 
   \hline
\end{tabular}
\end{table}


\begin{enumerate}
\item Проверьте $H_0$: $\beta_{totsp}=\beta_{livesp}$. В чём содержательный смысл этой гипотезы?
\item Постройте доверительный интервал дли $\beta_{totsp}-\beta_{livesp}$. В чём содержательный смысл этого доверительного интервала?
\end{enumerate}
\end{problem}


\begin{solution}



Из оценки ковариационной матрицы находим, что $se(\hb_{totsp}=\hb_{livesp})=0.269606$.

Исходя из $Z_{crit}=1.96$ получаем доверительный интервал, $[\ensuremath{-0.822083};0.2347725]$.

Вывод: при уровне значимости 5\% гипотеза о равенстве коэффициентов не отвергается.
\end{solution}





\begin{problem}
 По 2040 наблюдениям оценена модель зависимости стоимости квартиры в Москве (в 1000\$) от общего метража и метража жилой площади.
\begin{kframe}
\begin{alltt}
\hlstd{model1} \hlkwb{<-} \hlkwd{lm}\hlstd{(price}\hlopt{~}\hlstd{totsp}\hlopt{+}\hlstd{livesp,}\hlkwc{data}\hlstd{=flats)}
\hlstd{report} \hlkwb{<-} \hlkwd{summary}\hlstd{(model1)}
\hlstd{coef.table} \hlkwb{<-} \hlstd{report}\hlopt{$}\hlstd{coefficients}
\hlkwd{rownames}\hlstd{(coef.table)} \hlkwb{<-}
  \hlkwd{c}\hlstd{(}\hlstr{"Константа"}\hlstd{,}\hlstr{"Общая площадь"}\hlstd{,} \hlstr{"Жилая площадь"}\hlstd{)}
\hlkwd{xtable}\hlstd{(coef.table)}
\end{alltt}
\end{kframe}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:03 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
Константа & -88.81 & 4.37 & -20.34 & 0.00 \\ 
  Общая площадь & 1.70 & 0.10 & 17.78 & 0.00 \\ 
  Жилая площадь & 1.99 & 0.18 & 10.89 & 0.00 \\ 
   \hline
\end{tabular}
\end{table}


Сумма квадратов остатков равна $RSS=\ensuremath{2.2216891\times 10^{6}}$. Оценка ковариационной матрицы $\widehat{Var}(\hat{\beta})$ имеет вид
\begin{kframe}
\begin{alltt}
\hlkwd{xtable}\hlstd{(}\hlkwd{vcov}\hlstd{(model1),} \hlkwc{digits}\hlstd{=}\hlnum{4}\hlstd{)}
\end{alltt}
\end{kframe}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:03 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
  \hline
 & (Intercept) & totsp & livesp \\ 
  \hline
(Intercept) & 19.0726 & 0.0315 & -0.4498 \\ 
  totsp & 0.0315 & 0.0091 & -0.0151 \\ 
  livesp & -0.4498 & -0.0151 & 0.0335 \\ 
   \hline
\end{tabular}
\end{table}


\begin{enumerate}
\item Постройте 95\%-ый доверительный интервал для ожидаемой стоимости квартиры с жилой площадью $30$ м$^2$ и общей площадью $60$ м$^2$.
\item Постройте 95\%-ый прогнозный интервал для фактической стоимости квартиры с жилой площадью $30$ м$^2$ и общей площадью $60$ м$^2$.
\end{enumerate}
\end{problem}
 
\begin{solution}
\end{solution}



\begin{problem}
 По 2040 наблюдениям оценена модель зависимости стоимости квартиры в Москве (в 1000\$) от общего метража, метража жилой площади и дамми-переменной, равной 1 для кирпичных домов.
\begin{kframe}
\begin{alltt}
\hlstd{model1} \hlkwb{<-} \hlkwd{lm}\hlstd{(price}\hlopt{~}\hlstd{totsp}\hlopt{+}\hlstd{livesp}\hlopt{+}\hlstd{brick}\hlopt{+}\hlstd{brick}\hlopt{:}\hlstd{totsp}\hlopt{+}
  \hlstd{brick}\hlopt{:}\hlstd{livesp,}\hlkwc{data}\hlstd{=flats)}
\hlstd{report} \hlkwb{<-} \hlkwd{summary}\hlstd{(model1)}
\hlstd{coef.table} \hlkwb{<-} \hlstd{report}\hlopt{$}\hlstd{coefficients}
\hlcom{# rownames(coef.table) <- c("Константа","Общая площадь", }
\hlcom{#  "Жилая площадь")}
\hlkwd{xtable}\hlstd{(coef.table)}
\end{alltt}
\end{kframe}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:03 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & -66.03 & 6.07 & -10.89 & 0.00 \\ 
  totsp & 1.77 & 0.12 & 14.98 & 0.00 \\ 
  livesp & 1.27 & 0.25 & 5.05 & 0.00 \\ 
  brick & -19.59 & 9.01 & -2.17 & 0.03 \\ 
  totsp:brick & 0.42 & 0.20 & 2.10 & 0.04 \\ 
  livesp:brick & 0.09 & 0.38 & 0.23 & 0.82 \\ 
   \hline
\end{tabular}
\end{table}


\begin{enumerate}
\item Выпишите отдельно уравнения регрессии для кирпичных домов и для некирпичных домов
\item Проинтерпретируйте коэффициент при $brick_i \cdot totsp_i$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 По 20 наблюдениям оценивается линейная регрессия $\hat{y}=\hat{\beta}_1 +\hat{\beta}_2 x+\hat{\beta}_3 z$, причём истинная зависимость имеет вид $y=\beta_1 +\beta_2 x+\varepsilon$. Случайная ошибка $\varepsilon_i$ имеет нормальное распределение $N(0,\sigma^2)$.

\begin{enumerate}
\item Найдите вероятность $\P(\hat{\beta}_3>se(\hat{\beta}_3))$
\item Найдите вероятность $\P(\hat{\beta}_3>\sigma_{\hat{\beta}_3})$
\end{enumerate}
\end{problem}
 
\begin{solution}

\begin{enumerate}
\item $\P(\hat{\beta}_3>se(\hat{\beta}_3))=\P(t_{17}>1)=0.1656664$
\item $\P(\hat{\beta}_3>\sigma_{\hat{\beta}_3})=\P(N(0,1)>1)=0.1586553$
\end{enumerate}
\end{solution}


\begin{problem}
 К эконометристу Вовочке в распоряжение попали данные с результатами контрольной работы студентов по эконометрике. В данных есть результаты по каждой задаче, переменные $p_1$, $p_2$, $p_3$, $p_4$ и $p_5$, и суммарный результат за контрольную, переменная $kr$. Чему будут равны оценки коэффициентов, их стандартные ошибки, t-статистики, P-значения, $R^2$, $RSS$, если
\begin{enumerate}
\item Вовочка построит регрессию $kr$ на константу, $p_1$, $p_2$, $p_3$, $p_4$ и $p_5$
\item Вовочка построит регрессию $kr$ на $p_1$, $p_2$, $p_3$, $p_4$ и $p_5$ без константы
\end{enumerate}
\end{problem}
 
\begin{solution}
В обоих случаях можно так подобрать коэффициенты $\hb$, что $kr_i=\widehat{kr}_i$. А именно, идеальные прогнозы достигаются при  $\hb_{p_1}=1$, $\hb_{p_2}=1$, $\hb_{p_3}=1$, $\hb_{p_4}=1$, $\hb_{p_5}=1$ и (в первой модели) $\hb_1=0$. Отсюда $RSS=0$, $ESS=TSS$, поэтому $R^2=1$ даже в модели без свободного члена. Получаем $\hs^2=0$, поэтому строго говоря $t$ статистики и $P$-значения не существуют из-за деления на ноль. 

На практике при численной минимизации $RSS$ оказывается, что $t$-статистики коэффициентов при задачах принимают очень большие значения, а соответствующие $P$-значения крайне близки к нулю. В первой модели особенной на практике будет $t$ статистика свободного члена. В силу неопределенности вида $0/0$ свободный коэффициент на практике может оказаться незначим.
\end{solution}




\begin{problem}
 Сгенерируйте данные так, чтобы при оценке линейной регрессионной модели оказалось, что скорректированный коэффициент детерминации, $R^2_{adj}$, отрицательный.

\[ R^2_{adj}=1-(1-R^2)\frac{n-1}{n-k} \]

Следовательно, при $R^2$ близком к 0 и большом количестве регрессоров $k$ может оказаться, что $R^2_{adj}<0$. 

Например,

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{set.seed}\hlstd{(}\hlnum{42}\hlstd{)}
\hlstd{y} \hlkwb{<-} \hlkwd{rnorm}\hlstd{(}\hlnum{200}\hlstd{,}\hlkwc{sd}\hlstd{=}\hlnum{15}\hlstd{)}
\hlstd{X} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{rnorm}\hlstd{(}\hlnum{2000}\hlstd{),}\hlkwc{nrow}\hlstd{=}\hlnum{200}\hlstd{)}
\hlstd{model} \hlkwb{<-} \hlkwd{lm}\hlstd{(y}\hlopt{~}\hlstd{X)}
\hlstd{report} \hlkwb{<-} \hlkwd{summary}\hlstd{(model)}
\hlstd{report}\hlopt{$}\hlstd{adj.r.squared}
\end{alltt}
\begin{verbatim}
## [1] -0.02745184
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{problem}
 
\begin{solution}
\end{solution}


\begin{problem}
 Для коэффициентов регрессии $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i  + \beta_4 w_i + \e_i$ даны 95\%-ые
доверительные интервалы: $\beta_2 \in (0.16;0.66)$, $\beta_3 \in (-0.33;0.93)$ и $\beta_4 \in (-1.01; 0.54)$.

\begin{enumerate}
\item Найдите $\hb_2$, $\hb_3$, $\hb_4$
\item Определите, какие из переменных в регрессии значимы на уровне значимости 5\%.
\end{enumerate}
\end{problem}
 
\begin{solution}
 $\hb_2=0.41$, $\hb_3=0.3$, $\hb_4= -0.235$, переменная $x$ значима 
\end{solution}


\begin{problem}
 Для коэффициентов регрессии $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i  + \beta_4 w_i + \e_i$ даны 95\%-ые
доверительные интервалы: $\beta_2 \in (-0.15;1.65)$, $\beta_3 \in (0.32;0.93)$ и $\beta_4 \in (0.14; 1.55)$.

\begin{enumerate}
\item Найдите $\hb_2$, $\hb_3$, $\hb_4$
\item Определите, какие из переменных в регрессии значимы на уровне значимости 5\%.
\end{enumerate}
\end{problem}
 
\begin{solution}
 $\hb_2=0.75$, $\hb_3=0.625$, $\hb_4= 0.845$, переменные $z$ и $w$ значимы 
\end{solution}

\begin{problem}
 Эконометрэсса Мырли очень суеверна и поэтому оценила три модели:
\begin{enumerate}
\item[M1] $y_i=\beta_1 + \beta_2 x_i + \beta_3 w_i + \e_i$ по всем наблюдениям.
\item[M2] $y_i=\beta_1 + \beta_2 x_i + \beta_3 w_i + \beta_4 d_i + \e_i$ по всем наблюдениям, где $d_i$ --- дамми-переменная равная $1$ для 13-го наблюдения и нулю иначе.
\item[M3] $y_i=\beta_1 + \beta_2 x_i + \beta_3 w_i + \e_i$ по всем наблюдениям, кроме 13-го.
\end{enumerate}

\begin{enumerate}
\item Сравните между собой $RSS$ во всех трёх моделях
\item Есть ли совпадающие оценки коэффициентов в этих трёх моделях? Если есть, то какие?
\item Может ли Мырли не выполняя вычислений узнать ошибку прогноза для 13-го наблюдения при использовании третьей модели? Если да, то как?
\end{enumerate}
\end{problem}
 
\begin{solution}
$RSS_1 > RSS_2 = RSS_3$, в моделях два и три, ошибка прогноза равна $\hb_4$
\end{solution}

\begin{problem}
 Рассмотрим модель $y_i = \beta_1+ \beta_2 x_i + \beta_3 w_i +\beta_4 z_i + \e_i$.  При оценке модели по $24$ наблюдениям оказалось, что $RSS=15$, $\sum (y_i-\bar{y}-w_i+\bar{w})^2=20$. На уровне значимости 1\% протестируйте гипотезу 
\[
H_0:
\begin{cases}
\beta_2+\beta_3+\beta_4=1 \\
\beta_2=0 \\
\beta_3=1 \\
\beta_4=0
\end{cases}
\]
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 Модель регрессии $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i + \e_i$, в которой ошибки
$\e_i$ независимы и нормальны $N(0;\sigma^2)$, оценивается по 13 наблюдениям. Найдите $\E(RSS)$, $\Var(RSS)$, $\P(5\sigma^2<RSS<10\sigma^2)$, $\P(5\hs^2<RSS<10\hs^2)$
\end{problem}
 
\begin{solution}
 $RSS/\sigma^2\sim\chi^2_{n-k}$, $\E(RSS)=(n-k)\sigma^2$, $\Var(RSS)=2(n-k)\sigma^4$, $\P(5\sigma^2<RSS<10\sigma^2)\approx 0.451$ 
\end{solution}


\begin{problem}
 Рассмотрим модель регрессии $y_i=\beta_1+\beta_2 x_i + \beta_3 z_i+\e_i$, в которой
ошибки $\e_i$ независимы и имеют нормальное распределение $N(0,\sigma^2)$. Известно, что выборка в $n = 30$
наблюдений была разбита на три непересекающиеся подвыборки, содержащие $n_1 = 13$, $n_2 = 4$ и $n_3 = 13$ наблюдений. Пусть $\hs_{j}^2$ --- это оценка дисперсии случайных ошибок для
регрессии, оцененной по $j$-ой подвыборке. Найдите 
\begin{enumerate}
\item  $\P(\hs_3^2>\hs_1^2)$, $\P(\hs_1^2>2\hs_2^2)$
\item $\E(\hs_2^2/\hs_1^2)$, $\Var(\hs_2^2/\hs_1^2)$
\end{enumerate}
\end{problem}
 
\begin{solution}
 $\P(\hs_3^2>\hs_1^2)=0.5$, $\P(\hs_1^2>2\hs_2^2)=0.5044$, $\E(\hs_2^2/\hs_1^2)=1.25$, $\Var(\hs_2^2/\hs_1^2)=4.6875$
 \end{solution}

\begin{problem}
 Рассмотрим модель регрессии $y_i=\beta_1+\beta_2 x_i + \beta_3 z_i+\e_i$, в которой
ошибки $\e_i$ независимы и имеют нормальное распределение $N(0,\sigma^2)$. Для $n = 13$ наблюдения найдите уровень
доверия следующих доверительных интервалов для неизвестного параметра $\sigma^2$:
\begin{enumerate}
\item $(0;RSS/4.865)$ 
\item $(RSS/18.307;RSS/3.940)$
\item $(RSS/15.987;\infty)$
\end{enumerate}
\end{problem}
 
\begin{solution}
90\% во всех пунктах
\end{solution}



\begin{problem}
Пусть $\hat{\beta_1}$ и $\hat{\beta_2}$ --- МНК-оценки коэффициентов в регрессии $y_i = \beta_1 + \beta_2 x_i + \e_i$, оцененной по наблюдениям $i = 1, \ldots, m$, а $\hat{\mu}$, $\hat{\nu}$, $\hat{\gamma}$ и $\hat{\delta}$ --- МНК-коэффициенты в регрессии $y_i = \mu + \nu x_i + \gamma d_i + \delta x_i d_i + \e_i$, оцененной по наблюдениям $i = 1, \ldots, n$, где фиктивная переменная $d$ определяется следующим образом
$$d_i = \begin{cases}
1 & \text{ при } i \in \{1, \ldots, m\} \\
0 & \text{ при } i \in \{m + 1, \ldots, n\} \\
\end{cases}$$
Покажите, что $\hat{\beta_1} = \hat{\mu} + \hat{\gamma}$ и $\hat{\beta_2} = \hat{\nu} + \hat{\delta}$.
\end{problem}

\begin{solution}
Поскольку $\hat{\mu}$, $\hat{\nu}$, $\hat{\gamma}$ и $\hat{\delta}$ являются МНК-коэффициентами в регрессии $y_i = \mu + \nu x_i + \gamma d_i + \delta x_i d_i + \e_i$, $i = 1, \ldots, n$, то для любых $\mu$, $\nu$, $\gamma$ и $\delta$ имеет место
\begin{multline}
\label{task1:1}\sum_{i=1}^n (y_i - \hat{\mu} - \hat{\nu} x_i - \hat{\gamma} d_i - \hat{\delta} x_i d_i - \e_i)^2 \leqslant \\
\sum_{i=1}^n (y_i - \mu - \nu x_i - \gamma d_i - \delta x_i d_i - \e_i)^2
\end{multline}
Перепишем неравенство (\ref{task1:1}) в виде
\begin{multline}
\label{task1:2}\sum_{i=1}^m (y_i - (\hat{\mu} + \hat{\gamma}) - (\hat{\nu} + \hat{\delta}) x_i)^2 + \sum_{i= m + 1}^n (y_i - \hat{\mu} - \hat{\nu} x_i)^2 \leqslant \\
\sum_{i=1}^m (y_i - ({\mu} + {\gamma}) - ({\nu} + {\delta}) x_i)^2 + \sum_{i= m + 1}^n (y_i - {\mu} - {\nu} x_i)^2
\end{multline}
Учитывая, что неравенство (\ref{task1:2}) справедливо для всех $\mu$, $\nu$, $\gamma$ и $\delta$, то оно останется верным для $\mu = \hat{\mu}$, $\nu = \hat{\nu}$ и произвольных $\gamma$ и $\delta$. Имеем
\begin{multline}
\sum_{i=1}^m (y_i - (\hat{\mu} + \hat{\gamma}) - (\hat{\nu} + \hat{\delta}) x_i)^2 + \sum_{i= m + 1}^n (y_i - \hat{\mu} - \hat{\nu} x_i)^2 \leqslant \\
 \sum_{i=1}^m (y_i - (\hat{\mu} + {\gamma}) - (\hat{\nu} + {\delta}) x_i)^2 + \sum_{i= m + 1}^n (y_i - \hat{\mu} - \hat{\nu} x_i)^2
\end{multline}
Следовательно 
\begin{equation}
\sum_{i=1}^m (y_i - (\hat{\mu} + \hat{\gamma}) - (\hat{\nu} + \hat{\delta}) x_i)^2 \leqslant \sum_{i=1}^m (y_i - (\hat{\mu} + {\gamma}) - (\hat{\nu} + {\delta}) x_i)^2
\end{equation}
Обозначим $\tilde{\beta_1} := \hat{mu} + \gamma$ и $\tilde{\beta_2} := \hat{\nu} + \delta$. В силу произвольности $\gamma$ и $\delta$ коэффициенты $\tilde{\beta_1}$ и $\tilde{\beta_2}$ также произвольны. тогда для любых $\tilde{\beta_1}$ и $\tilde{\beta_2}$ выполнено неравенство:
$$\sum_{i=1}^m (y_i - (\hat{\mu} + \hat{\gamma}) - (\hat{\nu} + \hat{\delta}) x_i)^2 \leqslant \sum_{i=1}^m (y_i - \tilde{\beta_1} - \tilde{\beta_2} x_i)^2$$
которое означает, что $\hat{\mu} + \hat{\gamma}$ и $\hat{\nu} + \hat{\delta}$ являются МНК-оценками коэффициентов $\beta_1$ и $\beta_2$ в регрессии $y_i = \beta_1 + \beta_2 x_i + \e_i$, оцененной по наблюдениям $i = 1, \ldots, m$, то есть $\hat{\beta_1} = \hat{\mu} + \hat{\gamma}$ и $\hat{\beta_2} = \hat{\nu} + \hat{\delta}$.
\end{solution}

\begin{problem}
Верно ли, что $R_{adj}^2 = 1 - (1 - R^2)\frac{n-1}{n-k}$ распределен по $F(n-k, n-1)$? Если да, то объясните, почему, если нет, то тоже объясните, почему.
\end{problem}

\begin{solution}
не верно, поскольку $R_{adj}^2$ может принимать отрицательные значения, а $F(n-k, n-1)$ --– не может.
\end{solution}


\begin{problem}
Сгенерируйте набор данных, обладающий следующим свойством. Если попытаться сразу выкинуть регрессоры $x$ и $z$, то гипотеза о их совместной незначимости отвергается. Если вместо этого попытаться выкинуть отдельно $x$, или отдельно $z$, то гипотеза о незначимости не отвергается.
\end{problem}

\begin{solution}
Сгенерировать сильно коррелированные $x$ и $z$
\end{solution}


\begin{problem}
Сгенерируйте набор данных, обладающий следующим свойством. Если попытаться сразу выкинуть регрессоры $x$ и $z$, то гипотеза о их совместной незначимости отвергается. Если вместо сначала выкинуть отдельно $x$, то гипотеза о незначимости не отвергается. Если затем выкинуть $z$, то гипотезы о незначимости тоже не отвергается.
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Напишите свою функцию, которая бы оценивала регрессию методом наименьших квадратов. На вход функции должны подаваться вектор зависимых переменных $y$ и матрица регрессоров $X$. На выходе функция должна выдавать список из $\hb$, $\hVar(\hb)$, $\hy$, $\he$, $ESS$, $RSS$ и $TSS$. По возможности функция должна проверять корректность аргументов, например, что в $y$ и $X$ одинаковое число наблюдений и т.д. Использовать \verb|lm| или \verb|glm| запрещается.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Сгенерируйте данные так, чтобы при оценке модели $\hat{y}=\hat{\beta_1}+\hat{\beta_2}x+\hat{\beta_3}z$ оказывалось, что $\hat{\beta_2}>0$, а при оценке модели $\hat{y}=\hat{\beta_1}+\hat{\beta_2}x$ оказывалось, что $\hat{\beta_2}<0$.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Предложите способ, как построить доверительный интервал для вершины параболы.
\end{problem}

\begin{solution}
bootstrap, дельта-метод.
\end{solution}


\begin{problem}
Скачайте результаты двух контрольных работ по теории вероятностей, \url{https://github.com/bdemeshev/em301/raw/master/datasets/tvims2012_data.csv} с описанием данных, \url{https://github.com/bdemeshev/em301/raw/master/datasets/tvims2012_data_description.txt}. Наша задача попытаться предсказать результат второй контрольной работы зная позадачный результат первой контрольной, пол и группу студента. 
\begin{enumerate}
\item Какая задача из первой контрольной работы наиболее существенно влияет на результат второй контрольной?
\item Влияет ли пол на результат второй контрольной?
\item Что можно сказать про влияние группы, в которой учится студент?
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Сформулируйте теорему Гаусса-Маркова
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Эконометресса Эвридика хочет оценить модель $y_i=\beta_1 + \beta_2 x_i +\beta_3 z_i + \e_i$. К сожалению, она измеряет зависимую переменную с ошибкой. Т.е. вместо $y_i$ она знает значение $y_i^*=y_i+u_i$ и использует его в качестве зависимой переменной при оценке регрессии. Ошибки измерения $u_i$ некоррелированы между собой и с $\e_i$. 
\begin{enumerate}
\item Будут ли оценки Эвридики несмещенными?
\item Могут ли дисперсии оценок Эвридики быть ниже чем дисперсии МНК оценок при использовании настоящего $y_i$?
\item Могут ли оценки дисперсий оценок Эвридики быть ниже чем оценок дисперсий МНК оценок при использовании настоящего $y_i$?
\end{enumerate}
\end{problem}


\begin{solution}
При наличии ошибок в измерении зависимой переменной оценки остаются несмещенными, их дисперсия растет. Однако оценка дисперсии может случайно оказаться меньше. Например, могло случиться, что ошибки $u_i$ случайно компенсировали $\e_i$. 
\end{solution}

\begin{problem}
Эконометресса Ефросинья исследует зависимость удоев от возраста и породы коровы. Она оценила модель 
\[
\hy_i = \hb_1 + \hb_2 age_i +\hb_3 d_{1i} + \hb_4 d_{2i} 
\]
Эконометресса Глафира исследует ту же зависимость:
\[
\hy_i = \hb'_1 + \hb'_2 age_i +\hb'_3 d'_{1i} + \hb'_4 d'_{2i} 
\]
но вводит дамми-переменные вводит по-другому:

\begin{tabular}{c|cccc}
Порода коровы & $d_1$ & $d_2$ & $d'_1$ & $d'_2$  \\ 
\hline 
Холмогорская & 0 & 0 & 1 & 1 \\ 
Тагильская & 1 & 0 & 0 & 1  \\ 
Ярославская & 0 & 1 & 1 & 0  \\ 
\hline 
\end{tabular} 

Выразите оценки коэффициентов Глафиры через оценки коэффициентов Ефросиньи.
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
%\todo[inline]{предпосылки JB}
Для проверки гипотезы о нормальности ошибок регрессии используют в частности статистику Харке-Бера (Jarque-Bera):
\[
JB=\frac{S^2}{6} + \frac{(K-3)^2}{24},
\]
где $S=\sum_i^n \he_i^3 /\hs^3_{ML}$ 
Строго говоря статистика Харке-Бера проверяет гипотезу о том, что скошенность равна нулю, а эксцесс равен 3, т.е. $H_0:$ $\E(\e_i^3)=0$, $\E(\e_i^4)=3\sigma^4$. Асимптотически при верной $H_0$ статистика имеет хи-квадрат распределение с двумя степенями свободы.

По аналогии со статистикой Харке-Бера придумайте асимптотические статистики, которые бы проверяли гипотезы:
\begin{enumerate}
\item $H_0:$ $\E(\e_i^3)=0$
\item $H_0:$ $\E(\e_i^4)=3\sigma^4$
\item $H_0:$ $\E(\e_i^5)=0$
\end{enumerate}
Какое асимптотическое распределение при верной $H_0$ будут иметь придуманные статистики?
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Рассмотрим классическую модель линейной регрессии. Найдите предел $\lim R^2$ при условии, что $\sigma^2 \rightarrow \infty$.
\end{problem}


\begin{solution}
$0$
\end{solution}


\begin{problem}
В отделении буйно-помешанных 30 больных. В инфекционном отделении 20 больных. Эконометрист Василий из департамента НОБ (науко-образного бреда) построил регрессию температуры больного на константу по каждому отделению в отдельности и по обоим отделениям сразу. По инфекционному отделению $RSS=15$, по отделению буйно-помешанных $RSS=25$, по обоим отделениям сразу --- $RSS=80$. Эконометрист Василий хочет получить грант на публикацию в зарубежном рецензируемом журнале статьи, описывающей результаты регрессии по обоим отделениям сразу. 

Помогите главврачу с помощью теста Чоу проверить корректность действий эконометриста Василия
\end{problem}

\begin{solution}
Проводим тест Чоу
\end{solution}


\begin{problem}
Все предпосылки классической теоремы Гаусса-Маркова со стохастическими регрессорами для случая независимой выборки выполнены. Является ли оценка $\hs$ несмещенной и состоятельной? Если оценка смещена, то в большую или меньшую сторону относительно $\sigma$?
\end{problem}

\begin{solution}
Несмещенной является $\hs^2$, поэтому $\hs$ смещена, но состоятельна. 
\end{solution}


\chapter{МНК с матрицами и вероятностями}


\begin{problem} 
Пусть $y = X\beta + \e$ --- регрессионная модель.
\begin{enumerate}
\item Сформулируйте теорему Гаусса-Маркова
\item Верно ли, что оценка $\hb = (X'X)^{-1}X'y$ несмещённая?
\item В условиях теоремы Гаусса-Маркова найдите ковариационную матрицу $\hb$
\end{enumerate}
\end{problem}

\begin{solution}
\begin{enumerate}
\item 
В случае нестохастических регрессоров:

Пусть дана регрессионная модель $y=X\beta+\epsilon$ с $k$ регрессорами, включая свободный член и $n$ наблюдениями, тогда если
\begin{enumerate}
\item регрессионная модель правильно специфицирована
\item $\text{rang}(X)=k$
\item $X$ не являются стохастическими
\item $\E(\epsilon)=0$
\item $\Var(\epsilon)=\sigma^2 I$
\end{enumerate}
то $\hat\beta=(X'X)^{-1}X'y$ являются лучшими оценками в классе линейных несмещённых оценок, то есть BLUE-оценками.

В случае стохастических регрессоров:

Пусть дана регрессионная модель $y=X\beta+\epsilon$ с $k$ регрессорами, включая свободный член и $n$ наблюдениями, тогда если
\begin{enumerate}
\item регрессионная модель правильно специфицирована
\item $\text{rang}(X)=k$
\item $\E(\epsilon|X)=0$
\item $\Var(\epsilon|X)=\sigma^2 I$
\end{enumerate}
то $\hat\beta=(X'X)^{-1}X'y$ являются лучшими оценками в классе линейных несмещённых оценок, то есть BLUE-оценками.

\item
Да, верно. В самом деле, 
\begin{multline*}
\E(\hat\beta)=\E((X'X)^{-1}X'y)=(X'X)^{-1}X'\E(y)=(X'X)^{-1}X'\E(X\beta+\epsilon)=\\
=(X'X)^{-1}X'X\E(\beta)+(X'X)^{-1}X'\underbrace{\E(\epsilon)}_{=0}=\beta
\end{multline*}


\item
\begin{multline*}
\Var(\hat\beta)=\Var((X'X)^{-1}X'y)=(X'X)^{-1}X'\Var(y)((X'X)^{-1}X')'=\\
=(X'X)^{-1}X'\Var(X\beta+\epsilon)X(X'X)^{-1}
\end{multline*}
Так как $\beta$ является константой, то $\Var(X\beta)=0$. Тогда 
\begin{multline*}
\Var(\hat\beta)=(X'X)^{-1}X'\Var(X\beta+\epsilon)X(X'X)^{-1}=\\=(X'X)^{-1}X'\Var(\epsilon)X(X'X)^{-1}=
(X'X)^{-1}X'\sigma^2X(X'X)^{-1}=\\=\sigma^2(X'X)^{-1}X'X(X'X)^{-1}=\sigma^2(X'X)^{-1}
\end{multline*}
\end{enumerate}

\end{solution}

\begin{problem} 
Пусть $y = X\beta + \e$ --- регрессионная модель и $\tilde{\beta} = ((X'X)^{-1}X'+ A)y$ --- несмещённая оценка вектора неизвестных параметров $\beta$. Верно ли, что $AX=0$?
\end{problem}

\begin{solution}
Да, в общем случае (кроме случая $\beta=0$ это верно. Так как $\tilde\beta$ является несмещённой, то $\E(\tilde\beta)=\beta$.
\begin{multline*}
\E(\tilde\beta)=\E(((X'X)^{-1}X'+A)y)=\E[((X'X)^{-1}X'+A)(X\beta+\epsilon)]=\\=
\E(((X'X)^{-1}X'+A)X\beta)+((X'X)^{-1}X'+A)\underbrace{\E(\epsilon)}_{=0}=\\
=\E((X'X)^{-1}X'X\beta+AX\beta)=\beta+AX\beta
\end{multline*}
\[\E(\tilde\beta)=\beta\]
\[\beta+AX\beta=\beta\]
\[AX\beta=0\]
Значит, либо $AX=0$, либо $\beta=0$.

Заметим, что при $\beta=0$ при любом $AX$ оценка $\tilde\beta$ будет несмещённой.
\end{solution}




\begin{problem}
 Пусть $y = X\beta + \e$ --- регрессионная модель, $X = \begin{pmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 1 \end{pmatrix}$, $y = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}$, $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix}$, $\e = \begin{pmatrix} \e_1 \\ \e_2 \\ \e_3 \end{pmatrix}$, $\E(\e)$ = 0, $\Var(\e) = \sigma^2 I$. Найдите коэффициент корреляции $\Corr(\hb_1,\hb_2)$.

\end{problem}

\begin{solution}
\[X'X=\left(\begin{array}{cc}
3 & 2 \\ 
2 & 2
\end{array}\right) \]
\[(X'X)^{-1}=\left(\begin{array}{cc}
1 & -1 \\ 
-1 & 1.5
\end{array}\right) \]

\[\Var(\hat\beta_1)=\Var(\hat\beta)_{[1,1]}=\sigma^2\]
\[\Var(\hat\beta_2)=\Var(\hat\beta)_{[2,2]}=1.5\sigma^2\]
\[\Cov(\hat\beta_1,\hat\beta_2)=\Var(\hat\beta)_{[1,2]}=-\sigma^2\]

\[\corr(\hat\beta_1,\hat\beta_2)=\frac{\Cov(\hat\beta_1,\hat\beta_2)}{\sqrt{\Var(\hat\beta_1)}\Var(\hat\beta_2)}=\\
\\\frac{\sigma^2}{\sigma\cdot\sqrt{1.5}\sigma}=\sqrt{\frac{2}{3}}=\frac{\sqrt6}{3}\]
Показательно, что значения $y$ здесь не используются.

Ответ: $\frac{\sqrt6}{3}$.
\end{solution}


\begin{problem} 
Пусть $y = X\beta + \e$ --- регрессионная модель, где $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$. Пусть $Z = XD$, где $D = \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}$. Рассмотрите <<новую>> регрессионную модель $y = Z\alpha + u$, где $\alpha = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}$. Определите, как выражаются <<новые>> МНК-коэффициенты через <<старые>>.

\end{problem}

\begin{solution}
\[\hat\beta=(X'X)^{-1}X'y\]
\[X'X\hat\beta=X'y\]
\[\hat\alpha=(D'X'XD)^{-1}D'X'y\]
поэтому
\[\hat\alpha=(D'X'XD)^{-1}D'X'y=(D'X'XD)^{-1}D'X'X\hat\beta=(D'X'XD)^{-1}D'X'XDD^{-1}\hat\beta=D^{-1}\hat\beta\]
\[D^{-1}=\left(\begin{array}{ccc}
1 & -1 & 1\\ 
0 & 1 & -1\\
0 & 0 & 1
\end{array}\right)\]

\[\begin{cases}
\hat\alpha_1=\beta_1-\beta_2+\beta_3\\
\hat\alpha_2=\beta_2-\beta_3\\
\hat\alpha_3=\beta_3
\end{cases}\]
\end{solution}


\begin{problem}
Пусть $y = X\beta + \e$ --- регрессионная модель, где $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$. Пусть $Z = XD$, где $D = \begin{pmatrix} 1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}$. Рассмотрите <<новую>> регрессионную модель $y = Z\alpha + u$, где $\alpha = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}$. Определите, как выражаются <<новые>> МНК-коэффициенты через <<старые>>.

\end{problem}

\begin{solution}
\[\hat\beta=(X'X)^{-1}X'y\]
\[X'X\hat\beta=X'y\]
\[\hat\alpha=(D'X'XD)^{-1}D'X'y\]
поэтому
\[\hat\alpha=(D'X'XD)^{-1}D'X'y=(D'X'XD)^{-1}D'X'X\hat\beta=(D'X'XD)^{-1}D'X'XDD^{-1}\hat\beta=D^{-1}\hat\beta\]
\[D^{-1}=\left(\begin{array}{ccc}
1 & -1 & 0\\ 
0 & 1 & -1\\
0 & 0 & 1
\end{array}\right)\]

\[\begin{cases}
\hat\alpha_1=\beta_1-\beta_2\\
\hat\alpha_2=\beta_2-\beta_3\\
\hat\alpha_3=\beta_3
\end{cases}\]
\end{solution}


\begin{problem}
Пусть $y = X\beta + \e$ --- регрессионная модель, где $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$. Пусть $Z = XD$, где $D = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}$. Рассмотрите <<новую>> регрессионную модель $y = Z\alpha + u$, где $\alpha = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}$. Определите, как выражаются <<новые>> МНК-коэффициенты через <<старые>>.

\end{problem}

\begin{solution}
\[\hat\beta=(X'X)^{-1}X'y\]
\[X'X\hat\beta=X'y\]
\[\hat\alpha=(D'X'XD)^{-1}D'X'y\]
поэтому
\[\hat\alpha=(D'X'XD)^{-1}D'X'y=(D'X'XD)^{-1}D'X'X\hat\beta=(D'X'XD)^{-1}D'X'XDD^{-1}\hat\beta=D^{-1}\hat\beta\]
\[D^{-1}=\left(\begin{array}{ccc}
1 & 0 & 0\\ 
0 & 1 & -1\\
0 & 0 & 1
\end{array}\right)\]

\[\begin{cases}
\hat\alpha_1=\beta_1\\
\hat\alpha_2=\beta_2-\beta_3\\
\hat\alpha_3=\beta_3
\end{cases}\]
\end{solution}


\begin{problem}
Пусть $y = X\beta + \e$ --- регрессионная модель. Верно ли, что $\hat{\e}'\hat{y}=0$ и $\hat{y}'\hat{\e}=0$?
\end{problem}
\begin{solution}
Да, верно.
\begin{multline*}
\he'\hy=(y-\hy)'\hy=(y-X\hat\beta)'X\hat\beta=(y-X(X'X)^{-1}X'y)'X(X'X)^{-1}X'y=\\=
((I-X(X'X)^{-1}X')y)'X(X'X)^{-1}X'y=y'(I-X(X'X)^{-1}X')X(X'X)^{-1}X'y=\\
=y'(X(X'X)^{-1}X'-X(X'X)^{-1}X'X(X'X)^{-1}X')y=y'(X(X'X)^{-1}X'-X(X'X)^{-1}X')y=0
\end{multline*}

Да, верно.
\[\hy'\he=(\he'\hy)'=0\]
так как выше доказано, что $he'\hy=0$.
\end{solution}

\begin{problem}
Пусть $y = X\beta + \e$ --- регрессионная модель, где $\E(\e)=0$, $\Var(\e)=\sigma_{\e}^2 I$. Пусть $A$ --- неслучайная матрица размера $k \times k$, $\det(A) \not= 0.$ Совершается преобразование регрессоров по правилу $Z=XA$. В преобразованных регрессорах уравнение выглядит так: $y = Z\gamma + u$, где $\E(u)=0$, $\Var(u)=\sigma_{u}^2 I.$

\begin{enumerate}
\item Как связаны между собой МНК-оценки $\hb$ и $\hat{\gamma}$?
\item Как связаны между собой векторы остатков регрессий?
\item Как связаны между собой прогнозные значения, полученные по двум регрессиям?
\end{enumerate}
\end{problem}

\begin{solution}
\begin{enumerate}
\item 
\begin{multline}
\hat{\gamma} = (Z'Z)^{-1}Z'y = A^{-1}(X'X)^{-1}(A')^{-1}A'X'y =\\
 A^{-1}(X'X)^{-1} X'y = A^{-1}\hb
\end{multline}
\item $\hat{u} = y - Z\hat{\gamma} = y - XAA^{-1}\hb = y - X\hb = \hat{\e}$
\item Пусть $z^0 = \begin{pmatrix} 1 & z_1^0 & \dots & z_{k-1}^0 \end{pmatrix}$ --- вектор размера $1 \times k$ и $x^0 = \begin{pmatrix} 1 & x_1^0 & \dots & x_{k-1}^0 \end{pmatrix}$ --- вектор размера $1 \times k$. Оба эти вектора представляют собой значения факторов. Тогда $z^0 = x^0 A$ и прогнозное значение для регрессии с преобразованными факторами равно $z^0 \hat{\gamma} = x^0 AA^{-1} \hb = x^0 \hb$ прогнозному значению для регрессии с исходными факторами. 
\end{enumerate}
\end{solution}


\begin{problem}
Рассмотрим оценку вида $\tilde{\beta} = ((X'X)^{-1} + \gamma I)X'y$ для вектора коэффициентов регрессионного уравнения $y = X\beta + \e$, удовлетворяющего условиям классической регрессионной модели. Найдите $\E(\tilde{\beta})$ и $\Var(\tilde{\beta}).$ 
\end{problem}

\begin{solution}
\begin{enumerate}
\item 
\begin{multline}
\E(\tilde{\beta}) = ((X'X)^{-1} + \gamma I)X'\E(y) = \\
 ((X'X)^{-1} + \gamma I)X'X\beta = \beta + \gamma X'X\beta
\end{multline}
\item 
\begin{multline}
\Var(\tilde{\beta}) = \Var(((X'X)^{-1} + \gamma I)X'y) = \\
 \Var(((X'X)^{-1} + \gamma I)X'\e) = \\
 (((X'X)^{-1} + \gamma I)X')\Var(\e)(((X'X)^{-1} + \gamma I)X')'=  \\
  (((X'X)^{-1} + \gamma I)X')\sigma_{\e}^2 I(((X'X)^{-1} + \gamma I)X')'= \\
  \sigma_{\e}^2((X'X)^{-1} + \gamma I)X'X((X'X)^{-1} + \gamma I) = \\
  \sigma_{\e}^2((X'X)^{-1} + \gamma I)(I + \gamma X'X) =\\
   \sigma_{\e}^2((X'X)^{-1} + 2\gamma I + \gamma ^2 X'X)
\end{multline}
\end{enumerate}
\end{solution}



\begin{problem}
Верно ли, что при невырожденном преобразовании факторов $R^2$ не меняется? А именно, пусть заданы две регрессионные модели: $y = X\beta + \e$ и $y = Z\alpha + u$, где $y$ --- вектор размера $n \times 1$, $X$ и $Z$ --- матрицы размера $n \times k$, $\beta$ и $\alpha$ --- вектора рамзера $k \times 1$, $\e$ и $u$ --- вектора размера $n \times 1$, а также $Z=XD$, $\det(D) \not= 0.$ Верно ли, что коэффициенты детерминации представленных выше моделей равны между собой?
\end{problem}
\begin{solution}
Да, верно.
\[R^2=\frac{ESS}{TSS}=\frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}\]
\[TSS=\sum_{i=1}^n(y_i-\bar y)^2=y'\left(I-\frac{\overrightarrow{1}'}{\overrightarrow{1}^2}\right)y\]
не зависит от $X$.
\[RSS_a=\sum_{i=1}^n(y_i-\hat y_i)^2=y'(I-X(X'X)^{-1}X')y\]
\[RSS_b=y'(I-Z(Z'Z)^{-1}Z')y=y'(I-XD(D'X'XD)^{-1}D'X')y=\]
так как $D$ является квадратной и невырожденной, то используя формулу $(AB)^{-1}=B^{-1}A^{-1}$, получим:
\begin{multline*}
RSS_b=y'(I-XD(D'X'XD)^{-1}D'X')y=y'(I-XDD^{-1}(X'X)^{-1}D'^{-1}D'X')y=\\=y'(I-X(X'X)^{-1}X')y=RSS_a
\end{multline*}

Значит,
\[R^2_a=1-\frac{RSS_a}{TSS_a}=1-\frac{RSS_b}{TSS_b}=R^2_b\]
\end{solution}


\begin{problem}
Верно ли, что при невырожденном преобразовании факторов $RSS$ не меняется. А именно, пусть заданы две регрессиионные модели: $y = X\beta + \e$ и $y = Z\alpha + u$, где $y$ --- вектор размера $n \times 1$, $X$ и $Z$ --- матрицы размера $n \times k$, $\beta$ и $\alpha$ --- вектора рамзера $k \times 1$, $\e$ и $u$ --- вектора размера $n \times 1$, а также $Z=XD$, $\det(D) \not= 0.$ Верно ли, что сумма квадратов остатков в представленных выше моделях равны между собой?
\end{problem}
\begin{solution}
Да, верно.
\[RSS_1=\sum_{i=1}^n(y_i-\hat y_i)^2=y'(I-X(X'X)^{-1}X')y\]
\[RSS_2=y'(I-Z(Z'Z)^{-1}Z')y=y'(I-XD(D'X'XD)^{-1}D'X')y=\]
так как $D$ является квадратной и невырожденной, то используя формулу $(AB)^{-1}=B^{-1}A^{-1}$, получим:
\begin{multline*}
RSS_2=y'(I-XD(D'X'XD)^{-1}D'X')y=y'(I-XDD^{-1}(X'X)^{-1}D'^{-1}D'X')y=\\=y'(I-X(X'X)^{-1}X')y=RSS_1
\end{multline*}
\end{solution}


\begin{problem}
Пусть регрессионная модель $y_i = \beta_1 + \beta_2 x_{i2} + \beta_3 x_{i3} + \e_i$, $i = 1, \ldots, n$, задана в матричном виде при помощи уравнения $y = X \beta + \e$, где $\beta =  \begin{pmatrix}
\beta_1 & \beta_2 & \beta_3\\
\end{pmatrix} ^T$. Известно, что $\E \e = 0$ и $\Var (\e) = 4 \cdot I$. Известно также, что:

$y =  \begin{pmatrix}
1 \\
2 \\
3 \\
4 \\
5 \\
\end{pmatrix} $, $X =  \begin{pmatrix}
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 1 & 0 \\
1 & 1 & 1 \\
\end{pmatrix} $

Для удобства расчётов ниже приведены матрицы:

$X^T X =  \begin{pmatrix}
5 & 3 & 1 \\
3 & 3 & 1 \\
1 & 1 & 1 \\
\end{pmatrix} $ и $(X^T X)^{-1} =  \begin{pmatrix}
0.5 & -0.5 & 0 \\
-0.5 & 1 & -0.5 \\
0 & -0.5 & 1.5 \\
\end{pmatrix} $.

Найдите:
\begin{enumerate}
\item $\Var (\e_1)$
\item $\Var (\beta_1)$
\item $\Var (\hb_1)$
\item $\widehat{\Var }(\hb_1)$
\item $\E (\hb_1^2) - \beta_1^2$
\item $\Cov (\hb_2, \hb_3)$
\item $\widehat{\Cov }(\hb_2, \hb_3)$
\item $\Var (\hb_2 - \hb_3)$
\item $\widehat{\Var }(\hb_2 - \hb_3)$
\item $\Var (\beta_2 - \beta_3)$
\item $\Corr (\hb_2, \hb_3)$
\item $\widehat{\Corr}(\hb_2, \hb_3)$
\item $\E (\hs^2)$
\item $\hs^2$
\end{enumerate}
\end{problem}
\begin{solution}
\end{solution}



\begin{problem}
Пусть $y_i = \beta_1 + \beta_2 x_{i2} + \beta_3 x_{i3} + \e_i$ --- регрессионная модель, где $X = \begin{pmatrix} 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 1 \end{pmatrix}$, $y = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \\ 5 \end{pmatrix}$, $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$, $\e = \begin{pmatrix} \e_1 \\ \e_2 \\ \e_3 \\ \e_4 \\ \e_5  \end{pmatrix}$, ошибки $\e_i$ независимы и нормально распределены с $\E(\e)$ = 0, $Var(\e) = \sigma^2 I$. Для удобства расчётов даны матрицы: $X'X = \begin{pmatrix} 5 & 2 & 1 \\ 2 & 2 & 1\\ 1 & 1 & 1 \end{pmatrix}$ и $(X'X)^{-1}= \begin{pmatrix} 0.3333 & -0.3333 & 0.0000 \\ -0.3333 & 1.3333 & -1.0000 \\ 0.0000 & -1.0000 & 2.0000 \end{pmatrix}$





\begin{enumerate}
\item Укажите число наблюдений
\item Укажите число регрессоров в модели, учитывая свободный член
\item Найдите $TSS = \sum_{i=1}^n (y_i - \bar y)^2$
\item Найдите $RSS = \sum_{i=1}^n (y_i - \hat{y_i})^2$
\item Методом МНК найдите оценку для вектора неизвестных коэффициентов
\item Чему равен $R^2$ в модели? Прокомментируйте полученное значение с точки зрения качества оценённого уравнения регрессии
\item Сформулируйте основную и альтернативную гипотезы, которые соответствуют тесту на значимость переменной $x_2$ в уравнении регрессии
\item Протестируйте на значимость переменную $x_2$ в уравнении регрессии на уровне значимости $10\%$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод о значимости переменной $x_2$
\end{enumerate}
\item Найдите $P-$значение, соответствующее наблюдаемому значению тестовой статистики $(T_{obs})$ из предыдущего пункта. На основе полученного $P-$значения сделайте вывод о значимости переменной $x_2$
\item На уровне значимости $10\%$ проверьте гипотезу $H_0: \beta_2 = 1$ против альтернативной $H_a: \beta_2 \not= 1$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item На уровне значимости $10\%$ проверьте гипотезу $H_0: \beta_2 = 1$ против альтернативной $H_a: \beta_2 > 1$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item На уровне значимости $10\%$ проверьте гипотезу $H_0: \beta_2 = 1$ против альтернативной $H_a: \beta_2 < 1$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item Сформулируйте основную гипотезу, которая соответствует тесту на значимость регрессии <<в целом>>
\item На уровне значимости $5\%$ проверьте гипотезу о значимости регрессии <<в целом>>:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item Найдите $P-$значение, соответствующее наблюдаемому значению тестовой статистики $(T_{obs})$ из предыдущего пункта. На основе полученного $P-$значения сделайте вывод о значимости регрессии <<в целом>>
\item На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_2 + \beta_3 = 2$ против альтернативной $H_a: \beta_2 + \beta_3 \not= 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_2 + \beta_3 = 2$ против альтернативной $H_a: \beta_2 + \beta_3 > 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_2 + \beta_3 = 2$ против альтернативной $H_a: \beta_2 + \beta_3 < 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\end{enumerate}
\end{problem}

\begin{solution}
\begin{enumerate}
\item $n = 5$
\item $k = 3$
\item $TSS = 10$
\item $RSS = 2$
\item $\hb = \begin{pmatrix} \hb_1 \\ \hb_2 \\ \hb_3 \end{pmatrix} = (X'X)^{-1}X'y = \begin{pmatrix} 2 \\ 2 \\ 1 \end{pmatrix}$
\item $R^2 = 1 - \frac {RSS}{TSS} = 0.8.$ $R^2$ высокий, построенная эконометрическая модель <<хорошо>> описывает данные
\item Основная гипотеза --- $H_0: \beta_2 = 0$, альтернативная гипотеза --- $H_a: \beta_2 \not= 0$
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 - \beta_2}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - \beta_2}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hb_2 - 0}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - 0}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-0}{\sqrt{{\frac{2}{5-3}}1.3333}} = 1.7321$
\item Нижняя граница $= -2.920$, верхняя граница $= 2.920$
\item Поскольку $T_{obs} = 1.7321$, что принадлежит промежутку от -2.920 до 2.920, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item $p-value(T_{obs}) = \P (|T|>|T_{obs}|) = 2F_{T}(|T_{obs}|)$, где $F_{T}(|T_{obs}|)$ --- функция распределения $t-$распределения с $n - k = 5 - 3 = 2$ степенями свободы в точке $|T_{obs}|$. $p-value(T_{obs}) = 2tcdf(-|T_{obs}|, n - k) = 2tcdf(-1.7321,2) = 0.2253$. Поскольку $P-$значение превосходит уровень значимости $10\%$, то основная гипотеза --- $H_0: \beta_2 = 0$ не может быть отвергнута
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 - \beta_2}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - \beta_2}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hb_2 - 1}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - 1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-1}{\sqrt{{\frac{2}{5-3}}1.3333}} = 0.8660$
\item Нижняя граница $= -2.920$, верхняя граница $= 2.920$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от -2.920 до 2.920, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 - \beta_2}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - \beta_2}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hb_2 - 1}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - 1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-1}{\sqrt{{\frac{2}{5-3}}1.3333}} = 0.8660$
\item Нижняя граница $= -\infty$, верхняя граница $= 1.8856$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $-\infty$ до $1.8856$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 - \beta_2}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - \beta_2}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hb_2 - 1}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - 1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-1}{\sqrt{{\frac{2}{5-3}}1.3333}} = 0.8660$
\item Нижняя граница $= -1.8856$, верхняя граница $= +\infty$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $-1.8856$ до $+\infty$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item Основная гипотеза --- $H_0: \beta_2 = \beta_3 = 0$, альтернативная гипотеза --- $H_a: |\beta_2| + |\beta_3| > 0$
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{R^2}{1 - R^2} \cdot \frac{n-k}{k}; n = 5; k = 3$
\item $T \sim F(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{R^2}{1 - R^2} \cdot \frac{n-k}{k} = \frac{0.8}{1 - 0.8} \cdot \frac{5-3}{2} = 4$
\item Нижняя граница $= 0$, верхняя граница $= 19$
\item Поскольку $T_{obs} = 4$, что принадлежит промежутку от $0$ до $19$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$. Следовательно, регрессия в целом незначима. Напомним, что $R^2 = 0.8$, то есть он высокий. Но при этом регрессия <<в целом>> незначима. Такой эффект может возникать при малом объёме выборки, например, таком, как в данной задаче
\end{enumerate}
\item $p-value(T_{obs}) = \P (|T|>|T_{obs}|) = 2F_{T}(|T_{obs}|)$, где $F_{T}(|T_{obs}|)$ --- функция распределения $F-$распределения c $k = 3$ и $n - k = 5 - 3 = 2$ степенями свободы в точке $T_{obs}$. $p-value(T_{obs}) = 1 - fcdf(-|T_{obs}|, n - k) = 1 - fcdf(4,2) = 0.2$. Поскольку $P-$значение превосходит уровень значимости $10\%$, то основная гипотеза --- $H_0: \beta_2 = \beta_3 = 0$ не может быть отвергнута. Таким образом, регрессия <<в целом>> незначима
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 + \hb_3 - (\beta_2 + \beta_3)}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}}$, где $\widehat{\Var}(\hb_2 + \hb_3) = \widehat{\Var}(\hb_2) + \widehat{\Var}(\hb_3) + 2\widehat{\Cov}(\hb_2;\hb_3) = \hs^2 [(X'X)^{-1}]_{22} + 2\hs^2 [(X'X)^{-1}]_{23} + \hs^2 [(X'X)^{-1}]_{33}= \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33})$
\item $T \sim t(n-k); n = 5; k = 3$
\item $\widehat{\Var}(\hb_2 + \hb_3) = \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33}) = \frac{2}{5 - 3} (1.3333 + 2(-1.0000) + 2.0000) = 1.3333.$ Тогда $T_{obs} = \frac{\hb_2 + \hb_3 - 2}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} = \frac{2 + 1 - 2}{\sqrt{1.3333}} = 0.8660$
\item Нижняя граница $= - 4.3027$, верхняя граница $= 4.3027$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $- 4.3027$ до $4.3027$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 + \hb_3 - (\beta_2 + \beta_3)}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}}$, где $\widehat{\Var}(\hb_2 + \hb_3) = \widehat{\Var}(\hb_2) + \widehat{\Var}(\hb_3) + 2\widehat{\Cov}(\hb_2;\hb_3) = \hs^2 [(X'X)^{-1}]_{22} + 2\hs^2 [(X'X)^{-1}]_{23} + \hs^2 [(X'X)^{-1}]_{33}= \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33})$
\item $T \sim t(n-k); n = 5; k = 3$
\item $\widehat{\Var}(\hb_2 + \hb_3) = \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33}) = \frac{2}{5 - 3} (1.3333 + 2(-1.0000) + 2.0000) = 1.3333.$ Тогда $T_{obs} = \frac{\hb_2 + \hb_3 - 2}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} = \frac{2 + 1 - 2}{\sqrt{1.3333}} = 0.8660$
\item Нижняя граница $= - \infty$, верхняя граница $= 2.9200$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $- \infty$ до $2.9200$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 + \hb_3 - (\beta_2 + \beta_3)}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}}$, где $\widehat{\Var}(\hb_2 + \hb_3) = \widehat{\Var}(\hb_2) + \widehat{\Var}(\hb_3) + 2\widehat{\Cov}(\hb_2;\hb_3) = \hs^2 [(X'X)^{-1}]_{22} + 2\hs^2 [(X'X)^{-1}]_{23} + \hs^2 [(X'X)^{-1}]_{33}= \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33})$
\item $T \sim t(n-k); n = 5; k = 3$
\item $\widehat{\Var}(\hb_2 + \hb_3) = \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33}) = \frac{2}{5 - 3} (1.3333 + 2(-1.0000) + 2.0000) = 1.3333.$ Тогда $T_{obs} = \frac{\hb_2 + \hb_3 - 2}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} = \frac{2 + 1 - 2}{\sqrt{1.3333}} = 0.8660$
\item Нижняя граница $= - 2.9200$, верхняя граница $= + \infty$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $-  2.9200$ до $+ \infty$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$
\end{enumerate}
\end{enumerate}
\end{solution}




\begin{problem}
Пусть $y = X\beta + \e$ --- регрессионная модель, где $X = \begin{pmatrix} 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 1 \end{pmatrix}$, $y = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \\ 5 \end{pmatrix}$, $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$, $\e = \begin{pmatrix} \e_1 \\ \e_2 \\ \e_3 \\ \e_4 \\ \e_5  \end{pmatrix}$, $\E(\e)$ = 0, $Var(\e) = \sigma^2 I$.



На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_1 + \beta_2 = 2$ против альтернативной $H_a: \beta_1 + \beta_2 \not= 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики  при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\end{problem}
\begin{solution}
\end{solution}



\begin{problem}
По 13 наблюдениям Вася оценил модель со свободным членом, пятью количественными регрессорами и двумя качественными. Качественные регрессоры Вася правильно закодировал с помощью дамми-переменных. Одна качественная переменная принимала четыре значения, другая --- пять.

\begin{enumerate}
\item Найдите $SSR$, $R^2$
\item Как выглядит матрица $X(X'X)^{-1}X'$?
\item Почему 13 --- несчастливое число?
\end{enumerate} 
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
В рамках классической линейной модели найдите все математические ожидания и все ковариационные матрицы всех пар случайных векторов: $\ve$, $y$, $\hy$, $\he$, $\hb$. Т.е. найдите $\E(\e)$, $\E(y)$, \ldots и $\Cov(\e,y)$, $\Cov(\e,\hy)$, \ldots
\end{problem}

\begin{solution}
$\Var(\hb)=\sigma^2 (X'X)^{-1}$
\end{solution}


\begin{problem}
Найдите $\E(\sum (\e_i-\bar{\e})^2 )$, $\E(RSS)$
\end{problem}

\begin{solution}
$(n-1)\sigma^2$, $(n-k)\sigma^2$
\end{solution}


\begin{problem}
Используя матрицы $P=X(X'X)^{-1}X'$ и $\pi=\v1(\v1'\v1)^{-1}\v1'$ запишите $RSS$, $TSS$ и $ESS$ в матричной форме
\end{problem}
\begin{solution}
$TSS=y'(I-\pi)y$, $RSS=y'(I-P)y$, $ESS=y'(P-\pi)y$ 
\end{solution}



\begin{problem}
Найдите $\E(TSS)$, $\E(ESS)$. Надо быть морально готовым к тому, что они выйдут громоздкие 
\end{problem}
\begin{solution}
$\E(TSS)=(n-1)\sigma^2+\beta'X'(I-\pi)X\beta$
\end{solution}


\begin{problem}
Вася строит регрессию $y$ на некий набор объясняющих переменных и константу. А на самом деле $y_i=\beta_1+\e_i$. Чему равно $\E(TSS)$, $\E(RSS)$, $\E(ESS)$ в этом случае?
\end{problem}

\begin{solution}
$(n-1)\sigma^2$, $(n-k)\sigma^2$, $(k-1)\sigma^2$
\end{solution}



\begin{problem}
Рассмотрим классическую линейную модель. Являются ли векторы $\he$ и $\hy$ перпендикулярными? Найдите $\Cov(\he,\hy)$
\end{problem}
\begin{solution}
\end{solution}

\begin{problem}
Чему в классической модели регрессии равны $\E(\e)$, $\E(\he)$? Верно ли что $\sum \e_i$ равна 0? Верно ли что $\sum \he_i$ равна 0?
\end{problem}
\begin{solution}
$\E(\e)=0$, $\E(\he)=0$, $\sum \e_i$ может оказаться равной нулю только случайно, в нормальной модели это происходит с вероятностью 0, $\sum \he_i=0$ в модели со свободным членом
\end{solution}


\begin{problem}
Найдите на Картинке все перпендикулярные векторы. Найдите на Картинке все прямоугольные треугольники. Сформулируйте для них теоремы Пифагора.
\end{problem}

\begin{solution}
$\sum y_i^2=\sum \hy_i^2+\sum \he_i^2$, $TSS=ESS+RSS$, 
\end{solution}


\begin{problem}
Покажите на Картинке TSS, ESS, RSS, $R^2$, $\sCov(\hy,y)$
\end{problem}
\begin{solution}
\end{solution}

\begin{problem}
Предложите аналог $R^2$ для случая, когда константа среди регрессоров отсутствует. Аналог должен быть всегда в диапазоне $[0;1]$, совпадать с обычным $R^2$, когда среди регрессоров есть константа, равняться единице в случае нулевого $\he$.
\end{problem}

\begin{solution}
Спроецируем единичный столбец на <<плоскость>>, обозначим его $1'$. Делаем проекцию $y$ на <<плоскость>> и на $1'$. Далее аналогично. 
\end{solution}


\begin{problem}
Вася оценил регрессию $y$ на константу, $x$ и $z$. А затем, делать ему нечего, регрессию $y$ на константу и полученный $\hy$. Какие оценки коэффициентов у него получатся? Чему будет равна оценка дисперсии коэффицента при $\hy$? Почему оценка коэффициента неслучайна, а оценка её дисперсии положительна?
\end{problem}

\begin{solution}
Проекция $y$ на $\hy$ это $\hy$, поэтому оценки коэффициентов будут 0 и 1. Оценка дисперсии $\frac{RSS}{(n-2)ESS}$. Нарушены предпосылки теоремы Гаусса-Маркова, например, ошибки новой модели в сумме дают 0, значит коррелированы. 
\end{solution}



\begin{problem}
При каких условиях $TSS=ESS+RSS$?
\end{problem}
\begin{solution}
Либо в регрессию включена константа, либо единичный столбец (тут была опечатка, столбей) можно получить как линейную комбинацию регрессоров, например, включены дамми-переменные для каждого возможного значения качественной переменной.
\end{solution}




\begin{problem}
Истинная модель имеет вид $y=X\beta +\e$. Вася оценивает модель $\hy=X \hb$ по первой части выборки, получает $\hb_a$, по второй части выборки --- получает $\hb_b$ и по всей выборке --- $\hb_{tot}$. Как связаны между собой $\hb_a$, $\hb_b$, $\hb_{tot}$? Как связаны между собой ковариационные матрицы $\Var(\hb_a)$,  $\Var(\hb_b)$ и  $\Var(\hb_{tot})$?
\end{problem}

\begin{solution}
Сами оценки коэффициентов никак детерминистически не связаны, но при большом размере подвыборок примерно равны. А ковариационные матрицы связаны соотношением $\Var(\hb_a)^{-1}+\Var(\hb_b)^{-1}=\Var(\hb_{tot})^{-1}$ 
\end{solution}




\begin{problem}
Модель линейной регрессии имеет вид $y_i=\b_1 x_{i,1}+\b_2 x_{i,2} + u_i$.
Сумма квадратов остатков имеет вид $Q\left(\hb_1,\hb_2\right)=\sum_{i=1}^n (y_1-\hb_1 x_{i,1}-\hb_2 x_{i,2})^2$.
\begin{enumerate}
\item Выпишите необходимые условия минимума суммы квадратов остатков
\item Найдите матрицу $X'X$ и вектор $X'y$ если матрица $X$ имеет вид
$X=
\left(
\begin{array}{cc}
x_{1,1} & x_{1,2} \\
\vdots & \vdots \\
x_{n,1} & x_{n,2}
\end{array}
\right)
$,
а вектор $y$ имеет вид
$y=
\left(
\begin{array}{c}
y_1 \\
\vdots \\
y_n
\end{array}
\right)
$
\item Докажите, что необходимые условия равносильны матричному уравнению $X'X\hb=X'y$, где
$\hb=
\left(
\begin{array}{c}
\hb_1 \\
\hb_2
\end{array}
\right)
$
\item Предполагая, что матрица $X'X$ обратима, найдите $\hb$
\end{enumerate}
\end{problem}
\begin{solution}
\end{solution}



\begin{problem}
Вася оценил исходную модель:
\[
y_i=\b_1+\b_2 x_i + u_i
\]

Для надежности Вася стандартизировал переменные, т.е. перешёл к $y_i^*=(y_i-\bar{y})/s_y$ и $x_i^*=(x_i-\bar{x})/s_x$. Затем Вася оценил ещё две модели:

\[
y_i^*=\b_1'+\b_2' x^*_i + u_i'
\]

и 
\[
y_i^*=\b_2'' x^*_i + u_i''
\]

В решении можно считать $s_x$ и $s_y$ известными.
 
\begin{enumerate}
\item Найдите $\hb_1'$
\item Как связаны между собой $\hb_2$, $\hb_2'$ и $\hb_2''$? 
\item Как связаны между собой $\hat{u}_i$, $\hat{u}_i'$ и $\hat{u}_i''$?
\item Как связаны между собой $\hVar\left(\hb_2\right)$, $\hVar\left(\hb_2'\right)$ и $\hVar\left(\hb_2''\right)$?
\item Как выглядит матрица $\hVar\left(\hb'\right)$?
\item Как связаны между собой $t$-статистики $t_{\hb_2}$, $t_{\hb_2'}$ и $t_{\hb_2''}$?
\item Как связаны между собой $R^2$, $R^{2\prime}$ и $R^{2\prime\prime}$?
\item В нескольких предложениях прокомментируйте последствия перехода к стандартизированным переменным
\end{enumerate}
\end{problem}
\begin{solution}
\end{solution}



\begin{problem}
Регрессионная модель  задана в матричном виде при помощи уравнения $y=X\beta+\varepsilon$, где $\beta=(\beta_1,\beta_2,\beta_3)'$.
Известно, что $\E(\varepsilon)=0$  и  $\Var(\varepsilon)=\sigma^2\cdot I$.
Известно также, что 

$y=\left(
\begin{array}{c} 
1\\ 
2\\ 
3\\ 
4\\ 
5
\end{array}\right)$, 
$X=\left(\begin{array}{ccc}
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 1 & 1 
\end{array}\right)$.


Для удобства расчетов приведены матрицы 


$X'X=\left(
\begin{array}{ccc} 
5 & 2 & 1\\ 
2 & 2 & 1\\ 
1 & 1 & 1 
\end{array}\right)$ и $(X'X)^{-1}=\frac{1}{3}\left(
\begin{array}{ccc} 
1 & -1 & 0 \\
-1 & 4 & -3 \\
0 & -3 & 6
\end{array}\right)$.

\begin{enumerate}
\item Укажите число наблюдений.
\item Укажите число регрессоров с учетом свободного члена.
\item Запишите модель в скалярном виде
\item Рассчитайте $TSS=\sum (y_i-\bar{y})^2$, $RSS=\sum (y_i-\hat{y}_i)^2$ и $ESS=\sum (\hat{y}_i-\bar{y})^2$.
\item Рассчитайте при помощи метода наименьших квадратов $\hb$, оценку для вектора неизвестных коэффициентов.
\item Чему равен $\he_5$, МНК-остаток регрессии, соответствующий 5-ому наблюдению?
\item Чему равен $R^2$  в модели? Прокомментируйте полученное значение с точки зрения качества оцененного уравнения регрессии.
\item Используя приведенные выше данные, рассчитайте несмещенную оценку для неизвестного параметра $\sigma^2$ регрессионной модели.
\item Рассчитайте $\widehat{\Var}(\hb)$, оценку для ковариационной матрицы вектора МНК-коэффициентов $\hb$.  
\item Найдите $\widehat{\Var}(\hb_1)$, несмещенную оценку дисперсии МНК-коэффициента $\hb_1$.
\item Найдите $\widehat{\Var}(\hb_2)$, несмещенную оценку дисперсии МНК-коэффициента $\hb_2$.
\item Найдите $\widehat{\Cov}(\hb_1,\hb_2)$, несмещенную оценку ковариации МНК-коэффициентов $\hb_1$ и $\hb_2$.
\item Найдите $\widehat{\Var}(\hb_1+\hb_2)$, $\widehat{\Var}(\hb_1-\hb_2)$, $\widehat{\Var}(\hb_1+\hb_2+\hb_3)$, $\widehat{\Var}(\hb_1+\hb_2-2\hb_3)$
\item Найдите $\hCorr(\hb_1,\hb_2)$, оценку коэффициента корреляции МНК-коэффициентов $\hb_1$ и $\hb_2$.
\item Найдите $s_{\hb_1}$, стандартную ошибку МНК-коэффициента $\hb_1$.
\item Рассчитайте выборочную ковариацию $y$ и $\hy$.
\item Найдите выборочную дисперсию $y$, выборочную дисперсию $\hy$.
\end{enumerate}
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
Теорема Фриша-Вау. Регрессоры разбиты на две группы: матрицу $X_1$ размера $n\times k_1$ и матрицу $X_2$ размера $n\times k_2$. Рассмотрим две процедуры:
\begin{enumerate}
\item[M1.] Строим регрессию вектора $y$ на все регрессоры, т.е. оцениваем модель:
\[ 
y=X\beta + \e = X_1 \beta_1 + X_2 \beta_2 +\e 
\]
\item[M2.] Процедура из двух шагов:
\begin{enumerate}
\item Строим регрессию вектора $y$ на все регрессоры первой группы и получаем вектор остатков $M_1 y$, где $M_1=I-X_1(X_1'X_1)^{-1}X_1'$. Строим регрессию каждого регрессора из второй группы на все регрессоры первой группы и получаем в каждом случае вектор остатков. Эти остатки можно записать матрицей $M_1 X_2$. 
\item Строим регрессию вектора $M_1 y$ на остатки $M_1 X_2$.
\end{enumerate}
Другими словами мы оцениваем модель:
\[
M_1 y = M_1 X_2 \gamma_2 + u
\]

\end{enumerate}
\begin{enumerate}
\item Верно ли, что МНК оценки коэффициентов $\hb_2$ и $\hat{\gamma}_2$ совпадают?
\item Верно ли, что остатки в обеих регрессиях совпадают?
\end{enumerate}
\end{problem}
\begin{solution}
Подсказка: запишите матрицу $X$ как блочную и, пользуясь матричным выражением для $\hb$ и формулой Фробениуса, найдите $\hb_2$.

1. Да, верно.
$X=(X_1 X_2)$ --- блочная матрица. Аналогично, $\hat\beta=\left(\begin{array}{c}
\hat\beta_1\\ 
\hat\beta_2
\end{array}\right)$ --- блочная матрица (хотя на самом деле вектор).

\begin{multline*}
\hat\beta=(X'X)^{-1}X'y=((X_1X_2)'(X_1X_2))^{-1}(X_1X_2)'y=(\left(\begin{array}{c}
X_1'\\ 
X_2'
\end{array}\right)(X_1X_2))^{-1}(\left(\begin{array}{c}
X_1'\\ 
X_2'
\end{array}\right)y=\\=
\left(\begin{array}{cc}
X_1'X_1 & X_1'X_2\\ 
X_2'X_1 & X_2'X_2
\end{array}\right)^{-1}\left(\begin{array}{c}
X_1'\\ 
X_2'
\end{array}\right)y
\end{multline*}

Запишем и докажем формулу Фробениуса для обращения блочных матриц.

Формула Фробениуса:
\[
\begin{bmatrix} A & B \\ 
C & D \\ 
\end{bmatrix}^{-1}=
\begin{pmatrix} 
A^{-1} +A^{-1} BH^{-1} CA^{-1}  & -A^{-1} BH^{-1}\\
 -H^{-1} CA^{-1}  & H^{-1}
\end{pmatrix} 
\]
где $H=D-CA^{-1}B$.

Докажем формулу, обращая матрицу методом Гаусса. Умножим слева на $\left(\begin{array}{cc}
A^{-1} & 0\\ 
0 & I
\end{array}\right)$
\begin{multline*}
\left(\begin{array}{cc|cc}
A & B & I & 0\\ 
C & D & 0 & I
\end{array}\right)=
\left(\begin{array}{cc|cc}
I & A^{-1}B & A^{-1} & 0\\ 
C & D & 0 & I
\end{array}\right)=\\
\end{multline*}
вычтем из второй строки первую, умноженную на $C$
\begin{multline*}
=\left(\begin{array}{cc|cc}
I & A^{-1}B & A^{-1} & 0\\ 
0 & D-CA^{-1}B & -CA^{-1} & I
\end{array}\right)=
\end{multline*}
умножим слева на $\left(\begin{array}{cc}
I & 0\\ 
0 & (D-CA^{-1}B)^{-1}
\end{array}\right)$
\begin{multline*}
=\left(\begin{array}{cc|cc}
I & A^{-1}B & A^{-1} & 0\\ 
0 & I & -(D-CA^{-1}B)^{-1}CA^{-1} & (D-CA^{-1}B)^{-1}
\end{array}\right)=
\end{multline*}
вычтем из первой строки вторую, умноженную на $A^{-1}B$. 
\begin{multline*}
=\left(\begin{array}{cc|cc}
I & 0 & A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} & -A^{-1}B(D-CA^{-1}B)^{-1}\\ 
0 & I & -(D-CA^{-1}B)^{-1}CA^{-1} & (D-CA^{-1}B)^{-1}
\end{array}\right)
\end{multline*}

Значит,
\begin{multline*}
\begin{bmatrix} A & B \\ C & D \\ \end{bmatrix}^{-1}
=\left(\begin{array}{cc}
A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} & -A^{-1}B(D-CA^{-1}B)^{-1}\\ 
-(D-CA^{-1}B)^{-1}CA^{-1} & (D-CA^{-1}B)^{-1}
\end{array}\right)=\\=
\begin{pmatrix} A^{-1} +A^{-1} BH^{-1} CA^{-1}  & -A^{-1} BH^{-1}\\ -H^{-1} CA^{-1}  & H^{-1}\end{pmatrix}
\end{multline*}

По формуле Фробениуса получим, что
\[\left(\begin{array}{cc}
X_1'X_1 & X_1'X_2\\ 
X_2'X_1 & X_2'X_2
\end{array}\right)^{-1}=
\left(\begin{array}{cc}
(X_1'X_1)^{-1}+(X_1'X_1)^{-1}X_1'X_2H^{-1}X_2'X_1(X_1'X_1)^{-1} & -(X_1'X_1)^{-1}X_1'X_2H^{-1}\\ 
-H^{-1}X_2'X_1(X_1'X_1)^{-1} & H^{-1}
\end{array}\right),\]
где $H=X_2'X_2-X_2'X_1(X_1'X_1)^{-1}X_1'X_2$. Верхняя строка в данном пункте не важна, и сейчас её опустим. Заметим, что
\[H=X_2'X_2-X_2'X_1(X_1'X_1)^{-1}X_1'X_2=X_2'(I-X_1(X_1'X_1)^{-1}X_1')X_2=X_2'M_1X_2\]
Итак,
\begin{multline*}
\hat\beta=\left(\begin{array}{c}
\hat\beta_1\\ 
\hat\beta_2
\end{array}\right)=
\left(\begin{array}{cc}
X_1'X_1 & X_1'X_2\\ 
X_2'X_1 & X_2'X_2
\end{array}\right)^{-1}\left(\begin{array}{c}
X_1'\\ 
X_2'
\end{array}\right)y=\\=\left(\begin{array}{cc}
? & ?\\
-H^{-1}X_2'X_1(X_1'X_1)^{-1} & H^{-1}
\end{array}\right)\left(\begin{array}{c}
X_1'\\ 
X_2'
\end{array}\right)y=\\=\left(\begin{array}{c}
?\\ 
-H^{-1}X_2'X_1(X_1'X_1)^{-1}X_1'+H^{-1}X_2'
\end{array}\right)y=\\=
\left(\begin{array}{c}
?\\ 
H^{-1}X_2'(I-X_1(X_1'X_1)^{-1}X_1')
\end{array}\right)y=\\=
\left(\begin{array}{c}
?\\ 
H^{-1}X_2'M_1y
\end{array}\right)=
\left(\begin{array}{c}
?\\ 
(X_2'M_1X_2)^{-1}X_2'M_1y
\end{array}\right)
\end{multline*}

\[\hb_2=(X_2'M_1X_2)^{-1}X_2'M_1y\]
Заметим свойства матрицы-проектора $M_1$. 
\[M_1'=(I-X_1(X_1'X_1)^{-1}X_1')'=I-X_1(X_1'X_1)^{-1}X_1'=M_1\]
\begin{multline*}
(M_1)^2=(I-X_1(X_1'X_1)^{-1}X_1')^2=I-2X_1(X_1'X_1)^{-1}X_1'+X_1(X_1'X_1)^{-1}X_1'\cdot X_1(X_1'X_1)^{-1}X_1'=\\
=I-X_1(X_1'X_1)^{-1}X_1'=M_1
\end{multline*}

Значит,
\begin{multline*}
\hb_2=(X_2'M_1X_2)^{-1}X_2'M_1y=(X_2'M_1M_1X_2)^{-1}X_2'M_1M_1y=(X_2'M_1'M_1X_2)^{-1}X_2'M_1'M_1y=\\=
((M_1X_2)'M_1X_2)^{-1}(M_1X_2)'M_1y
\end{multline*}
но ведь и
\[
\hat\gamma_2=
((M_1X_2)'M_1X_2)^{-1}(M_1X_2)'M_1y
\]
Значит, $\hb_2=\hat\gamma_2=
((M_1X_2)'M_1X_2)^{-1}(M_1X_2)'M_1y$
, что и требовалось доказать.

2. Да, верно.
\[\hy=X_1\hb_1+X_2\hb_2\]

\[M_1 \he=M_1y-M_1\hy=M_1y-M_1(X_1\hat\beta_1+X_2\hb_2)=M_1y-M_1X_2\hb_2-M_1X_1\hat\beta_1\]
\[M_1X_1=(I-X_1(X_1'X_1)^{-1}X_1')X_1=X_1-X_1(X_1'X_1)^{-1}X_1'X_1=0\]
\[M_1 \he=M_1y-M_1X_2\hb_2=M_1y-M_1X_2\hat\gamma_2=\hat u\]

\begin{multline*}
M_1\he=M_1(y-\hy)=M_1(I-X(X'X)^{-1}X')y=(I-X(X'X)^{-1}X')y
\end{multline*}
так как $M_1$ ортогональное дополнение к $X_1$, а $(I-X(X'X)^{-1}X')y$ уже лежит в ортогональном дополнении к $X_1$, так как $I-X(X'X)^{-1}X'$ ортогональное дополнение к к прямой сумме пространств $X_1$ и $X_2$ ---$X_1\oplus X_2$.\end{solution}



\begin{problem}
Всего имеется $100$ наблюдений. Для первых 50-ти наблюдений $X'X=\begin{pmatrix}
50 & 300 \\
300 & 2100
\end{pmatrix}$, $X'y=\begin{pmatrix}
300 & 2000
\end{pmatrix}'$, $y'y=2100$. По последним 50-ти наблюдениям: $X'X=\begin{pmatrix}
50 & 300 \\
300 & 2100
\end{pmatrix}$, $X'y=\begin{pmatrix}
300 & 2200
\end{pmatrix}'$, $y'y=2500$. По первым 50-ти наблюдениям оценивается модель $y_i = \beta_1 + \beta_2 x_i + \e_i$, по последним 50-ти наблюдениям оценивается модель $y_i = \gamma_1 + \gamma_2 x_i + \e_i$. Предположеним, что во всех 100 наблюдениях $\e_i$ независимы и нормальны $N(0;\sigma^2)$. На уровне значимости 5\% проверьте гипотезу $H_0: \, \beta=\gamma$.
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
Докажите, что МНК-оценки $\hb = (X^T X)^{-1} X^T y$ являются несмещенными и линейными по переменной $y$.
\end{problem}

\begin{solution}
Докажем несмещенность МНК-оценок.
\[\E\hb = \E\left( (X^T X)^{-1} X^T y \right) = (X^T X)^{-1} X^T \E(y) = \]
\[= (X^T X)^{-1} X^T \E(X\beta + \e) = (X^T X)^{-1} X^T X\beta = \beta\]
Обозначим $\varphi(X, y) = (X^T X)^{-1} X^T y$. Тогда $\hb = \varphi(X, y)$. Покажем, что функция $\varphi$ линейна по переменной $y$.
\begin{enumerate}
\item $\varphi(X, \lambda \cdot y) = (X^T X)^{-1} X^T (\lambda \cdot y) = \lambda (X^T X)^{-1} X^T y = \lambda \cdot \varphi(X, y)$
\item $\varphi(X, y + z) = (X^T X)^{-1} X^T (y + z) = (X^T X)^{-1} X^T y + (X^T X)^{-1} X^T z = \varphi(X, y) + \varphi(X, z)$
\end{enumerate}
Что и требовалось доказать.
\end{solution}

\begin{problem}
Являются ли МНК-оценки линейными по переменной $X$?
\end{problem}

\begin{solution}
Нет, так как для функции $\varphi(X, y) = (X^T X)^{-1} X^T y$ не выполнено, например, свойство однородности по переменной $X$. Действительно,
\[\varphi(X, \lambda \cdot y) = ((\lambda \cdot X)^T (\lambda \cdot X))^{-1} (\lambda \cdot X)^T y = \frac{1}{\lambda} \cdot (X^T X)^{-1} X^T y = \frac{1}{\lambda} \varphi(X, y)\].
\end{solution}

\begin{problem}
Приведите пример несмещенной и линейной по переменной $y$ оценки, отличной от МНК.
\end{problem}

\begin{solution} 
$\tilde{\beta} = (X^T CX)^{-1} X^T Cy$, где
\[C = \begin{bmatrix}
1 & 0 & 0 & \cdots & 0 \\
0 & 2 & 0 & \cdots & 0 \\
0 & 0 & 3 & \ddots & \vdots \\
\vdots & \vdots & \ddots & \ddots & 0 \\
0 & 0 & \cdots & 0 & n \\
\end{bmatrix} \]
\end{solution}


\begin{problem}
Если для регрессии $y = X\beta + \e$ не выполняется условие $Pi = i$, где $i$ --- единичный столбец, а $P \equiv X(X^TX)^{-1}X^T$, $\pi \equiv \frac{ii^T}{i^Ti}$, то будут неверны равенства:
\begin{enumerate}
\begin{minipage}[h]{0.49\linewidth}
\item[(1)] $P\pi = \pi$
\item[(2)] $P^2 = P$
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\item[(3)] $\sum_{i=1}^n \hat{\e}_i = 0$
\item[(4)] $\overline{Y} = \overline{\hat{Y}}$
\end{minipage}
\end{enumerate}
\end{problem}

\begin{solution} 
$Pi = i \Leftrightarrow P\pi = \pi$ поскольку, если матрицу $\pi$ записать по столбцам $\pi = \frac{1}{n}\begin{bmatrix}
i & i & \ldots & i
\end{bmatrix}$, то можно записать следующую цепочку равенств $P\pi = P\frac{1}{n}\begin{bmatrix}
i & i & \ldots & i
\end{bmatrix} = \frac{1}{n}\begin{bmatrix}
Pi & Pi & \ldots & Pi
\end{bmatrix} = \frac{1}{n}\begin{bmatrix}
i & i & \ldots & i
\end{bmatrix} \Leftrightarrow Pi = i$.

Свойство $P^2 = P$ имеет место независимо от выполнимости условия $Pi = i$. Действительно, $P^2 = X (X^T X)^{-1}X^TX(X^TX)^{-1}X^T = X(X^TX)^{-1}X^T = P$.

Рассмотрите пример $y = \begin{bmatrix}
1 & -1 & 0
\end{bmatrix}^T$, $x = \begin{bmatrix}
1 & 0 & -1
\end{bmatrix}^T$. Постройте регрессию $y = \beta x + \e$ без свободного члена. Убедитесь, что $\sum_{i=1}^n \hat{\e}_i = 0$ и $\overline{Y} = \overline{\hat{Y}} = 0$, но $Pi \neq i$.


Ответ: $P\pi = \pi$
\end{solution}



\begin{problem}
Необходимыми условиями теоремы Гаусса-Маркова являются
\begin{enumerate}
\item Правильная специфицикация модели: $y = X\beta + \e$,
\item Полный ранг матрицы $X$,
\item Невырожденность матрицы $X^T X$
\item Нормальность распределения случайной составляющей
\item Скалярность (пропорциональность единичной матрице) ковариационной матрицы случайной составляющей,
\item Наличие в матрице $X$ единичного столбца
\end{enumerate}
\end{problem}

\begin{solution}
(1), (2) $\Leftrightarrow$ (3), (5)
\end{solution}


\begin{problem}
Для регрессии $y = X\beta + \e$ с $\E(\e) = 0$, $\Var(\e) = \begin{bmatrix}
\sigma_1^2 &  & 0 \\
  & \ddots &   \\
0 &  & \sigma^2_n
\end{bmatrix}$ найдите математическое ожидание квадратичной формы $\e^T \pi \e$.
\end{problem}


\begin{solution}
\begin{multline}
\E(\e^T \pi \e) = \E(\tr[\e^T \pi \e]) = \E(\tr[\pi \e \e^T]) = \tr[\pi \E(\e\e^T)]  =\\
\tr[\pi \Var(\e)] = \tr\left[ \frac{1}{n} \begin{bmatrix}
1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 \\
\ldots & \ldots & \ldots & \ldots \\
1 & 1 & 1 & 1 \\
\end{bmatrix} \begin{bmatrix}
\sigma_1^2 & 0 & \ldots & 0 \\
0 & \sigma_2^2 & \ldots & 0 \\
\ldots & \ldots & \ldots & \ldots \\
0 & \ldots & 0 & \sigma_n^2 \\
\end{bmatrix} \right] = \\
\frac{1}{n} tr\begin{bmatrix}
\sigma_1^2 & \sigma_2^2 & \ldots & \sigma_n^2 \\
\sigma_1^2 & \sigma_2^2 & \ldots & \sigma_n^2 \\
\ldots & \ldots & \ldots & \ldots \\
\sigma_1^2 & \sigma_2^2 & \ldots & \sigma_n^2 \\
\end{bmatrix} = \frac{1}{n} \sum_{i=1}^n \sigma_i^2
\end{multline}
\end{solution}

\begin{problem}
Рассмотрим регрессию, для которой выполнены условия теоремы Гаусса-Маркова. Уравнение регрессии имеет вид $\hat{y} = \hb_1 i + \hb_2 x_2 + \hb_3 x_3 + \hb_4 x_4$. Известны следующие данные:

\[
X^T X = \begin{bmatrix}
100 & 123 & 96 & 109 \\
 & 252 & 125 & 189 \\
 & & 167 & 146 \\
 & & & 168 \\
\end{bmatrix}
\]
\[
(X^TX)^{-1} = \begin{bmatrix}
0.03767 & & & \\
-0.06263 & 1.129 & & \\
-0.06247 & 1.107 & 1.110 & \\
0.1003 & -2.192 & -2.170 & 4.292 \\
\end{bmatrix}
\] 

$X^Ty = \begin{bmatrix}
460\\
810\\
615\\
712\\
\end{bmatrix}$; $y^Ty = 3924$

\begin{enumerate}
\item Найти $\widehat{\Corr}(\hb_1, \hb_2)$ (1 балл)
\item Найти $\widehat{\Corr}(x_2, x_3)$ (1 балл)
\item Проверить гипотезу $H_0: \beta_2 = 0$ (1 балл)
\end{enumerate}
\end{problem}

\begin{solution}
\begin{enumerate}
\item 
\begin{multline}
RSS = \hat{\e}^T\hat{\e} y^T (I - P) y = y^T y - y^TPy = y^Ty - y^T X(X^TX)^{-1}X^Ty ;
\end{multline}
При этом $y^Ty=3924$,  а
\begin{multline}
y^T X(X^TX)^{-1}X^Ty= \\
 \begin{bmatrix}
460 & 810 & 615 & 712
\end{bmatrix} \begin{bmatrix}
0.038 & -0.063 & -0.063 & 0.100 \\
-0.063 & 1.129 & 1.107 & -2.192 \\
-0.063 & 1.107 & 1.110 & -2.170 \\
0.100 & -2.192 & -2.170 & 4.292 \\
\end{bmatrix} \cdot \\
\cdot 
\begin{bmatrix}
460\\
810\\
615\\
712\\
\end{bmatrix} = 3051.2
\end{multline}
Итого, $RSS= 3924 - 3051.2 = 872.8$
 
$\hs_{\e}^2 = \frac{RSS}{n-k} = \frac{872.8}{100-4} = 9.0917$

$\widehat{\Var}(\hb) = \hs_{\e}^2 (X^T X)^{-1} \Rightarrow \widehat{\Cov}(\hb_1, \hb_2) = -0.56939$, $\widehat{\Var}(\hb_1) = 0.34251$, $\widehat{\Var}(\hb_2) = 10.269$

$\widehat{\Corr}(\hb_1, \hb_2) = \frac{\widehat{\Cov}(\hb_1, \hb_2)}{\sqrt{\widehat{\Var}(\hb_1)}\sqrt{\widehat{\Var}(\hb_2)}} = -0.30361$

\item(указание) $\widehat{\Corr}(x_2, x_3) = \frac{\sum (x_{i2} - \overline{x}_2) (x_{i3} - \overline{x}_3)}{\sqrt{\sum (x_{i2} - \overline{x}_2)}\sqrt{\sum (x_{i3} - \overline{x}_3)}}$. Все необходимые величины можно извлечь из матрицы $X^T X$ --- это величины $\sum x_{i2}$ и $\sum x_{i3}$, а остальное -- из матрицы $X^T (I - \pi) X = X^T X - X^T \pi X = X^T X - (\pi X)^T \pi X$. При этом имейте в виду, что
$\pi X = \begin{bmatrix}
1 & \overline{x}_1 & \overline{x}_2 & \overline{x}_3 \\
\ldots & \ldots & \ldots & \ldots \\
1 & \overline{x}_1 & \overline{x}_2 & \overline{x}_3 \\
\end{bmatrix}$ и $\overline{x}_1 = 1.23$, $\overline{x}_2 = 0.96$, $\overline{x}_3 = 1.09$

\item $\begin{bmatrix}
\hb_1 \\
\hb_2 \\
\hb_3 \\
\hb_4 \\
\hb_5 \\
\end{bmatrix} = \begin{bmatrix}
0.03767 & -0.06263 & -0.06247 & 0.1003 \\
-0.06263 & 1.129 & 1.107 & -2.192 \\
-0.06247 & 1.107 & 1.110 & -2.170 \\
0.1003 & -2.192 & -2.170 & 4.292 \\
\end{bmatrix} \begin{bmatrix}
460\\
810\\
615\\
712\\
\end{bmatrix} = \begin{bmatrix}
-0.40221 \\
6.1234 \\
5.9097 \\
-7.5256 \\
\end{bmatrix}$

$t = \frac{\hb_2}{\sqrt{\Var(\hb_2)}} \sim t_{100-4}$

$t = \frac{\hb_2}{\sqrt{\Var(\hb_2)}} = \frac{6.1234}{\sqrt{10.269}} = 1.9109 \Rightarrow \hb_2$ --- не значим.
\end{enumerate}
\end{solution}


\begin{problem}
По данным для 15 фирм ($n = 15$) была оценена производственная функция Кобба-Дугласа: $\ln Q_i = \beta_1 + \beta_2 \ln L_i + \beta_3 \ln K_i + \e_i$. Полученные оценки: 
\[\underset{s.e.}{\widehat{\ln Q}} = \underset{(4.48)}{0.5} + \underset{(0.7)}{0.76} \ln L + \underset{(0.138)}{0.19} \ln K\]
где $Q$ --- выпуск, $L$ --- трудозатраты, $K$ --- капиталовложения. Матрица обратная к матрице регрессоров имеет вид:

$(X^T X)^{-1} = \begin{bmatrix}
121573 & -19186 & 3718 \\
-19186 & 3030 & -589 \\
3718 & -589 & 116 \\
\end{bmatrix}$

Требуется:
\begin{enumerate}
\item Написать формулу для несмещенной оценки ковариации $\widehat{\Cov}(\hb_2, \hb_3)$ и вычислить её по имеющимся данным (если это возможно);
\item Проверить $H_0: \beta_2 + \beta_3 = 1$ при помощи t-статистики (обязательно требуется указать формулу для статистики, а также указать число степеней свободы);
\item Построить 95\% доверительный интервал для величины $\beta_2 + \beta_3$.
\end{enumerate}
\end{problem}

\begin{solution}
\begin{enumerate}
\item $\widehat{\Cov}\begin{bmatrix}
\hb_1 \\
\hb_2 \\
\hb_3 \\
\end{bmatrix} = \hs_{\e}^2 (X^TX)^{-1}$ --- несмещённая оценка для ковариационной матрицы \\ МНК-коэффициентов. Действительно, $\E\widehat{\Cov}\begin{bmatrix}
\hb_1 \\
\hb_2 \\
\hb_3 \\
\end{bmatrix} = \E\hs_{\e}^2 (X^TX)^{-1} = \sigma_{\e}^2 (X^TX)^{-1} = \Cov\begin{bmatrix}
\hb_1 \\
\hb_2 \\
\hb_3 \\
\end{bmatrix}$. Поэтому искомая оценка $\widehat{\Cov}(\hb_2, \hb_3) = \hs_{\e}^2 \left[ (X^T X)^{-1} \right]_{23}$, где $\left[ (X^T X)^{-1} \right]_{23}$ --- элемент матрицы $(X^T X)^{-1}$, расположенный во второй строке, 3-м столбце.

Заметим, что $\hs_{\hb_2}^2 = \hs_{\e}^2 \left[ (X^T X)^{-1} \right]_{22} \Rightarrow 0.7^2 = \hs_{\e}^2 \cdot (3030) \Rightarrow \hs_{\e}^2 = 0.00016172$

Значит, $\widehat{\Cov}(\hb_2, \hb_3) = 0.00016172 \cdot (-589) = -0.095253$.

\item $t = \frac{\hb_2 + \hb_3 - \beta_2 - \beta_3}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} \sim t_{n-k}$

Требуется проверить $H_0: \beta_2 + \beta_3 = 1$.

$\widehat{\Var}(\hb_2 + \hb_3) = \widehat{\Var}(\hb_2) + \widehat{\Var}(\hb_3) + 2\widehat{\Cov}(\hb_2, \hb_3) = 0.7^2 + 0.138^2 + 2 \cdot 0.095253 = 0.319044$

$t = \frac{\hb_2 + \hb_3 - \beta_2 - \beta_3}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} = \frac{0.76 + 0.19 - 1}{\sqrt{0.319044}} = -0.088520674$

Значит, гипотеза не отвергается на любом <<разумном>> уровне значимости.

\item Мы знаем, что $\frac{\hb_2 + \hb_3 - \beta_2 - \beta_3}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} \sim t_{n-k} = t_{15-3}$, поэтому построить доверительный интервал для $\beta_2 + \beta_3$ не составляет труда. $\P \left( \left| \frac{\hb_2 + \hb_3 - \beta_2 - \beta_3}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} \right| < t^* \right) = 0.95$

Обозначим $se=\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}$, тогда:

\begin{multline}
\P \left( \left| \frac{\hb_2 + \hb_3 - \beta_2 - \beta_3}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} \right| < t^* \right) = \\
\P \left( -t^* se < \hb_2 + \hb_3 - \beta_2 - \beta_3 < t^* se \right) = \\ 
\P \left( -t^*se  - (\hb_2 + \hb_3) < - \beta_2 - \beta_3  < -(\hb_2 + \hb_3) + t^* se \right) =\\ 
\P \left( (\hb_2 + \hb_3) + t^* se 
> \beta_2 + \beta_3 
> (\hb_2 + \hb_3) - t^* se \right) 
\end{multline}
Отсюда получаем доверительный интервал
\begin{multline}
\beta_2 + \beta_3 \in \\
[(0.76 + 0.19) - 2.16 \cdot 0.319;  (0.76 + 0.19) + 2.16 \cdot 0.319 ]
\end{multline}
Или $0.26< \beta_2 + \beta_3  < 1.639  $
\end{enumerate}
\end{solution}


\begin{problem}
Напишите формулу для оценок коэффициентов в множественной регрессии с матрицами. Напишите формулу для ковариационной матрицы оценок.
\end{problem}

\begin{solution}

Метод наименьших квадратов:
\[\he'\he\rightarrow \min_{\hb}\]
\[(y-\hy)'(y-\hy)\rightarrow\min_{\hb}\]
\[(y-X\hb)'(y-X\hb)\rightarrow\min_{\hb}\]
\[y'y-\hb' X'y-y'X\hb+\hb'X'X\hb\rightarrow\min_{\hb}\]

Воспользуемся тем, что : $\frac{\partial x'A}{\partial x'}=A'$, $\frac{\partial Ax}{\partial x'}=A$,
$\frac{\partial x'Ax}{\partial x'}=x'(A'+A)$
Условие первого порядка:
\[-2(X'y)'+(X'X+(X'X)')\hb'=0\]
\[-2X'y+2\hb X'X=0\]
и
\[\hb=(X'X)^{-1}X'y\]

\begin{multline*}
\Var(\hat\beta)=\Var((X'X)^{-1}X'y)=(X'X)^{-1}X'\Var(y)((X'X)^{-1}X')'=\\
=(X'X)^{-1}X'\Var(X\beta+\epsilon)X(X'X)^{-1}
\end{multline*}
Так как $\beta$ является константой, то $\Var(X\beta)=0$. Тогда 
\begin{multline*}
\Var(\hat\beta)=(X'X)^{-1}X'\Var(X\beta+\epsilon)X(X'X)^{-1}=\\=(X'X)^{-1}X'\Var(\epsilon)X(X'X)^{-1}=
(X'X)^{-1}X'\sigma^2X(X'X)^{-1}=\\=\sigma^2(X'X)^{-1}X'X(X'X)^{-1}=\sigma^2(X'X)^{-1}
\end{multline*}
\end{solution}

\begin{problem}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model} \hlkwb{<-} \hlkwd{lm}\hlstd{(dist}\hlopt{~}\hlstd{speed,}\hlkwc{data}\hlstd{=cars)}
\hlstd{model.sum} \hlkwb{<-} \hlkwd{summary}\hlstd{(model)}
\hlstd{hat.sigma} \hlkwb{<-} \hlstd{model.sum}\hlopt{$}\hlstd{sigma}
\end{alltt}
\end{kframe}
\end{knitrout}

Исследователь оценил зависимость длины тормозного пути в футах от скорости автомобиля в милях в час по данным 1920-х годов.
При построении парной регрессии у него получилась $\hs=15.3795867$ и оценка ковариационной матрицы 


\begin{kframe}
\begin{alltt}
\hlkwd{xtable}\hlstd{(}\hlkwd{vcov}\hlstd{(model))}
\end{alltt}
\end{kframe}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:03 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & (Intercept) & speed \\ 
  \hline
(Intercept) & 45.68 & -2.66 \\ 
  speed & -2.66 & 0.17 \\ 
   \hline
\end{tabular}
\end{table}



\begin{enumerate}
\item Определите количество наблюдений
\item Найдите среднюю скорость автомобиля в милях в час
\end{enumerate}
\end{problem}

\begin{solution}
Находим $X'X$, её элементы и есть то, что нужно.
\end{solution}



\begin{problem}
Дана оценка ковариацонной матрицы, 
\[
\widehat{Var}(\hat{\beta})=\begin{pmatrix}
1/3 & -1/3 & 0 \\
-1/3 & 4/3 & -1 \\
0 & -1 & 2
\end{pmatrix}
\]

Найдите $\widehat{Var}(\hb_1+\hb_2-\hb_3)$
\end{problem}

\begin{solution}
\[\widehat{Var}(\hb_1+\hb_2-\hb_3)=\Var(\hb1)+\Var(\hb2)+\Var(\hb3)+
2\Cov(\hb1,\hb2)-2\Cov(\hb1,\hb3)-2\Cov(\hb2,\hb3)\]

\[\widehat{Var}(\hb_1+\hb_2-\hb_3)=1/3+4/3+2-2/3+2=5\]
\end{solution}

\begin{problem}
Методом наименьших квадратов по 5 наблюдениям оценивается модель $y_i=\beta_1+\beta_2 x_{i2}+\beta_3 x_{i3}+\varepsilon_i$. Известно, что
\[
\hb=\begin{pmatrix}
1 \\
2 \\
3
\end{pmatrix}, \;
(X'X)^{-1}= \begin{pmatrix} 
1/3 & -1/3 & 0 \\ 
-1/3 & 4/3 & -1 \\ 
0 & -1 & 2.0000 
\end{pmatrix}
\]
\[
\widehat{Var}(\hb)=\begin{pmatrix}
1 & -1 & 0 \\
-1 & 4 & -3 \\ 
0 & -3 & 6
\end{pmatrix}, \; TSS=60
\]

Найдите $\hs^2$ и проверьте гипотезу о значимости регрессии в целом на уровне значимости 5\%.
\end{problem}

\begin{solution}
Из того, что $\widehat{Var}(\hb)=\sigma^2\cdot (X'X)^{-1}$ видно, что $\sigma^2=\frac13$.

$RSS=\sigma^2\cdot(n-k)=1/3\cdot 2=2/3$
\[R^2=49\frac13/50=148/150\]


$F=\frac{R^2/(k-1)}{(1-R^2)/(n-k)}=\frac{148/2}{2/2}=74$
$F^{crit}_{0.05, 2,2}=19<74$
гипотеза отвергается, регрессия значима.

\end{solution}



\begin{problem}
По 30 наблюдениям оценивается модель парной регрессии $y_t=\beta_1 + \beta_2 x_t + \e_t$. Известны матрицы:
\[
X'X=\begin{pmatrix}
30 & 10 \\
10 & 40
\end{pmatrix}, \;
X'y=\begin{pmatrix}
40 \\
70 
\end{pmatrix}, \;
y'y=80
\]

\begin{enumerate}
\item Оцените модель парной регрессии по 30 наблюдениям
\item К имеющимся 30 наблюдениям добавили ещё одно, $x_{31}=1$, $y_{31}=2$. Оцените модель по 31 наблюдению
\item Проведите тест Чоу на прогнозную силу. То есть проверьте, что зависимость на выборке из  31 наблюдения совпадает с зависимостью по 30 наблюдениям
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}




% ML


\chapter{Метод максимального правдоподобия} % --- общая теория}

\begin{problemtext}
Пусть

$X = (X_1,\ldots,X_n)$ --- случайная выборка

$x = (x_1,\ldots,x_n)$ --- реализация данной случайной выборки

$f_{X_i}(x_i, \theta)$ --- плотность распределения случайной величины $X_i$, $i = 1,\ldots, n$

$\theta = (\theta_1, \ldots, \theta_k)$ --- вектор неизвестных параметров

$\Theta \subseteq \mathbb{R}^k$ --- множество допустимых значений вектора неизвестных параметров

$\text{L}(\theta) = \prod_{i=1}^n f_{X_i}(x_i, \theta)$ --- функция правдоподобия

$l(\theta) := \ln \text{L}(\theta)$ --- логарифмическая функция правдоподобия

Пусть требуется протестировать систему (нелинейных) ограничений относительно вектора неизвестных параметров

$$H_0: \begin{cases}
g_1(\theta) = 0 \\
g_2(\theta) = 0 \\
\ldots \\
g_r(\theta) = 0 \\
\end{cases}$$
где $g_i(\theta)$ --- функция, которая задаёт $i$-ое ограничение на вектор параметров $\theta$, $i = 1,\ldots, r$.

$\frac{\partial g}{\partial \theta'} = \begin{bmatrix}
\partial g_1/\partial \theta' \\
\partial g_2/\partial \theta' \\
\vdots \\
\partial g_r/\partial \theta' \\
\end{bmatrix} = \begin{bmatrix}
\frac{\partial g_1}{\partial \theta_1} & \frac{\partial g_1}{\partial \theta_2} & \ldots & \frac{\partial g_1}{\partial \theta_k}\\
\frac{\partial g_2}{\partial \theta_1} & \frac{\partial g_2}{\partial \theta_2} & \ldots & \frac{\partial g_2}{\partial \theta_k}\\
\ldots & \ldots & \ldots & \ldots \\
\frac{\partial g_r}{\partial \theta_1} & \frac{\partial g_r}{\partial \theta_2} & \ldots & \frac{\partial g_r}{\partial \theta_k}\\
\end{bmatrix}$

$\frac{\partial g'}{\partial \theta} = \begin{bmatrix}
\frac{\partial g'_1}{\partial \theta} & \frac{\partial g'_2}{\partial \theta} & \ldots & \frac{\partial g'_r}{\partial \theta} \\
\end{bmatrix} = \begin{bmatrix}
\frac{\partial g_1}{\partial \theta_1} & \frac{\partial g_2}{\partial \theta_1} & \ldots & \frac{\partial g_r}{\partial \theta_1}\\
\frac{\partial g_2}{\partial \theta_2} & \frac{\partial g_2}{\partial \theta_2} & \ldots & \frac{\partial g_r}{\partial \theta_2}\\
\ldots & \ldots & \ldots & \ldots \\
\frac{\partial g_1}{\partial \theta_k} & \frac{\partial g_2}{\partial \theta_k} & \ldots & \frac{\partial g_r}{\partial \theta_k}\\
\end{bmatrix}$

$I(\theta) = -\E \left(\frac{\partial^2 l}{\partial \theta \partial \theta'}\right) = - \E \begin{bmatrix}
\frac{\partial^2 l}{\partial \theta_1 \partial \theta_1} & \frac{\partial^2 l}{\partial \theta_1 \partial \theta_2} & \ldots & \frac{\partial^2 l}{\partial \theta_1 \partial \theta_k} \\
\frac{\partial^2 l}{\partial \theta_2 \partial \theta_1} & \frac{\partial^2 l}{\partial \theta_2 \partial \theta_2} & \ldots & \frac{\partial^2 l}{\partial \theta_2 \partial \theta_k} \\
\ldots & \ldots & \ldots & \ldots \\
\frac{\partial^2 l}{\partial \theta_k \partial \theta_1} & \frac{\partial^2 l}{\partial \theta_k \partial \theta_2} & \ldots & \frac{\partial^2 l}{\partial \theta_k \partial \theta_k} \\
\end{bmatrix}$ --- информационная матрица Фишера

$\frac{\partial l}{\partial \theta} = \begin{bmatrix}
\frac{\partial l}{\partial \theta_1} \\
\frac{\partial l}{\partial \theta_2} \\
\vdots \\
\frac{\partial l}{\partial \theta_k} \\
\end{bmatrix}$

$\Theta_{UR} := \Theta$ --- множество допустимых значений вектора неизвестных параметров без учёта ограничений

$\Theta_{R} := \{ \theta \in \Theta: g(\theta) = 0\}$ --- множество допустимых значений вектора неизвестных параметров с учётом ограничений

$\hat{\theta}_{UR} \in \Theta_{UR}$ --- точка максимума функции $l$ на множестве $\Theta_{UR}$

$\hat{\theta}_{R} \in \Theta_{R}$ --- точка максимума функции $l$ на множестве $\Theta_{R}$

Тогда для тестирования гипотезы$H_0$ можно воспользоваться одной из следующих ниже статистик.

$LR := -2(l(\hat{\theta}_{R}) - l) \overset{a}{\sim} \chi^2_r$ --- статистика отношения правдоподобия

$W := g'(\hat{\theta}_{UR}) \cdot \left[ \frac{\partial g}{\partial \theta'}(\hat{\theta}_{UR}) \cdot I^{-1}(\hat{\theta}_{UR}) \cdot \frac{\partial g'}{\partial \theta}(\hat{\theta}_{UR}) \right]^{-1} g(\hat{\theta}_{UR}) \overset{a}{\sim} \chi^2_r$ --- статистика Вальда

$LM := \left[ \frac{\partial l}{\partial \theta}(\hat{\theta}_{R}) \right]' \cdot I^{-1}(\hat{\theta}_{R}) \cdot \left[ \frac{\partial l}{\partial \theta}(\hat{\theta}_{R}) \right] \overset{a}{\sim} \chi^2_r$ --- статистика множителей Лагранжа
\end{problemtext}


\begin{problem}
Дядя Вова (Владимир Николаевич) и Скрипач (Гедеван) зарабатывают на Плюке чатлы, чтобы купить гравицапу. Число заработанных за $i$-ый день чатлов имеет пуассоновское распределение, заработки за разные дни независимы. За прошедшие 100 дней они заработали 250 чатлов. 
\begin{enumerate}
\item Оцените параметр $\lambda$ пуассоновского распределения методом максимального правдоподобия
\item Сколько дней им нужно давать концерты, чтобы оценка вероятности купить гравицапу составила 0.99? Гравицапа стоит пол кц или 2200 чатлов.
\item Постройте 95\% доверительный интервал для $\lambda$
\item Проверьте гипотезу о том, что средний дневной заработок равен 2 чатла с помощью теста отношения правдоподобия, теста Вальда, теста множителей Лагранжа
\end{enumerate}
\end{problem}
\begin{solution}
\end{solution}




\begin{problem}
Инопланетянин Капп совершил вынужденную посадку на Землю. Каждый день он выходит на связь со своей далёкой планетой. Продолжительность каждого сеанса связи имеет экспоненциальное распределение с параметром $\lambda$. Прошедшие 100 сеансов связи в сумме длились 11 часов.
\begin{enumerate}
\item Оцените параметр $\lambda$ экспоненциального распределения методом максимального правдоподобия
\item Постройте 95\% доверительный интервал для $\lambda$
\item Проверьте гипотезу о том, что средняя продолжительность сеанса связи равна 5 минутам с помощью теста отношения правдоподобия, теста Вальда, теста множителей Лагранжа
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}




\begin{problem}
\useR По ссылке \url{http://people.reed.edu/~jones/141/Coal.html} скачайте данные о количестве крупных аварий на английских угольных шахтах. 
\begin{enumerate}
\item Методом максимального правдоподобия оцените две модели: 
\begin{enumerate}
\item Пуассоновская модель: количества аварий независимы и имеют Пуассоновское распределение с параметром $\lambda$.
\item Модель с раздутым нулём  <<zero inflated poisson model>>: количества аварий независимы, с вероятностью $p$ аварий не происходит вообще, с вероятностью $(1-p)$ количество аварий имеет Пуассоновское распределение с параметром $\lambda$. Смысл этой модели в том, что по сравнению с Пуассоновским распределением у события $\{X_i=0\}$ вероятность выше, а пропорции вероятностей положительных количеств аварий сохраняются. В модели с раздутым нулём дисперсия и среднее количества аварий отличаются. Чему в модели с раздутым нулём равна $\P(X_i=0)$?
\end{enumerate}
\item С помощью тестов множителей Лагранжа, Вальда и отношения правдоподобия проверьте гипотезу $H_0$: верна пуассоновская модель против $H_{a}$: верна модель с раздутым нулём
\item Постройте доверительные интервалы для оценённых параметров в обоих моделях
\item Постройте доверительный интервал для вероятности полного отсутствия аварий по обеим моделям
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Совместное распределение величин $X$ и $Y$ задано функцией
\[
f(x,y)=\frac{\theta(\beta y)^x e^{-(\theta+\beta)y}}{x!}
\]
Величина $X$ принимает целые неотрицательные значения, а величина $Y$ --- действительные неотрицательные. Имеется случайная выборка $(X_1,Y_1)$, \ldots $(X_n,Y_n)$.
\begin{enumerate}
\item С помощью метода максимального правдоподобия оцените $\theta$ и $\beta$
\item С помощью метода максимального правдоподобия оцените $a=\theta/(\beta+\theta)$
\end{enumerate} 
\end{problem}

\begin{solution}
$\hat{\theta}=1/\bar{Y}$, $\hat{\beta}=\bar{X}/\bar{Y}$, $\hat{a}=1/(1+\bar{X})$
\end{solution}



\begin{problem}
Пусть $X = (X_1,\ldots,X_n)$ --- случайная выборка из нормального распределения с математическим ожиданием $\mu$ и дисперсией $\nu$; $\mu \in \mathbb{R}$ и $\nu >0$ --- неизвестные параметры. Реализация случайной выборки $x = (x_1,\ldots,x_n)$ известна: $-2.80$, $-1.12$, $-2.27$, $-1.31$, $-0.98$, $-2.15$, $-1.52$, $-2.82$, $-1.19$, $0.87$.


При помощи теста отношения правдоподобия, теста Вальда и теста множителей Лагранжа протестируйте гипотезу:
$$H_0: \begin{cases}
\mu = 0 \\
\nu = 1 \\
\end{cases}$$
на уровне значимости 5\%.
\end{problem}

\begin{solution}
В данном примере мы имеем

$\theta = \begin{bmatrix}
\mu & \nu
\end{bmatrix}'$ --- вектор неизвестных параметров

$\Theta = \mathbb{R} \times (0; +\infty)$ --- множество допустимых значений вектора неизвестных параметров

Функция правдоподобия имеет вид:
\begin{multline}
\text{L}(\theta) = \prod_{i=1}^n f_{X_i}(x_i, \theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\nu}} \cdot \exp\left\lbrace -\frac{(x_i - \mu)^2}{2\nu} \right\rbrace =\\
 (2\pi)^{-n/2} \cdot \nu^{-n/2} \cdot \exp \left\lbrace -\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\nu} \right\rbrace
\end{multline}

Логарифмическая функция правдоподобия:
$$l(\theta) := \ln \text{L}(\theta) = -\frac{n}{2} \ln(2\pi) - \frac{n}{2} \ln\nu - \frac{\sum_{i=1}^n (x_i - \mu)^2}{2\nu}$$

$\Theta_{UR} = \Theta$

$\Theta_{R} = \{(0,1)\}$

Из системы уравнений
$$\begin{cases}
\frac{\partial l}{\partial \mu} = \frac{\sum_{i=1}^n (x_i - \mu)}{\nu} = 0 \\
\frac{\partial l}{\partial \nu} = -\frac{n}{2\nu} + \frac{\sum_{i=1}^n (x_i - \mu)^2}{2\nu^2} = 0 \\
\end{cases}$$

находим

$\hat{\theta}_{UR} = (\hat{\mu}_{UR}, \hat{\nu}_{UR})$, где $\hat{\mu}_{UR} = \overline{x} = -1.5290$, $\hat{\nu}_{UR} = \frac{1}{n} \sum_{i=1}^n (x_i - \overline{x})^2 = 1.0603$

$\hat{\theta}_{R} = (\hat{\mu}_{R}, \hat{\nu}_{R}) = (0,1)$

По имеющимся данным находим

$l(\hat{\theta}_{R}) = -\frac{10}{2} \ln(2\pi) - \frac{10}{2} \ln 1 - \frac{\sum_{i=1}^n (x_i - 0)^2}{2 \cdot 1} = -26.1804$

$l = -\frac{10}{2} \ln(2\pi) -  \frac{10}{2} \ln (1.0603) - \frac{\sum_{i=1}^n (x_i + 1.5290)^2}{2 \cdot 1.0603} = -14.4824$

$LR_{\text{набл}} = -2(l(\hat{\theta}_{R}) - l) = -2 \cdot (-26.1804 + 14.4824) = 23.3959$

Критическое значение $\chi^2$ распределения с двумя степенями свободы, отвечающее уровню значимости 5\%, равно 5.9915. Следовательно, тест отношения правдоподобия говорит о том, что гипотеза $H_0$ должна быть отвергнута.

Для выполнения тестов Вальда и множителей Лагранжа нам понадобится информационная матрица Фишера

$\frac{\partial^2 l}{\partial \mu^2} = -\frac{n}{v}$, $\frac{\partial^2 l}{\partial \nu \partial \mu} = -\frac{\sum_{i=1}^n (x_i - \mu)}{\nu^2}$, $\frac{\partial^2 l}{\partial \nu^2} = \frac{n}{2\nu^2} - \frac{\sum_{i=1}^n (x_i - \mu)^2}{\nu^3}$

$\E \frac{\partial^2 l}{\partial \nu \partial \mu} = -\frac{\sum_{i=1}^n \E(x_i - \mu)}{\nu^2} = 0$, $\E \frac{\partial^2 l}{\partial \nu^2} = \frac{n}{2\nu^2} - \frac{\sum_{i=1}^n \E(x_i - \mu)^2}{\nu^3} = \frac{n}{2\nu^2} - \frac{n\nu}{nu^3} =- \frac{n}{2\nu^2}$

$I(\theta) = -\E \begin{bmatrix}
\frac{\partial^2 l}{\partial \mu^2} & \frac{\partial^2 l}{\partial \nu \partial \mu} \\
\frac{\partial^2 l}{\partial \nu \partial \mu} & \frac{\partial^2 l}{\partial \nu^2} \\
\end{bmatrix} = \begin{bmatrix}
\frac{n}{\nu} & 0 \\
0 & \frac{n}{2\nu^2}
\end{bmatrix}$

$I(\hat{\theta}_{UR}) = \begin{bmatrix}
\frac{n}{\hat{\nu}_{UR}} & 0 \\
0 & \frac{n}{2 \cdot \hat{\nu}_{UR}^2}
\end{bmatrix} = \begin{bmatrix}
\frac{10}{1.0603} & 0 \\
0 & \frac{10}{2 \cdot 1.0603^2}
\end{bmatrix} = \begin{bmatrix}
9.4307 & 0 \\
0 & 4.4469 \\
\end{bmatrix}$

$g(\hat{\theta}_{UR}) = \begin{bmatrix}
\hat{\mu}_{UR} - 0 \\
\hat{\nu}_{UR} - 1 \\
\end{bmatrix} = \begin{bmatrix}
-1.5290 - 0\\
1.0603 - 1 \\
\end{bmatrix} = \begin{bmatrix}
-1.5290\\
0.0603 \\
\end{bmatrix}$

$\frac{\partial g}{\partial \theta'} = \begin{bmatrix}
\frac{\partial g_1}{\partial \mu} & \frac{\partial g_1}{\partial \nu} \\
\frac{\partial g_2}{\partial \mu} & \frac{\partial g_2}{\partial \nu} \\
\end{bmatrix} = \begin{bmatrix}
1 & 0\\
0 & 1\\
\end{bmatrix}$, $\frac{\partial g'}{\partial \theta} = \begin{bmatrix}
\frac{\partial g_1}{\partial \mu} & \frac{\partial g_2}{\partial \mu} \\
\frac{\partial g_1}{\partial \nu} & \frac{\partial g_2}{\partial \nu} \\
\end{bmatrix} = \begin{bmatrix}
1 & 0\\
0 & 1\\
\end{bmatrix}$

$W_{\text{набл}} = g'(\hat{\theta}_{UR}) \cdot \left[ \frac{\partial g}{\partial \theta'}(\hat{\theta}_{UR}) \cdot I^{-1}(\hat{\theta}_{UR}) \cdot \frac{\partial g'}{\partial \theta}(\hat{\theta}_{UR}) \right]^{-1} g(\hat{\theta}_{UR}) = \\
\begin{bmatrix}
-1.5290 & 0.0603\\
\end{bmatrix} \cdot \left[ \begin{bmatrix}
1 & 0\\
0 & 1\\
\end{bmatrix} \cdot \begin{bmatrix}
9.4307 & 0 \\
0 & 4.4469 \\
\end{bmatrix}^{-1} \cdot \begin{bmatrix}
1 & 0\\
0 & 1\\
\end{bmatrix} \right]^{-1} \cdot  \begin{bmatrix}
-1.5290\\
0.0603 \\
\end{bmatrix} = 22.0635$

Тест Вальда также говорит о том, что на основании имеющихся наблюдений гипотеза $H_0$ должна быть отвергнута.

$I(\hat{\theta}_{R}) = \begin{bmatrix}
\frac{n}{\hat{\nu}_{R}} & 0 \\
0 & \frac{n}{2 \cdot \hat{\nu}_{R}^2}
\end{bmatrix} = \begin{bmatrix}
\frac{10}{1} & 0 \\
0 & \frac{10}{2\cdot 1^2} \\
\end{bmatrix} = \begin{bmatrix}
10 & 0\\
0 & 5 \\
\end{bmatrix}$

$\frac{\partial l}{\partial \theta}(\hat{\theta}_{R}) = \begin{bmatrix}
\frac{\sum_{i=1}^n (x_i - \hat{\mu}_R)}{\hat{\nu}_R}\\
-\frac{n}{2\cdot \hat{\nu}_R} + \frac{\sum_{i=1}^n (x_i - \hat{\mu}_R)^2}{2 \cdot \hat{\nu}_R^2}
\end{bmatrix} = \begin{bmatrix}
\frac{\sum_{i=1}^n (x_i - 0)}{1}\\
-\frac{10}{2\cdot 1} + \frac{\sum_{i=1}^n (x_i - 0)^2}{2 \cdot 1^2}
\end{bmatrix} = \begin{bmatrix}
-15.29 \\
11.9910\\
\end{bmatrix}$

$LM_{\text{набл}} = \left[ \frac{\partial l}{\partial \theta}(\hat{\theta}_{R}) \right]' \cdot I^{-1}(\hat{\theta}_{R}) \cdot \left[ \frac{\partial l}{\partial \theta}(\hat{\theta}_{R}) \right] = \begin{bmatrix}
-15.29 & 11.9910 \\
\end{bmatrix} = \cdot \begin{bmatrix}
10 & 0\\
0 & 5 \\
\end{bmatrix}^{-1} \cdot \begin{bmatrix}
-15.29 \\
11.9910\\
\end{bmatrix} = 52.1354$

Тест множителей Лагранжа также указывает на то, что гипотеза $H_0$ должна быть отвергнута.
\end{solution}


\begin{problem}
Пусть $p$ --- неизвестная вероятность выпадения орла при бросании монеты. Из 100 испытаний  42 раза выпал <<Орел>> и 58 --- <<Решка>>. Протестируйте на 5\%-ом уровне значимости гипотезу о том, что монетка --- <<правильная>> с помощью: 
\begin{enumerate}
\item теста отношения правдоподобия
\item теста Вальда
\item теста множителей Лагранжа
\end{enumerate}
\end{problem}

\begin{solution}
В данной задаче мы имеем:

$\theta = p$ --- вектор неизвестных параметров

$\Theta = (0, 1)$ --- множество допустимых значений вектора неизвестных параметров

Функция правдоподобия имеет вид:
$$\text{L}(\theta) = \prod_{i=1}^n \mathbb{P}_{\theta}(X_i = x_i) = \prod_{i=1}^n p^{x_i} \cdot (1-p)^{1-x_i} = p^{\sum_{i=1}^n x_i} \cdot (1-p)^{n - \sum_{i=1}^n x_i}$$

Логарифмическая функция правдоподобия:

$$l(\theta) := \ln \text{L}(\theta) = \left( \sum_{i=1}^n x_i \right) \cdot \ln p + \left(n - \sum_{i=1}^n x_i \right) \cdot \ln (1 - p)$$

$\Theta_{UR} = \Theta$

$\Theta_{R} = \{0.5\}$

Решая уравнение правдоподобия

$$\frac{\partial l}{\partial p} = \frac{\sum_{i=1}^n x_i}{p} - \frac{n - \sum_{i=1}^n x_i}{1 - p} = 0$$

получаем

$\hat{\theta}_{UR} = \hat{p}_{UR}$, где $\hat{p}_{UR} = \overline{x} = 0.42$

$\hat{\theta}_{R} = \hat{p}_{R} = 0.5$

По имеющимся данным находим

$l(\hat{\theta}_{R}) = 42 \cdot \ln(0.5) + (100-42) \cdot \ln(1-0.5) = -69.3147$

$l(\hat{\theta}_{UR} = 42 \cdot \ln(0.42) + (100-42) \cdot \ln(1-0.42) = -68.0292$

$LR_{\text{набл}} = -2(l(\hat{\theta}_{R}) - l) = -2 \cdot (-69.3147 + 68.0292) = 2.5710$

Критическое значение $\chi^2$ распределения с одной степенью свободы, отвечающее за 5\% уровень значимости, равно 3.8414. Следовательно, тест отношения правдоподобия говорит о том, что на основании имеющихся данных, основная гипотеза $H_0: p = 0.5$ не может быть отвергнута.

Для выполнения тестов Вальда и множителей Лагранжа нам понадобится информационная матрица Фишера

$\frac{\partial^2 l}{\partial p^2} = - \frac{\sum_{i=1}^n x_i}{p^2} - \frac{n - \sum_{i=1}^n x_i}{(1 - p)^2}$

$I(\theta) = -\E\left[ \frac{\partial^2 l}{\partial p^2} \right] = -\E \left[ - \frac{\sum_{i=1}^n x_i}{p^2} - \frac{n - \sum_{i=1}^n x_i}{(1 - p)^2} \right] = -\left(-\frac{np}{p^2} - \frac{n - np}{(1-p)^2}\right) = \frac{n}{p(1-p)}$

$I(\hat{\theta}_{UR}) = \frac{n}{\hat{p}_{UR}(1-\hat{p}_{UR})} = \frac{100}{0.42 \times (1 - 0.42)} = 172.4138$

$g(\hat{\theta}_{UR}) = \hat{\theta}_{UR} - 0.5 = 0.42 - 0.5 = -0.08$

$\frac{\partial g}{\partial \theta'} = 1'$, $\frac{\partial g'}{\partial \theta} = 1$

$W_{\text{набл}} = g'(\hat{\theta}_{UR}) \cdot \left[ \frac{\partial g}{\partial \theta'}(\hat{\theta}_{UR}) \cdot I^{-1}(\hat{\theta}_{UR}) \cdot \frac{\partial g'}{\partial \theta}(\hat{\theta}_{UR}) \right]^{-1} g(\hat{\theta}_{UR}) = [-0.08]' \cdot [1' \cdot 172.4138^{-1} \cdot 1]^{-1} \cdot [-0.08] = 2.6272$

Тест Вальда также говорит о том, что гипотеза $H_0$ не отвергается.

$I(\hat{\theta}_{R}) = \frac{n}{\hat{p}_{R}(1-\hat{p}_{R})} = \frac{100}{0.5 \times (1 - 0.5)} = 400$

$\frac{\partial l}{\partial \theta}(\hat{\theta}_R) = \frac{\sum_{i=1}^n x_i}{\hat{p}_R} - \frac{n - \sum_{i=1}^n x_i}{1 - \hat{p}_R} = \frac{42}{0.5} - \frac{100 - 42}{1 - 0.5} = -32$

$LM_{\text{набл}} = \left[ \frac{\partial l}{\partial \theta}(\hat{\theta}_{R}) \right]' \cdot I^{-1}(\hat{\theta}_{R}) \cdot \left[ \frac{\partial l}{\partial \theta}(\hat{\theta}_{R}) \right] = [-32]' \cdot [400]^{-1} \cdot [-32] = 2.56$

Согласно тесту множителей Лагранжа, основная гипотеза $H_0$ не может быть отвергнута.
\end{solution}


\begin{problem}
Пусть $x = (x_1, \ldots, x_n)$ --- реализация случайной выборки из распределения Пуассона с неизвестным параметром $\lambda > 0$. Известно, что выборочное среднее $\overline{x}$ по 80 наблюдениям равно 1.7. Протестируйте на 5\%-ом уровне значимости гипотезу $H_0: \lambda = 2$ с помощью
\begin{enumerate}
\item теста отношения правдоподобия
\item теста Вальда
\item теста множителей Лагранжа
\end{enumerate}
\end{problem}


\begin{solution}
В данной задаче мы имеем

$\theta = \lambda$ --- вектор неизвестных параметров

$\Theta = (0, +\infty)$ --- множество допустимых значений вектора неизвестных параметров

Функция правдоподобия имеет вид:
$$\text{L}(\theta) = \prod_{i=1}^n \mathbb{P}_{\theta}(X_i = x_i) = \prod_{i=1}^n \frac{\lambda^{x_i}}{x_i !} e^{-\lambda} = \frac{\lambda^{\sum_{i=1}^n x_i}}{x_1!\ldots x_n!}e^{-\lambda n}$$

Логарифмическая функция правдоподобия:

$$l(\theta) := \ln \text{L}(\theta) = \left( \sum_{i=1}^n x_i \right) \cdot \ln\lambda - \sum_{i=1}^n \ln (x_i!) - \lambda n$$

$\Theta_{UR} = \Theta$

$\Theta_{R} = \{2\}$

Решая уравнение правдоподобия

$$\frac{\partial l}{\partial p} = \frac{\sum_{i=1}^n x_i}{\lambda} - n = 0$$

получаем

$\hat{\theta}_{UR} = \hat{\lambda}_{UR}$, где $\hat{\lambda}_{UR} = \overline{x} = 1.7$

$\hat{\theta}_{R} = \hat{p}_{R} = 2$

По имеющимся данным находим

$l(\hat{\theta}_{R}) = (80 \cdot 1.7) \cdot \ln(2) - \sum_{i=1}^n \ln (x_i!) - 2 \cdot 80 = -65.7319$

$l(\hat{\theta}_{UR} = (80 \cdot 1.7) \cdot \ln(1.7) - \sum_{i=1}^n \ln (x_i!) - 1.7 \cdot 80 = -63.8345$

$LR_{\text{набл}} = -2(l(\hat{\theta}_{R}) - l) = -2 \cdot (-65.7319 + 63.8345) = 3.7948$

Критическое значение $\chi^2$ распределения с одной степенью свободы, отвечающее за 5\% уровень значимости, равно 3.8414. Следовательно, тест отношения правдоподобия говорит о том, что на основании имеющихся данных, основная гипотеза $H_0: \lambda = 2$ не может быть отвергнута.

Для выполнения тестов Вальда и множителей Лагранжа нам понадобится информационная матрица Фишера

$\frac{\partial^2 l}{\partial p^2} = - \frac{\sum_{i=1}^n x_i}{\lambda^2}$

$I(\theta) = -\E\left[ \frac{\partial^2 l}{\partial p^2} \right] = -\E \left[ - \frac{\sum_{i=1}^n x_i}{\lambda^2} \right] = -\left(-\frac{n\lambda}{\lambda^2}\right) = \frac{n}{\lambda}$

$I(\hat{\theta}_{UR}) = \frac{n}{\hat{\lambda}_{UR}} = \frac{80}{1.7} = 47.0588$

$g(\hat{\theta}_{UR}) = \hat{\theta}_{UR} - 2 = 1.7 - 2 = -0.3$

$\frac{\partial g}{\partial \theta'} = 1'$, $\frac{\partial g'}{\partial \theta} = 1$

$W_{\text{набл}} = g'(\hat{\theta}_{UR}) \cdot \left[ \frac{\partial g}{\partial \theta'}(\hat{\theta}_{UR}) \cdot I^{-1}(\hat{\theta}_{UR}) \cdot \frac{\partial g'}{\partial \theta}(\hat{\theta}_{UR}) \right]^{-1} g(\hat{\theta}_{UR}) = [-0.3]' \cdot [1' \cdot 47.0588^{-1} \cdot 1]^{-1} \cdot [-0.3] = 4.2352$

Поскольку наблюдаемое значение статистики Вальда превосходит критическое значение 3.8414, то гипотеза $H_0$ должна быть отвергнута.

$I(\hat{\theta}_{R}) = \frac{n}{\hat{\lambda}_{R}} = \frac{80}{2} = 40$

$\frac{\partial l}{\partial \theta}(\hat{\theta}_R) = \frac{\sum_{i=1}^n x_i}{\hat{\lambda}_R} - n = \frac{80 \cdot 1.7}{2} - 80 = -12$

$LM_{\text{набл}} = \left[ \frac{\partial l}{\partial \theta}(\hat{\theta}_{R}) \right]' \cdot I^{-1}(\hat{\theta}_{R}) \cdot \left[ \frac{\partial l}{\partial \theta}(\hat{\theta}_{R}) \right] = [-12]' \cdot [40]^{-1} \cdot [-12] = 3.6$

Согласно тесту множителей Лагранжа, основная гипотеза $H_0$ не может быть отвергнута.
\end{solution}



\begin{problem}
Выпишите в явном виде функцию максимального правдоподобия для модели $y=\b_1+\b_2 x+\e$, если $\e\sim N(0,A)$. 
Матрица $A$ устроена по принципу: $\Cov(\e_i,\e_j)=0$ при $i\neq j$, и $\Var(\e_i)=\sigma^2 x_i^2$.
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Выпишите в явном виде функцию максимального правдоподобия для модели $y=\b_1+\b_2 x+\e$, если $\e\sim N(0,A)$. 
Матрица $A$ устроена по принципу: $\Cov(\e_i,\e_j)=0$ при $i\neq j$, и $\Var(\e_i)=\sigma^2 |x_i|$.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Предположим, что в классической линейной модели ошибки имеют нормальное распределение, т.е.
\[
y_i=\beta_1+\beta_2 x_{2,i}+\ldots+\beta_k x_{k,i}+\e_i
\]
где $\e_i$ нормальны $N(0,\sigma^2)$ и независимы
\begin{enumerate}
\item Найдите оценки для $\beta$ и $\sigma^2$ методом максимального правдоподобия. 
\item Являются ли полученные оценки $\hb_{ML}$ и $\hs^2_{ML}$ несмещенными?
\item Выведите формулу $LR$-статистики у теста отношения правдоподобия для тестирования гипотезы об адекватности регрессии $H_0$: $\beta_2=\beta_3=\ldots=\beta_k=0$.
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Наблюдения $X_1$, \ldots, $X_n$ независимы и нормальны $N(\mu,1)$. По 100 наблюдениям оказалось, что $\sum x_i=200$, $\sum x_i^2=900$.
\begin{enumerate}
\item Оцените $\mu$ методом максимального правдоподобия
\item Постройте 95\% доверительный интервал для $\mu$
\item Проверьте гипотезу о том, что $\mu=3$ против альтернативной $\mu\neq 3$ с помощью тестов Вальда, множителей Лагранжа и отношения правдоподобия
\item Постройте 95\% доверительный интервал для неизвестной величины $\P(X_i>2.5)$
\end{enumerate}
Наблюдения $X_1$, \ldots, $X_n$ независимы и нормальны $N(\mu,1)$. По 100 наблюдениям оказалось, что $\sum x_i=200$, $\sum x_i^2=900$.
\begin{enumerate}
\item Оцените $\mu$ методом максимального правдоподобия
\item Постройте 95\% доверительный интервал для $\mu$
\item Проверьте гипотезу о том, что $\mu=3$ против альтернативной $\mu\neq 3$ с помощью тестов Вальда, множителей Лагранжа и отношения правдоподобия
\item Постройте 95\% доверительный интервал для неизвестной величины $\P(X_i>2.5)$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Наблюдения $X_1$, \ldots, $X_n$ независимы и нормальны $N(0,\sigma^2)$. По 100 наблюдениям оказалось, что $\sum x_i=200$, $\sum x_i^2=900$.
\begin{enumerate}
\item Оцените $\sigma^2$ методом максимального правдоподобия
\item Постройте 95\% доверительный интервал для $\sigma^2$
\item Проверьте гипотезу о том, что $\sigma^2=4$ против альтернативной $\sigma^2\neq 4$ с помощью тестов Вальда, множителей Лагранжа и отношения правдоподобия
\item Постройте 95\% доверительный интервал для неизвестной величины $\P(X_i>2.5)$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}

\begin{problem} 
Наблюдения $X_1$, \ldots, $X_n$ независимы и нормальны $N(\mu,\sigma^2)$. По 100 наблюдениям оказалось, что $\sum x_i=200$, $\sum x_i^2=900$.
\begin{enumerate}
\item Оцените $\mu$ и $\sigma^2$ методом максимального правдоподобия
\item Постройте 95\% доверительный интервал для $\mu$, $\sigma^2$
\item \useR Проверьте гипотезу о том, что $\sigma^2=4$ против альтернативной $\sigma^2\neq 4$ с помощью тестов Вальда, множителей Лагранжа и отношения правдоподобия
\item \useR Проверьте гипотезу о том, что $\mu=3$ против альтернативной $\mu\neq 3$ с помощью тестов Вальда, множителей Лагранжа и отношения правдоподобия
\item \useR Постройте 95\% доверительный интервал для неизвестной величины $\P(X_i>2.5)$
\item \useR На графике постройте двумерную 95\% доверительную область для вектора $(\mu,\sigma^2)$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
\useR Эконометрессе Зульфие нравятся жёлтые и красные эмэндэмсины. Она записала цвета сотни случайно выбранных эмэндэмсин. Из сотни оказалось $X_1$ жёлтых и $X_2$ красных. Настоящие вероятности обнаружить жёлтую и красную эмэндэмсину неизвестны и равны $p_1$ и $p_2$, $p_1+p_2<1$.

\begin{enumerate}
\item Оцените неизвестные параметры с помощью максимального правдоподобия в общем виде. Найдите точечное значение оценки, если $X_1=20$ и $X_2=30$.
\item Оцените ковариационную матрицу оценок правдоподобия двумя способами: простой подстановкой оценок в матрицу Гессе и подстановкой оценок в математическое ожидание матрицы Гессе. Совпадают ли эти два способа в данном случае?
\item Постройте 95\% доверительный интервал для каждого неизвестного параметра
\item С помощью теста отношения правдоподобия, теста множителей Лагранжа, теста Вальда проверьте гипотезу $H_0$: $p_1=0.25$ и $p_2=0.25$.
\item С помощью теста отношения правдоподобия, теста множителей Лагранжа, теста Вальда проверьте гипотезу $H_0$: $p_1=0.25$ 
\item С помощью теста отношения правдоподобия, теста множителей Лагранжа, теста Вальда проверьте гипотезу $H_0$: $p_1+p_2=0.5$. Постройте 95\%-ый доверительный интервал для суммы $p_1+p_2$
\end{enumerate}

\end{problem}

\begin{solution}
$\hat{p}_1=X_1/n$, $\hat{p}_2=X_2/n$.
\end{solution}


\begin{problem}
Случайные величины $X_{1}$, ..., $X_{n}$ --- независимы и одинаково распределены с функцией плотности $f(t)=\frac{\theta \cdot\left(\ln  t\right)^{\theta -1}}{t} $  при  $t\in
\left[1;e\right]$. По выборке из 100 наблюдений оказалось, что $\sum{\ln(\ln(X_{i}))}=-30$ 
\begin{enumerate}
\item Найдите ML оценку параметра $\theta$
\item Постройте 95\% доверительный интервал для $\theta$
\item С помощью LR, LM и W теста проверьте гипотезу о том, что $\theta=1$.
\end{enumerate}
\end{problem}


\begin{solution}
 Функция правдоподобия: \[\calL = \prod_{i=1}^{n} \frac{\theta (\ln X_i)^{\theta-1}}{X_i}\]

Логарифм функции правдоподобия: \[\ln \calL = \sum\limits_{i=1}^{n} \bigl[\ln \bigl(\theta (\ln X_i)^{\theta-1} \bigr) - \ln X_i \bigr] = \sum\limits_{i=1}^{n} [\ln \theta + (\theta-1) \ln\ln X_i - \ln X_i] \hm= n \ln \theta + (\theta-1) \sum\limits_{i=1}^{n} \ln \ln X_i - \sum\limits_{i=1}^{n} \ln X_i\]

\[\frac{\p \ln \calL}{\p \theta} = \frac{n}{\theta} + \sum\limits_{i=1}^{n} \ln \ln X_i \]

FOC: \[\frac{\p \ln \calL}{\p \theta} = 0 \hence \frac{n}{\theta} = -\sum\limits_{i=1}^{n} \ln \ln X_i \hence \hat\theta_{\text{ML}} = - \frac{n}{\sum\limits_{i=1}^{n} \ln \ln X_i} \]

Подставим имеющиеся данные: $- \frac{n}{\sum \ln \ln X_i} = -\frac{100}{-30} = \frac{10}{3}$.

\par\smallskip

\textbf{(b) (3 балла)} Так как оценки ММП асимптотически нормальны, то для нахождения доверительного интервала достаточно найти стандартное отклонение параметра  и~домножить на квантиль двухстороннего распределения: $\PP\left(\left\{|\hat\theta-\theta| \le z_{0{,}025} \sqrt{\var(\hat\theta)}\right\}\right) = 0{,}95$. Известно, что $\widehat{\var}(\hat \theta) = -\bs{H}^{-1} |_{\hat\theta}$. Матрица $\bs{H}$ "--- это матрица вторых производных логарифма функции правдоподобия.
\[
\bs{H} = \frac{\p^2 \ln \calL}{\p \theta^2} = -\frac{n}{\theta^2} \hence -\bs{H}^{-1} = \frac{\theta^2}{n} \hence \widehat{\var}(\hat\theta) = \frac{100/9}{100} = \frac19 \hence \hat\sigma_{\theta} = \frac13
\]
Следовательно, с вероятность 0,95 $\theta$ лежит в интервале $\frac{10}{3} \pm 1{,}96 \cdot \frac{1}{3} \approx \frac{10}{3} \pm \frac{2}{3}$, или [2{,}680; 3{,}987].

\par\smallskip

\textbf{(c) (3 $\times$ 3 = 9 баллов)} Тест Вальда  выглядит следующим образом:
\[
W = \bigl(\mathbf{c}(\bs\theta) - \bs{q}\bigr)' (\bs{C} \bs{I}^{-1}(\bs\theta) \bs{C}')^{-1} \bigl(\mathbf{c}(\bs\theta) - \bs{q} \bigr) \simhypo \chi^2_{r}
\]

За $\bs{C}$ обозначено $\frac{\p \mathbf{c}(\bs\theta)}{\p \bs\theta}$, за $\bs{I}$ "--- информационная матрица Фишера ($\bs{I}(\theta) = -\E\left( \frac{\p^2 \ln \calL}{\p \bs\theta^2} \right)$). В данном случае $\bs\theta = \theta$, и~нулевая гипотеза $\mathbf{c}(\theta) = \bs{q}$ выглядит как $\theta=1$ ($\mathbf{c}(\theta) = \theta$) "--- одномерный случай, одна степень свободы хи"=квадрата, $W \simhypo \chi^2_1$. $\mathbf{c}'(\theta) = 1$, поэтому расчётная статистика выглядит следующим образом:
\[
W = (\hat\theta - \theta_0) \frac{n}{\theta^2} (\hat\theta - \theta_0) = \left( \frac{10}{3} - 1 \right) \cdot \frac{100}{100/9} \cdot \left( \frac{10}{3} - 1 \right) = 49
\]

Тест отношения правдоподобия:
\[
LR = -2 \bigl( \ln \calL_{\text{R}} - \ln \calL_{\text{UR}} \bigr) \simhypo \chi^2_r
\]
\begin{multline*}
LR = -2 \left( \left[n \ln \theta_0 + (\theta_0-1) \sum\limits_{i=1}^{n} \ln \ln X_i \cancel{- \sum\limits_{i=1}^{n} \ln X_i}\right] - \left[n \ln \hat\theta + (\hat\theta-1) \sum\limits_{i=1}^{n} \ln \ln X_i \cancel{- \sum\limits_{i=1}^{n} \ln X_i}\right] \right) = \\
= -2 \left( \cancel{100 \ln 1} \cancel{+ (1-1) (-30)}  - 100 \ln \frac{10}{3} - \left(\frac{10}{3}-1 \right) (-30)  \right) = -2 \left( - 100 \ln \frac{10}{3} + \frac73 \cdot 30  \right) \approx 100{,}8
\end{multline*}

Тест множителей Лагранжа:
\[
LM = \bs{S}({\theta}_0)' \bs{I}^{-1}({\theta}_{0}) \bs{S}({\theta}_{0}) \simhypo \chi^2_r
\]

$\bs{S} = \left.\frac{\p \ln \calL}{\p \theta}\right|_{\theta_0}$. В точке $\theta_0$ значение частной производной логарифма функции правдоподобия равно $\frac{100}{1} -30=70$, $\bs{I}^{-1}(\theta_0) = \frac{\theta^2_0}{n} = \frac{1}{100}$, откуда
\[
LM = 70\cdot \frac{1}{100} \cdot 70 = 49
\]

Для уровня значимости 5\,\% критическое значение $\chi^2_1$ равно $\approx3{,}84$, поэтому во всех трёх тестах гипотеза $\hypo_0\colon \theta=1$ отвергается.

\end{solution}


\begin{problem}
 Величины $X_{1}$, ..., $X_{n}$ --- независимы и нормально распределены, $N(\mu,\sigma^2)$. По 100 наблюдениям $\sum X_i=100$ и  $\sum X_i^2=900$. 
\begin{enumerate}
\item Найдите ML оценки неизвестных параметров $\mu$ и $\sigma^2$.
\item Постройте 95\%-ые доверительные интервалы для $\mu$ и $\sigma^2$
\item С помощью LR, LM и W теста проверьте гипотезу о том, что $\sigma^2=1$.
\item С помощью LR, LM и W теста проверьте гипотезу о том, что $\sigma^2=1$ и одновременно $\mu=2$.
\end{enumerate}
\end{problem}

\begin{solution}
\[
\bs\theta =
\begin{pmatrix} \mu \\ \sigma^2 \end{pmatrix}, \quad
\calL = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(X_i-\mu)^2}{2\sigma^2} \right), \quad
\ln \calL = -\frac{n}{2}\ln 2\pi - \frac{n}{2} \ln \sigma^2 - \frac{1}{2\sigma^2} \sum\limits_{i=1}^n (X_i-\mu)^2
\]

\[
\text{FOC:} \quad
\frac{\p \ln \calL}{\p \mu} = \frac{1}{\sigma^2} \sum\limits_{i=1}^n (X_i-\mu), \quad \frac{\p \ln \calL}{\p (\sigma^2)} = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum\limits_{i=1}^n (X_i-\mu)^2
\]

\[
\frac{\p^2 \ln \calL}{\p \mu^2} = -\frac{n}{\sigma^2}, \quad \frac{\p^2 \ln \calL}{\p \mu \, \p (\sigma^2)} = -\frac{1}{(\sigma^2)^2} \sum\limits_{i=1}^n (X_i - \mu), \quad \frac{\p^2 \ln \calL}{\p (\sigma^2)^2} = \frac{n}{2(\sigma^2)^2} - \frac{1}{(\sigma^2)^3} \sum\limits_{i=1}^n (X_i - \mu)^2
\]

Т.\,к. даны $\sum X_i$ и $\sum X_i^2$, то можно вывести, что $\sum(X_i - \mu)^2 = \sum X_i^2 - \sum 2\mu X_i + \sum \mu^2 = \sum X_i^2 - 2\mu \sum X_i + n \mu^2$.

Из условий первого порядка следует, что ММП"=оценка матожидания $\hat\mu_{\text{ML}}$ "--- это выборочное среднее, а~дисперсии $\hat\sigma^2_{\text{ML}}$ "--- выборочная дисперсия (без коррекции на одну степень свободы):
\[
\hat \mu_{\text{ML}} = \frac1n \sum\limits_{i=1}^n X_i = \frac{100}{100} = 1, \quad \frac{1}{n}\sum\limits_{i=1}^n (X_i - \mu)^2  = \frac{1}{100}(900 - 2\cdot 1 \cdot 100 + 100\cdot 1^2) = \frac{800}{100} = 8
\]

\par\smallskip

\textbf{(b) (2 + 2 = 4 балла)}
\[
\bs{I}(\bs\theta) = -\E\left( \frac{\p^2 \ln \calL}{\p \bs\theta^2} \right), \quad \bs{I}(\hat{\bs\theta}) = \begin{pmatrix} \frac{n}{\hat\sigma^2} & 0 \\ 0 & \frac{n}{2(\hat\sigma^2)^2} \end{pmatrix} = \begin{pmatrix} \frac{100}{8} & 0 \\ 0 & \frac{100}{128}  \end{pmatrix}, \quad \bs{I}^{-1}(\hat{\bs\theta}) = \begin{pmatrix} \frac{2}{25} & 0 \\ 0 & \frac{32}{25} \end{pmatrix}
\]

Так как ММП"=оценки асимптотически нормальны, то 95\%-й доверительный интервал для вектора неизвестных параметров выглядит как
\[
\begin{pmatrix} \hat\mu \pm z_{\frac{\alpha}{2}} \sqrt{\widehat{\var}(\hat\mu)} \\ \hat\sigma^2 \pm z_{\frac{\alpha}{2}} \sqrt{\widehat{\var}(\hat\mu)} \end{pmatrix} \approx \begin{pmatrix} 1 \pm 1{,}96 \sqrt{\frac{2}{25}} \\ 8 \pm 1{,}96 \sqrt{\frac{32}{25}} \end{pmatrix} \approx \begin{pmatrix} [0{,}446; 1{,}554] \\ [5{,}783; 10{,}217] \end{pmatrix}
\]

\par\smallskip

\textbf{(c) (3 $\times$ 3 = 9 баллов)}

Тест Вальда:
\[
W = \bigl(c(\sigma^2) - \sigma^2_0 \bigr)' (\bs{C} \bs{I}^{-1}(\bs\theta) \bs{C}')^{-1} \bigl(c(\sigma^2) - \sigma^2_0 \bigr) \simhypo \chi^2_{r}
\]

$\frac{\p c}{\p \sigma^2}=1$, поэтому
\[
W = \bigl(8-1 \bigr)^2 \frac{n}{2(\sigma^2)^2} = 49\cdot \frac{100}{128} \approx 38{,}28
\]

Тест отношения правдоподобия:
\[
LR = -2 \bigl( \ln \calL_{\text{R}} - \ln \calL_{\text{UR}} \bigr) \simhypo \chi^2_r
\]
\begin{multline*}
LR =  -2 \bigl( \ln \calL(\sigma^2_0) - \ln \calL(\hat\sigma^2) \bigr)
 = -2 \left( -\frac{n}{2} \ln \sigma^2_0 - \frac{1}{2\sigma^2_0} \cdot 800 + \frac{n}{2} \ln \hat\sigma^2 + \frac{1}{2\hat\sigma^2} \cdot 800 \right) = \\
 -2 \left( \cancel{-50 \ln 1} - \frac{1}{2} \cdot 800 + 50 \ln 8 + \frac{1}{16} \cdot 800 \right) \approx 492
\end{multline*}

Тест множителей Лагранжа:
\[
LM = \bs{S}(\sigma^2_0)' \bs{I}^{-1}(\sigma^2_{0}) \bs{S}(\sigma^2_{0}) \simhypo \chi^2_r
\]
\[
\bs{I}(\sigma^2_0) = \frac{n}{2(\sigma^2_0)^2} = 50, \quad \bs{S}(\sigma^2_0) = \left. \frac{\p \ln \calL}{\p (\sigma^2)} \right|_{\sigma^2_0} = -\frac{100}{2} + \frac{1}{2} \cdot 800 = 350
\]
\[ LM = 350^2 \cdot \frac{1}{50} = 2450
\]

Для уровня значимости 5\,\% критическое значение $\chi^2_1$ равно $\approx3{,}84$, поэтому во всех трёх тестах гипотеза $\hypo_0\colon \sigma^2=1$ отвергается.

\par\smallskip

\textbf{(d) (3 $\times$ 3 = 9 баллов)}

Тест Вальда  выглядит следующим образом:
\[
W = \bigl(\mathbf{c}(\bs{\hat\theta}) - \bs{q}\bigr)' (\bs{C} \bs{I}^{-1}(\bs{\hat\theta}) \bs{C}')^{-1} \bigl(\mathbf{c}(\bs{\hat\theta}) - \bs{q} \bigr) \simhypo \chi^2_{r}
\]

За $\bs{C}$ обозначено $\frac{\p \mathbf{c}(\bs\theta)}{\p \bs\theta}$, за $\bs{I}$ "--- информационная матрица Фишера ($\bs{I}(\bs\theta) = -\E\left( \frac{\p^2 \ln \calL}{\p \bs\theta^2} \right)$). В данном случае нулевая гипотеза $\mathbf{c}(\bs\theta) = \bs{q}$ записывается как $\mathbf{c}({\bs{\theta}}) = \begin{pmatrix} \mu \\ \sigma^2 \end{pmatrix} = \begin{pmatrix} 2 \\ 1 \end{pmatrix}$, поэтому все статистики имеют две степени свободы хи"=квадрата. $\bs{C} = \frac{\p  \mathbf{c}}{\p \bs\theta} = \begin{pmatrix} \frac{\p  c_1}{\p \mu} & \frac{\p  c_2}{\p \mu} \\ \frac{\p  c_1}{\p \sigma^2} & \frac{\p  c_2}{\p \sigma^2} \end{pmatrix} =  \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$, $\mathbf{c}(\bs\theta) - \bs{q}  = \begin{pmatrix} 1 \\ 8 \end{pmatrix} - \begin{pmatrix} 2 \\ 1 \end{pmatrix} = \begin{pmatrix} -1 \\ 7 \end{pmatrix}$, поэтому расчётная статистика выглядит следующим образом:
\[
W = \begin{pmatrix} -1 & 7 \end{pmatrix} \left[ \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} \frac{100}{8} & 0 \\ 0 & \frac{100}{128}  \end{pmatrix} \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \right] \begin{pmatrix} -1  \\  7 \end{pmatrix} =
\begin{pmatrix} -1 & 7 \end{pmatrix} \begin{pmatrix} \frac{25}{2} & 0 \\ 0 & \frac{25}{32} \end{pmatrix} \begin{pmatrix} -1  \\  7 \end{pmatrix} = 50{,}78
\]

Тест отношения правдоподобия:
\[
LR = -2 \bigl( \ln \calL_{\text{R}} - \ln \calL_{\text{UR}} \bigr) \simhypo \chi^2_r
\]
\begin{multline*}
LR =  -2 \bigl( \ln \calL(\bs{q}) - \ln \calL(\hat{\bs\theta}) \bigr) = \\
 = -2 \left( -\frac{n}{2} \ln \sigma^2_0 - \frac{1}{2\sigma^2_0} \left( \sum X_i^2 - 2\mu_0 \sum X_i + n \mu^2_0\right) + \frac{n}{2} \ln \hat\sigma^2 + \frac{1}{2\hat\sigma^2} \left( \sum X_i^2 - 2\hat\mu \sum X_i + n \hat\mu^2\right) \right) = \\
= -2 \left( \cancel{- \frac{100}{2} \ln 1} - \frac{1}{2} ( 900 \xcancel{- 2\cdot 2 \cdot 100 + 100\cdot 2^2}) +  \frac{100}{2} \ln 8 + \frac{1}{16} ( 900 - 2\cdot 1 \cdot 100 + 100\cdot 1)  \right) \approx 592
\end{multline*}

Тест множителей Лагранжа:
\[
LM = \bs{S}(\bs{\theta}_0)' \bs{I}^{-1}(\bs{\theta}_{0}) \bs{S}(\bs{\theta}_{0}) \simhypo \chi^2_r
\]

\[
\bs{I}(\bs\theta_0) = \begin{pmatrix}  \frac{n}{\sigma^2_0} & 0 \\ 0  & \frac{n}{2(\sigma^2_0)^2} \end{pmatrix} = \begin{pmatrix}  100 & 0 \\ 0  & 50 \end{pmatrix}, \quad \bs{I}^{-1}(\bs\theta_0) = \begin{pmatrix}  \frac{1}{100} & 0 \\ 0  & \frac{1}{50} \end{pmatrix}\]

\[\bs{S}(\bs\theta_0) =  \begin{pmatrix} \frac{1}{\sigma^2_0 } (100-100\mu_0) \\ -\frac{100}{2\sigma^2_0} + \frac{1}{2(\sigma_0^2)^2 } (900 - 200\mu_0 + 100\mu_0^2)  \end{pmatrix} = \begin{pmatrix} -100 \\ 400 \end{pmatrix}
\]


\[
LM =  \begin{pmatrix} -100 & 400 \end{pmatrix}  \begin{pmatrix}  \frac{1}{100} & 0 \\ 0  & \frac{1}{50} \end{pmatrix}  \begin{pmatrix} -100 \\ 400 \end{pmatrix} = 3300
\]

Для уровня значимости 5\,\% критическое значение $\chi^2_2$ равно $\approx5{,}99$, поэтому во всех трёх тестах гипотеза $\hypo_0\colon \bs\theta=\bs\theta_0$ отвергается.

\end{solution}



%%%% попробовать взять несколько значений эпсилон с разной вероятностью?
\begin{problem}
Рассмотрим модель регрессии 
\begin{equation}
\begin{pmatrix}
y_1 \\
y_2 \\
y_3
\end{pmatrix}
=
\begin{pmatrix}
1 & 0 \\
1 & 0 \\
1 & 1
\end{pmatrix} \cdot
\begin{pmatrix}
\b_1 \\
\b_2 
\end{pmatrix} +
\begin{pmatrix}
\e_1 \\
\e_2 \\
e_3
\end{pmatrix}
\end{equation}
, где $\e_1$, $\e_2$, $\e_3$ независимы и равновероятно принимаю значения $+1$ и $-1$. При помощи метода максимального правдоподобия оцените $\b_1$, $\b_2$, если $y'=(-1,1,2)$.
\end{problem}

\begin{solution}
Можно решать перебором вариантов.
\end{solution}

\chapter{Логит и пробит}




\begin{problem}
Случайная величина $X$ имеет логистическое распределение, если её функция плотности имеет вид $f(x)=e^{-x}/(1+e^{-x})^2$.
\begin{enumerate}
\item Является ли $f(x)$ чётной?
\item Постройте график $f(x)$
\item Найдите функцию распределения, $F(x)$
\item Найдите $\E(X)$, $\Var(X)$
\item На какое известный закон распределения похож логистический?
\end{enumerate}
\end{problem}

\begin{solution}
$f(x)$ чётная, $\E(X)=0$, $\Var(X)=\pi^2/3$, логистическое похоже на $N(0,\pi^2/3)$
\end{solution}


\begin{problem}
Логит модель часто формулируют в таком виде:
\[
y_i^*=\beta_1+\beta_2 x_i +\e_i
\]
где $\e_i$ имеет логистическое распределение, и 
\[
y_i=\begin{cases}
1,\: y_i^*\geq 0 \\
0,\: y_i^*<0
\end{cases}
\]
\begin{enumerate}
\item Выразите $\P(y_i=1)$ с помощью логистической функции распределения 
\item Найдите $\ln \left(\frac{\P(y_i=1)}{\P(y_i=0)} \right)$
\end{enumerate}
\end{problem}

\begin{solution}
$\ln \left(\frac{\P(y_i=1)}{\P(y_i=0)} \right)=\beta_1+\beta_2 x_i$.
\end{solution}


\begin{problem}
\useR Сравните на одном графике
\begin{enumerate}
\item Функции плотности логистической и нормальной $N(0,\pi^2/3)$ случайных величин
\item Функции распределения логистической и нормальной $N(0,\pi^2/3)$ случайных величин
\end{enumerate}
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
Как известно, Фрекен Бок любит пить коньяк по утрам. За прошедшие 4 дня она записала, сколько рюмочек коньяка выпила утром, $x_i$, и видела ли она в этот день привидение, $y_i$, 

\begin{tabular}{c|ccccc}
$y_i$ & 1 & 0 & 1 & 0 & 0 \\
\hline
$x_i$ & 2 & 0 & 3 & 3 & 0 
\end{tabular}  

Зависимость между $y_i$ и $x_i$ описывается логит-моделью, 
\[
\ln 
\left(
  \frac{\P(y_i=1)}{\P(y_i=0)}
\right)
=\beta_1+\beta_2 x_i
\]

Предположим корректность использования нормального распределения для оценок правдоподобия.

\begin{enumerate}
\item Выпишите в явном виде логарифмическую функцию максимального правдоподобия
\item \useR Найдите оценки параметров $\beta_1$ и $\beta_2$
\item Найдите оценку ковариационной матрицы оценок коэффийиентов
\item Постройте доверительные интервалы для параметров $\beta_1$ и $\beta_2$
\item Постройте доверительный интервал для склонности увидеть привидение при трёх выпитых рюмках, $\beta_1 + \beta_2 x_i$.
\item Постройте доверительный интервал для вероятности увидеть привидение при трёх выпитых рюмках двумя способами:
\begin{enumerate}
\item Преобразованием доверительного интервала для склонности
\item С помощью дельта-метода
\end{enumerate}
\item Постройте график зависимости спрогнозированной вероятности увидеть приведение от количества рюмок. Фрекен Бок обладает способностью выпить дробное количество рюмок.
\item Постройте график зависимости предельного эффекта количества рюмок на вероятность увидеть приведение
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
При оценке логит модели 
\[
\P(y_i=1)=\Lambda(\b_1+\b_2 x_i)
\]
оказалось, что $\hb_1=0.7$ и $\hb_2=3$. Найдите максимальный предельный эффект роста $x_i$ на вероятность $\P(y_i=1)$.
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
Винни-Пух знает, что мёд бывает правильный, $honey_i=1$, и неправильный, $honey_i=0$. Пчёлы также бывают правильные, $bee_i=1$, и неправильные, $bee_i=0$. По 100 своим попыткам добыть мёд Винни-Пух составил таблицу сопряженности:

\begin{tabular}{c|cc}
 & $honey_i=1$ & $honey_i=0$ \\ 
\hline 
$bee_i=1$ & 12 & 36 \\ 
$bee_i=0$ & 32 & 20 \\ 
\end{tabular} 

Используя метод максимального правдоподобия Винни-Пух хочет оценить логит-модель для прогнозирования правильности мёда с помощью правильности пчёл:
\[
\ln \left(\frac{\mathbb{P}(honey_i=1)}{\mathbb{P}(honey_i=0)} \right)=\beta_1 + \beta_2 bee_i 
\]
\begin{enumerate}
\item Выпишите функцию правдоподобия для оценки параметров $\beta_1$ и $\beta_2$
\item Оцените неизвестные параметры
\item С помощью теста отношения правдоподобия проверьте гипотезу о том, правильность пчёл не связана с правильностью мёда на уровне значимости 5\%.
\item Держась в небе за воздушный шарик, Винни-Пух неожиданно понял, что перед ним неправильные пчёлы. Помогите ему оценить вероятность того, что они делают неправильный мёд.
\end{enumerate}
\end{problem}

\begin{solution}
Для краткости введем следующие обозначения: $y_i = honey_i$, $d_i = bee_i$\footnote{$Y_i$ --- случайный Мёд, $y_i$ --- реализация случайного Мёда (наблюдаемый Мёд)}.

\begin{enumerate}
\item Функция правдоподобия имеет следующий вид:

\begin{multline}
\text{L}(\beta_1, \beta_2) = \prod_{i=1}^n \mathbb{P}_{\beta_1, \beta_2} \left(\left\lbrace Y_i = y_i \right\rbrace \right) =\\
 \prod_{i: y_i = 0} \mathbb{P}_{\beta_1, \beta_2} \left(\left\lbrace Y_i = 1 \right\rbrace \right) \cdot \prod_{i: y_i = 1} \mathbb{P}_{\beta_1, \beta_2} \left(\left\lbrace Y_i = 0 \right\rbrace \right) =\\
\prod_{i: y_i = 1} \Lambda(\beta_1 + \beta_2 d_i) \cdot \prod_{i: y_i = 0} [1 - \Lambda(\beta_1 + \beta_2 d_i)] = \\
\prod_{i: y_i = 1, d_i = 1} \Lambda(\beta_1 + \beta_2) \cdot \prod_{i: y_i = 1, d_i = 0} \Lambda(\beta_1) \cdot \prod_{i: y_i = 0, d_i = 1} [1 - \Lambda(\beta_1 + \beta_2)] \cdot \\
\cdot \prod_{i: y_i = 0, d_i = 0} [1 - \Lambda(\beta_1)] = \\
\Lambda(\beta_1 + \beta_2)^{\#\{i: y_i=1, d_i=1\}} \cdot \Lambda(\beta_1)^{\#\{i: y_i=1, d_i=0\}} \cdot [1 - \Lambda(\beta_1 + \beta_2)]^{\#\{i: y_i=0, d_i=1\}} \cdot \\
\cdot  [1 - \Lambda(\beta_1)]^{\#\{i: y_i=0, d_i=0\}}
\end{multline}
где 
\begin{equation}
\label{eq: logist df}\Lambda(x) = \frac{e^x}{1 + e^x}
\end{equation}
логистическая функция распределения, $\#A$ означает число элементов множества $A$.

\item Введём следующие обозначения:
\begin{equation}
\label{eq: short a}a := \Lambda(\beta_1)
\end{equation}
\begin{equation}
\label{eq: short b}b := \Lambda(\beta_1 + \beta_2)
\end{equation}

Тогда с учетом имеющихся наблюдений функция правдоподобия принимает вид:

$$\text{L}(a, b) = b^{12} \cdot a^{32} \cdot [1 - b]^{36} \cdot [1 - a]^{20}$$

Логарифмическая функция правдоподобия:

$$l(a, b) = \ln \text{L}(a, b) = 12\ln b + 32\ln a + 36\ln[1-b] + 20\ln[1 - a]$$

Решая систему уравнений правдоподобия

$$\begin{cases}
\frac{\partial l}{\partial a} = \frac{32}{a} - \frac{20}{1 - a} = 0 \\
\frac{\partial l}{\partial b} = \frac{12}{b} - \frac{36}{1 - b} = 0 \\
\end{cases}$$
получаем $\hat{a} = \frac{8}{13}$, $\hat{b} = \frac{1}{4}$. Из формул (\ref{eq: logist df}) и (\ref{eq: short a}), находим $\hat{\beta}_{1, UR} = \ln\left( \frac{\hat{a}}{1 - \hat{a}} \right)= \ln \left(\frac{8}{5} \right) = 0.47$. Далее, из (\ref{eq: logist df}) и (\ref{eq: short b}) имеем $\hat{\beta}_{1, UR} + \hat{\beta}_{2, UR} = \ln \left( \frac{\hat{b}}{1 - \hat{b}} \right)$. Следовательно, $\hat{\beta}_{2, UR} = \ln \left( \frac{\hat{b}}{1 - \hat{b}} \right) - \hat{\beta}_{1, UR} = \ln \left( \frac{1}{3} \right) - \ln \left( \frac{8}{5} \right) = -1.57$.

\item Гипотеза, состоящая в том, что <<правильность Мёда не связана с правильностью 
пчёл>> формализуется как $H_0: \beta_2 = 0$. Протестируем данную гипотезу при помощи теста 
отношения правдоподобия. Положим в функции правдоподобия $\text{L}(\beta_1, \beta_2)$ $\beta_2 = 0$. Тогда с учетом (\ref{eq: short a}) и (\ref{eq: short b}) получим
$$\text{L}(a, b=a) = a^{32+12} \cdot [1 - a]^{20+36}$$
В этом случае логарифмическая функция правдоподобия имеет вид:
$$l(a, b=a) := \text{L}(a, b=a) = 44 \ln a + 56 \ln[1 - a]$$
Решаем уравнение правдоподобия
$$\frac{\partial l}{\partial a} = \frac{44}{a} - \frac{56}{1 - a} = 0$$
и получаем $\hat{a} = \frac{11}{25}$. Следовательно, согласно (\ref{eq: logist df}) и (\ref{eq: short a}), $\hat{\beta}_{1, R} = -0.24$ и $\hat{\beta}_{2, R} = 0$.

Статистика отношения правдоподобия имеет вид:
$$LR = -2(l(\hat{\beta}_{1, R}, \hat{\beta}_{2, R}) - l(\hat{\beta}_{1, UR}, \hat{\beta}_{2, UR}))$$
и имеет асимптотическое $\chi^2$ распределение с числом степеней свободы, равным числу ограничений, составляющих гипотезу $H_0$, т.е. в данном случае $LR \overset{a}{\sim} \chi^2_1$.

Находим наблюдаемое значение статистики отношения правдоподобия:
\begin{multline}
l(\hat{\beta}_{1, R}, \hat{\beta}_{2, R}) = l(\hat{a}_R, \hat{b}_R = \hat{a}_R) = 44\ln\hat{a}_R + 56\ln[1-\hat{a}_R] =\\
 44\ln \left[ \frac{11}{25} \right] + 56\ln \left[ 1 - \frac{11}{25} \right] = -68.59
\end{multline}
\begin{multline}
l(\hat{\beta}_{1, UR}, \hat{\beta}_{2, UR}) = l(\hat{a}_{UR}, \hat{b}_{UR}) =\\
 12\ln \hat{b}_{UR} + 32 \ln \hat{a}_{UR} + 36\ln[1 - \hat{b}_{UR}] + 20\ln[1 - \hat{a}_{UR}] = \\
12\ln \left[ \frac{1}{4} \right] + 32\ln \left[ \frac{8}{13} \right] + 36\ln \left[ 1 - \frac{1}{4} \right] + 20\ln \left[1 - \frac{8}{13} \right] = -61.63
\end{multline}
Следовательно, $LR_{\text{набл}} = -2(-68.59 + 61.63) = 13.92$, при этом критическое значение $\chi^2$ распределения с одной степенью свободы для 5\% уровня значимости равна 3.84. Значит, на основании теста отношения правдоподобия гипотеза $H_0: \beta_2 = 0$ должна быть отвергнута. Таким образом, данные показывают, что, в действительности, правильность мёда связана с правильностью пчёл.

\item 
\begin{multline}
\hat{\mathbb{P}}\{honey = 0| bee = 0\} = 1 - \hat{\mathbb{P}}\{honey=1|bee=0\} = \\
1 - \frac{\exp\{\hat{\beta}_{1, UR} + \hat{\beta}_{2, UR} \cdot 0\}}{1 + \exp\{\hat{\beta}_{1, UR} + \hat{\beta}_{2, UR} \cdot 0\}} =\\
1 - \frac{\exp\{\ln\left( \frac{8}{5} \right)\}}{1 + \exp\{\ln\left( \frac{8}{5} \right)\}} = 1 - 0.62 = 0.38
\end{multline}
\end{enumerate}
\end{solution}


\begin{problem}
Имеются следующие наблюдения

\begin{tabular}{c|ccc}
$y_i$ & 1 & 0 & 0  \\
\hline
$x_i$ & 1 & 2 & 3 
\end{tabular}  

Предположим, что логит модель верна.

\begin{enumerate}
\item Существуют ли оценки метода максимального правдоподобия?
\item Что произойдёт, если попробовать оценить эту модель в R?
\item Спасёт ли ситуацию рассмотрение пробит-модели?
\item В рамках байесовского подхода найдите среднее апостериорного распределения коэффициентов при априорном предположении $\beta_1 \sim N(0, 100^2)$, $\beta_2 \sim N(0, 100^2)$ и их независимости.
\end{enumerate}

\end{problem}

\begin{solution}
в теории оценки не существуют, в R получатся некие точечные оценки, достаточно далеко лежащие от нуля с огромными стандартными ошибками и P-значением близким к 1.
\end{solution}






% Нарушения


\chapter{Мультиколлинеарность}


\begin{problem}
Сгенерируйте данные так, чтобы при оценке модели $\hat{y}=\hat{\beta_1}+\hat{\beta_2}x+\hat{\beta_3}z$ оказывалось, что по отдельности оценки коэффициентов $\hat{\beta_2}$ и $\hat{\beta_3}$ незначимы, но модель в целом --- значима.
\end{problem}
\begin{solution}
\end{solution}

\begin{problem}
В этом задании нужно сгенерировать зависимую переменную $y$ и два регрессора $x$ и $z$.
\begin{enumerate}
\item Сгенерируйте данные так, чтобы корреляция между регрессорами $x$ и $z$ была больше $0.9$, и проблема мультиколлинеарности есть, т.е. по отдельности регрессоры не значимы, но регрессия в целом --- значима.
\item А теперь сгенерируйте данные так, чтобы корреляция между регрессорами была по-прежнему больше $0.9$, но проблемы мультиколлинеарности бы не было, т.е. все коэффициенты были бы значимы.
\item Есть несколько способов, как изменить генерации случайных величин, чтобы перейти от ситуации <<a>> к ситуации <<b>>. Назовите хотя бы два.   
\end{enumerate}
\end{problem}
\begin{solution}
увеличить количество наблюдений, уменьшить дисперсию случайной ошибки
\end{solution}


\begin{problem}
Исследуем зависимость длины тормозного пути автомобиля от скорости по историческим данным 1920-х годов.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{h} \hlkwb{<-} \hlstd{cars}
\hlkwd{ggplot}\hlstd{(h,}\hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=speed,}\hlkwc{y}\hlstd{=dist))}\hlopt{+}\hlkwd{geom_point}\hlstd{()}\hlopt{+}
    \hlkwd{labs}\hlstd{(}\hlkwc{title}\hlstd{=}\hlstr{"Зависимость длины тормозного пути"}\hlstd{,}
    \hlkwc{x}\hlstd{=}\hlstr{"Скорость, миль в час"}\hlstd{,}\hlkwc{y}\hlstd{=}\hlstr{"Длина пути, футов"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-50-1} 
\begin{kframe}\begin{alltt}
\hlstd{speed.mean} \hlkwb{<-} \hlkwd{mean}\hlstd{(h}\hlopt{$}\hlstd{speed)}
\end{alltt}
\end{kframe}
\end{knitrout}

Построим результаты оценивания нецентрированной регрессии:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{cars.model} \hlkwb{<-} \hlkwd{lm}\hlstd{(dist}\hlopt{~}\hlstd{speed}\hlopt{+}\hlkwd{I}\hlstd{(speed}\hlopt{^}\hlnum{2}\hlstd{)}\hlopt{+}\hlkwd{I}\hlstd{(speed}\hlopt{^}\hlnum{3}\hlstd{),}\hlkwc{data}\hlstd{=h)}
\hlstd{cars.table} \hlkwb{<-} \hlkwd{as.table}\hlstd{(}\hlkwd{coeftest}\hlstd{(cars.model))}
\hlkwd{rownames}\hlstd{(cars.table)} \hlkwb{<-}
  \hlkwd{c}\hlstd{(}\hlstr{"Константа"}\hlstd{,}\hlstr{"speed"}\hlstd{,}\hlstr{"speed^2"}\hlstd{,}\hlstr{"speed^3"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

%\todo[inline]{с тремя переменными руками громоздко делать, а с двумя вроде не видно мультик.}

\begin{kframe}
\begin{alltt}
\hlkwd{xtable}\hlstd{(cars.table)}
\end{alltt}
\end{kframe}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:05 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
Константа & -19.51 & 28.41 & -0.69 & 0.50 \\ 
  speed & 6.80 & 6.80 & 1.00 & 0.32 \\ 
  speed\verb|^|2 & -0.35 & 0.50 & -0.70 & 0.49 \\ 
  speed\verb|^|3 & 0.01 & 0.01 & 0.91 & 0.37 \\ 
   \hline
\end{tabular}
\end{table}


Ковариационная матрица коэффициентов имеет вид:
\begin{kframe}
\begin{alltt}
\hlstd{cars.vcov} \hlkwb{<-} \hlkwd{vcov}\hlstd{(cars.model)}
\hlkwd{rownames}\hlstd{(cars.vcov)} \hlkwb{<-}
  \hlkwd{c}\hlstd{(}\hlstr{"Константа"}\hlstd{,}\hlstr{"speed"}\hlstd{,}\hlstr{"speed^2"}\hlstd{,}\hlstr{"speed^3"}\hlstd{)}
\hlkwd{colnames}\hlstd{(cars.vcov)} \hlkwb{<-}
  \hlkwd{c}\hlstd{(}\hlstr{"Константа"}\hlstd{,}\hlstr{"speed"}\hlstd{,}\hlstr{"speed^2"}\hlstd{,}\hlstr{"speed^3"}\hlstd{)}
\hlkwd{xtable}\hlstd{(cars.vcov)}
\end{alltt}
\end{kframe}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:05 2015
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Константа & speed & speed\verb|^|2 & speed\verb|^|3 \\ 
  \hline
Константа & 806.86 & -186.20 & 12.88 & -0.27 \\ 
  speed & -186.20 & 46.26 & -3.35 & 0.07 \\ 
  speed\verb|^|2 & 12.88 & -3.35 & 0.25 & -0.01 \\ 
  speed\verb|^|3 & -0.27 & 0.07 & -0.01 & 0.00 \\ 
   \hline
\end{tabular}
\end{table}


\begin{enumerate}
\item Проверьте значимость всех коэффициентов и регрессии в целом
\item Постройте 95\%-ый доверительный интервал для $\E(dist)$ при $speed=10$
\item Постройте 95\%-ый доверительный интервал для $\E(ddist/dspeed)$ при $speed=10$
\item Как выглядит уравнение регрессии, если вместо $speed$ использовать центрированную скорость? Известно, что средняя скорость равна $15.4$
\item С помощью регрессии с центрированной скоростью ответьте на предыдущие вопросы.
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Пионеры, Крокодил Гена и Чебурашка собирали металлолом несколько дней подряд. В распоряжение иностранной шпионки, гражданки Шапокляк, попали ежедневные данные по количеству собранного металлолома: вектор $g$ --- для Крокодила Гены, вектор $h$ --- для Чебурашки и вектор $x$ --- для Пионеров. Гена и Чебурашка собирали вместе, поэтому выборочная корреляция $\sCorr(g,h)=-0.9$. Гена и Чебурашка собирали независимо от Пионеров, поэтому выборочные корреляции $\sCorr(g,x)=0$, $\sCorr(h,x)=0$. Если регрессоры $g$, $h$ и $x$ центрировать и нормировать, то получится матрица $\tilde{X}$.
\begin{enumerate}
\item Найдите параметр обусловленности матрицы $(\tilde{X}'\tilde{X})$
\item Вычислите одну или две главные компоненты (выразите их через вектор-столбцы матрицы $\tilde{X}$), объясняющие не менее 70\% общей выборочной дисперсии регрессоров
\item Шпионка Шапокляк пытается смоделировать ежедневный выпуск танков, $y$. Выразите коэффициенты регрессии $y = \beta_1 + \beta_2 g +\beta_3 h +\beta_4 x+\varepsilon$ через коэффициенты регрессии на главные компоненты, объясняющие не менее 70\% общей выборочной дисперсии. 
\end{enumerate}
\end{problem}
\begin{solution}
\end{solution}



\begin{problem}
Для модели $y_i=\beta x_i+\e$ рассмотрите модель Ridge regression с коэффициентом $\lambda$.
\begin{enumerate}
\item Выведите формулу для $\hb_{RR}$
\item Найдите $\E(\hb_{RR})$, смещение оценки $\hb_{RR}$,
\item Найдите $\Var(\hb_{RR})$, $MSE(\hb_{RR})$
\item Всегда ли оценка $\hb_{RR}$ смещена?
\item Всегда ли оценка $\hb_{RR}$ имеет меньшую дисперсию, чем $\hb_{ols}$?
\item Найдите такое $\lambda$, что $MSE(\hb_{RR})<MSE(\hb_{ols})$
\end{enumerate}
\end{problem}
\begin{solution}
\end{solution}



\begin{problem}
Известно, что в модели $y=X\beta+\e$ все регрессоры ортогональны. 
\begin{enumerate}
\item Как выглядит матрица $X'X$ в случае ортогональных регрессоров?
\item Выведите $\hb_{rr}$ в явном виде
\item Как связаны между собой $\hb_{rr}$ и $\hb_{ols}$?
\end{enumerate}
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
Для модели $y_i=\beta x_i + \e_i$ выведите в явном виде $\hb_{lasso}$. 
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
Предположим, что для модели $y_i= \beta_1 + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4}  + \e_i$ выборочная корреляционная матрица регрессоров $x_2$, $x_3$, $x_4$ имеет вид

\[
C =
\begin{pmatrix}
1 & r & r  \\
r & 1 & r  \\
r & r & 1   
\end{pmatrix}
\]


\begin{enumerate}
\item Найдите такое значение $r^* \in (-1;1)$ коэффициента корреляции, при котором $\det C = 0$.
\item Найдите собственные значения и собственные векторы матрицы $C$ при корреляции равной найденному $r^*$.
\item Найдите число обусловленности матрицы $C$ при корреляции равной найденному $r^*$.
\item Сделайте вывод о наличии мультиколлинеарности в модели при корреляции равной найденному $r^*$.
\end{enumerate}
\end{problem}

\begin{solution}
$r^* = -1/2$
\end{solution}




\begin{problem}
Предположим, что для модели $y_i= \beta_1 + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \beta_5 x_{i5} + \e_i$ выборочная корреляционная матрица регрессоров $x_2$, $x_3$, $x_4$ и $x_5$ имеет вид

\[
C =
\begin{pmatrix}
1 & r & r & r \\
r & 1 & r & r \\
r & r & 1 & r \\
r & r & r & 1 
\end{pmatrix}
\]


\begin{enumerate}
\item Найдите такое значение $r^* \in (-1;1)$ коэффициента корреляции, при котором $\det C = 0$.
\item Найдите собственные значения и собственные векторы матрицы $C$ при корреляции равной найденному $r^*$.
\item Найдите число обусловленности матрицы $C$ при корреляции равной найденному $r^*$.
\item Сделайте вывод о наличии мультиколлинеарности в модели при корреляции равной найденному $r^*$.
\end{enumerate}
\end{problem}

\begin{solution}
$r^* = -1/3$
\end{solution}





%%%%%%% здесь?... LASSO, rigdre regression



\chapter{Гетероскедастичность}


\begin{problem}
Что такое гетероскедастичность? Гомоскедастичность?
\end{problem}
\begin{solution}
\end{solution}

\begin{problem}
В модели $y_i = \beta_1 + \beta_2 x_i + \e_i$ присутствует гетероскедастичность вида $\Var(\e_i)=\sigma^2 x^2_i$. Как надо преобразовать исходные регрессоры и зависимую переменную, чтобы устранить гетероскедастичность?
\end{problem}

\begin{solution}
Поделить зависимую переменную и каждый регрессор, включая единичный столбец, на $|x_i|$.
\end{solution}


\begin{problem}
В модели $y_i = \beta_1 + \beta_2 x_i + \e_i$ присутствует гетероскедастичность вида $\Var(\e_i)=\lambda |x_i|$. Как надо преобразовать исходные регрессоры и зависимую переменную, чтобы устранить гетероскедастичность?
\end{problem}

\begin{solution}
Поделить зависимую переменную и каждый регрессор, включая единичный столбец, на $\sqrt{|x_i|}$.
\end{solution}


\begin{problem}
Известно, что после деления каждого уравнения регрессии $y_i = \beta_1 + \beta_2 x_i + \e_i$ на $x_i^2$ гетероскедастичность ошибок была устранена. Какой вид имела дисперсия ошибок, $\Var(\e_i)$?
\end{problem}

\begin{solution}
$\Var(\e_i)=cx_i^4$
\end{solution}


\begin{problem}
Известно, что после деления каждого уравнения регрессии $y_i = \beta_1 + \beta_2 x_i + \e_i$ на $\sqrt{x_i}$ гетероскедастичность ошибок была устранена. Какой вид имела дисперсия ошибок, $\Var(\e_i)$?
\end{problem}
\begin{solution}
$\Var(\e_i)=c x_i$
\end{solution}


\begin{problem}
Диаграмма рассеяния стоимости квартиры в Москве (в 1000\$) и общей площади квартиры имеет вид:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{ggplot}\hlstd{(flats,}\hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=totsp,}\hlkwc{y}\hlstd{=price))}\hlopt{+}\hlkwd{geom_point}\hlstd{()}\hlopt{+}
    \hlkwd{labs}\hlstd{(}\hlkwc{x}\hlstd{=}\hlstr{"Общая площадь, кв. м."}\hlstd{,}
    \hlkwc{y}\hlstd{=}\hlstr{"Цена квартиры, 1000\textbackslash{}\textbackslash{}$"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-54-1} 

\end{knitrout}

Какие подходы к оцениванию зависимости имеет смысл посоветовать исходя из данного графика?
\end{problem}


\begin{solution}
По графику видно, что с увеличением общей площади увеличивается разброс цены. Поэтому разумно, например, рассмотреть следующие подходы:
\begin{enumerate}
\item Перейти к логарифмам, т.е. оценивать модель $\ln price_i=\beta_1+\beta_2 \ln totsp_i +\varepsilon_i$
\item Оценивать квантильную регрессию. В ней угловые коэффициенты линейной зависимости будут отличаться для разных квантилей переменной $price$.
\item Обычную модель линейной регрессии с гетероскедастичностью вида $Var(\varepsilon_i)=\sigma^2 totsp_i^2$
\end{enumerate}
\end{solution}



\begin{problem}
По наблюдениям $x=(1,2,3)'$, $y=(2,-1,3)'$ оценивается модель $y=\b_1+\b_2 x+\e$. Ошибки $\e$ гетероскедастичны и известно, что $\Var(\e_i)=\sigma^2 \cdot x_i^2$.
\begin{enumerate}
\item Найдите оценки $\hb_{ols}$ с помощью МНК и их ковариационную матрицу
\item Найдите оценки $\hb_{gls}$ с помощью обобщенного МНК и их ковариационную матрицу
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}




\begin{problem}
Для линейной регрессии $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i + \e_i$ была
выполнена сортировка наблюдений по возрастанию переменной $x$. Исходная модель оценивалась по разным частям выборки:

\begin{tabular}{c|cccc}
Выборка & $\hb_1$ & $\hb_2$ & $\hb_3$ & $RSS$ \\
\hline
$i=1,\ldots, 30$ & $1.21$ & $1.89$ & $2.74$ & $48.69$ \\
$i=1,\ldots, 11$ & $1.39$ & $2.27$ & $2.36$ & $10.28$ \\
$i=12,\ldots, 19$ & $0.75$ & $2.23$ & $3.19$ & $5.31$ \\
$i=20,\ldots, 30$ & $1.56$ & $1.06$ & $2.29$ & $14.51$ \\
\end{tabular}

Известно, что ошибки в модели являются независимыми нормальными случайными величинами с нулевым математическим ожиданием. Протестируйте
ошибки на гетероскедастичность на уровне значимости 5\%.
\end{problem}


\begin{solution}
Протестируем гетероскедастичность ошибок при помощи теста Голдфельда-
Квандта. $H_0: \Var(\e_i)=\sigma^2$, $H_a: \Var(\e_i)=f(x_i)$

\begin{enumerate}
\item Тестовая статистика $GQ=\frac{RSS_3/(n_3-k)}{RSS_1/(n_1-k)}$, где $n_1=11$ --- число наблюдений в первой подгруппе, $n_3=11$ --- число наблюдений в
последней подгруппе, $k=3$ --- число факторов в модели, считая единичный столбец.
\item Распределение тестовой статистики при верной $H_0$: $GQ\sim F_{n_3-k,n_1-k}$
\item Наблюдаемое значение $GQ_{obs}=1.41$
\item Область, в которой $H_0$ не отвергается: $GQ\in [0;3.44]$
\item Статистический вывод: поскольку $GQ_{obs} \in [0;3.44]$, то на основании имеющихся наблюдений на уровне значимости 5\% основная гипотеза $H_0$ не может быть отвергнута. Таким образом, тест Голдфельда-Квандта не выявил гетероскедастичность.
\end{enumerate}
\end{solution}


\begin{problem}
Для линейной регрессии $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i + \e_i$ была
выполнена сортировка наблюдений по возрастанию переменной $x$. Исходная модель оценивалась по разным частям выборки:

\begin{tabular}{c|cccc}
Выборка & $\hb_1$ & $\hb_2$ & $\hb_3$ & $RSS$ \\
\hline
$i=1,\ldots, 50$ & $1.16$ & $1.99$ & $2.97$ & $174.69$ \\
$i=1,\ldots, 21$ & $0.76$ & $2.25$ & $3.18$ & $20.41$ \\
$i=22,\ldots, 29$ & $0.85$ & $1.81$ & $3.32$ & $3.95$ \\
$i=30,\ldots, 50$ & $1.72$ & $1.41$ & $2.49$ & $130.74$ \\
\end{tabular}

Известно, что ошибки в модели являются независимыми нормальными случайными величинами с нулевым математическим ожиданием. Протестируйте
ошибки на гетероскедастичность на уровне значимости 1\%.
\end{problem}

\begin{solution}
Протестируем гетероскедастичность ошибок при помощи теста Голдфельда-
Квандта. $H_0: \Var(\e_i)=\sigma^2$, $H_a: \Var(\e_i)=f(x_i)$

\begin{enumerate}
\item Тестовая статистика $GQ=\frac{RSS_3/(n_3-k)}{RSS_1/(n_1-k)}$, где $n_1=21$ --- число наблюдений в первой подгруппе, $n_3=21$ --- число наблюдений в
последней подгруппе, $k=3$ --- число факторов в модели, считая единичный столбец.
\item Распределение тестовой статистики при верной $H_0$: $GQ\sim F_{n_3-k,n_1-k}$
\item Наблюдаемое значение $GQ_{obs}=6.49$
\item Область, в которой $H_0$ не отвергается: $GQ\in [0;3.12]$
\item Статистический вывод: поскольку $GQ_{obs} \notin [0;3.12]$, то на основании имеющихся наблюдений на уровне значимости 1\% основная гипотеза $H_0$ отвергается. Таким образом, тест Голдфельда-Квандта выявил гетероскедастичность.
\end{enumerate}
\end{solution}


\begin{problem}
Для линейной регрессии $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i + \e_i$ была
выполнена сортировка наблюдений по возрастанию переменной $x$. Исходная модель оценивалась по разным частям выборки:

\begin{tabular}{c|cccc}
Выборка & $\hb_1$ & $\hb_2$ & $\hb_3$ & $RSS$ \\
\hline
$i=1,\ldots, 30$ & $0.96$ & $2.25$ & $3.44$ & $52.70$ \\
$i=1,\ldots, 11$ & $1.07$ & $2.46$ & $2.40$ & $5.55$ \\
$i=12,\ldots, 19$ & $1.32$ & $1.01$ & $2.88$ & $11.69$ \\
$i=20,\ldots, 30$ & $1.04$ & $2.56$ & $4.12$ & $16.00$ \\
\end{tabular}

Известно, что ошибки в модели являются независимыми нормальными случайными величинами с нулевым математическим ожиданием. Протестируйте
ошибки на гетероскедастичность на уровне значимости 5\%.
\end{problem}

\begin{solution}
Протестируем гетероскедастичность ошибок при помощи теста Голдфельда-
Квандта. $H_0: \Var(\e_i)=\sigma^2$, $H_a: \Var(\e_i)=f(x_i)$

\begin{enumerate}
\item Тестовая статистика $GQ=\frac{RSS_3/(n_3-k)}{RSS_1/(n_1-k)}$, где $n_1=11$ --- число наблюдений в первой подгруппе, $n_3=11$ --- число наблюдений в
последней подгруппе, $k=3$ --- число факторов в модели, считая единичный столбец.
\item Распределение тестовой статистики при верной $H_0$: $GQ\sim F_{n_3-k,n_1-k}$
\item Наблюдаемое значение $GQ_{obs}=2.88$
\item Область, в которой $H_0$ не отвергается: $GQ\in [0;3.44]$
\item Статистический вывод: поскольку $GQ_{obs} \in [0;3.44]$, то на основании имеющихся наблюдений на уровне значимости 5\% основная гипотеза $H_0$ не может быть отвергнута. Таким образом, тест Голдфельда-Квандта не выявил гетероскедастичность.
\end{enumerate}
\end{solution}


\begin{problem}
Для линейной регрессии $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i + \e_i$ была выполнена сортировка наблюдений по возрастанию переменной $x$. Исходная модель оценивалась по разным частям выборки:

\begin{tabular}{c|cccc}
Выборка & $\hb_1$ & $\hb_2$ & $\hb_3$ & $RSS$ \\

\hline
$i=1,\ldots, 50$ & $0.93$ & $2.02$ & $3.38$ & $145.85$ \\
$i=1,\ldots, 21$ & $1.12$ & $2.01$ & $3.32$ & $19.88$ \\
$i=22,\ldots, 29$ & $0.29$ & $2.07$ & $2.24$ & $1.94$ \\
$i=30,\ldots, 50$ & $0.87$ & $1.84$ & $3.66$ & $117.46$ \\
\end{tabular}

Известно, что ошибки в модели являются независимыми нормальными случайными величинами с нулевым математическим ожиданием. Протестируйте
ошибки на гетероскедастичность на уровне значимости 5\%.
\end{problem}
\begin{solution}
Протестируем гетероскедастичность ошибок при помощи теста Голдфельда-
Квандта. $H_0: \Var(\e_i)=\sigma^2$, $H_a: \Var(\e_i)=f(x_i)$

\begin{enumerate}
\item Тестовая статистика $GQ=\frac{RSS_3/(n_3-k)}{RSS_1/(n_1-k)}$, где $n_1=21$ --- число наблюдений в первой подгруппе, $n_3=21$ --- число наблюдений в
последней подгруппе, $k=3$ --- число факторов в модели, считая единичный столбец.
\item Распределение тестовой статистики при верной $H_0$: $GQ\sim F_{n_3-k,n_1-k}$
\item Наблюдаемое значение $GQ_{obs}=5.91$
\item Область, в которой $H_0$ не отвергается: $GQ\in [0;2.21]$
\item Статистический вывод: поскольку $GQ_{obs} \notin [0;2.21]$, то на основании имеющихся наблюдений на уровне значимости 5\% основная гипотеза $H_0$ отвергается. Таким образом, тест Голдфельда-Квандта выявил гетероскедастичность.
\end{enumerate}
\end{solution}


\begin{problem}
Рассмотрим линейную регрессию $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i + \e_i$ по 50 наблюдениям. При оценивании с помощью МНК были получены результаты: $\hb_1=1.21$, $\hb_2=1.11$, $\hb_3=3.15$, $R^2=0.72$.

Оценена также вспомогательная регрессия: $\he^2_i=\delta_1+\delta_2 x_i +\delta_3 z_i+\delta_4 x_i^2+\delta_5 z_i^2+\delta_6 x_i z_i + u_i$. Результаты оценивания следующие: $\hat{\delta}_1=1.50$, $\hat{\delta}_2=-2.18$,  $\hat{\delta}_3=0.23$,  $\hat{\delta}_4=1.87$,  $\hat{\delta}_5=-0.56$,  $\hat{\delta}_6=-0.09$,  $R^2_{aux}=0.36$


Известно, что ошибки в модели являются независимыми нормальными случайными величинами с нулевым математическим ожиданием. Протестируйте
ошибки на гетероскедастичность на уровне значимости 5\%.
\end{problem}

\begin{solution}
Протестируем гетероскедастичность ошибок при помощи теста Уайта. $H_0: \Var(\e_i)=\sigma^2$, $H_a: \Var(\e_i)=\delta_1+\delta_2 x_i +\delta_3 z_i+\delta_4 x_i^2+\delta_5 z_i^2+\delta_6 x_i z_i$.
\begin{enumerate}
\item Тестовая статистика $W=n\cdot R^2_{aux}$, где $n$ --- число наблюдений, $R^2_{aux}$ --- коэффициент детерминации для вспомогательной регрессии.
\item Распределение тестовой статистики при верной $H_0$: $W\sim \chi^2_{k_{aux}-1}$, где $k_{aux}=6$ --- число регрессоров во вспомогательной регрессии, считая константу.
\item Наблюдаемое значение тестовой статистики: $W_{obs}=18$
\item Область, в которой $H_0$ не отвергается: $W\in [0;W_{crit}]=[0;11.07]$
\item Статистический вывод: поскольку $W_{obs} \notin [0;11.07]$, то на основании имеющихся наблюдений на уровне значимости 5\% основная гипотеза $H_0$ отвергается. Таким образом, тест Уайта выявил гетероскедастичность.
\end{enumerate}
\end{solution}



\begin{problem}
Объясните, с какой целью используются стандартные ошибки в форме Уайта. Приведите развернутый ответ. Верно ли, что стандартные ошибки в форме Уайта позволяют
\begin{enumerate}
\item устранить гетероскедастичность?
\item корректно тестировать гипотезы относительно коэффициентов регрессии в условиях гетероскедастичности?
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Объясните, с какой целью используются стандартные ошибки в форме Невье–Веста. Приведите развернутый ответ. Верно ли, что стандартные ошибки в форме Невье–Веста позволяют
\begin{enumerate}
\item устранить гетероскедастичность?
\item корректно тестировать гипотезы относительно коэффициентов регрессии в условиях гетероскедастичности?
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Рассматривается модель $y_t=\beta_1+\e_t$, где ошибки $\e_t$  --- независимые
случайные величины с $\E(\e_t)=0$ и $\Var(\e_t)=t$. Найдите наиболее эффективную
оценку неизвестного параметра $\beta_1$ в классе линейных по $y$ и несмещенных оценок.
\end{problem}
\begin{solution}
\end{solution}

\begin{problem}
Рассматривается модель $y_t=\beta_1+\e_t$, где ошибки $\e_t$  --- независимые
случайные величины с $\E(\e_t)=0$ и $\Var(\e_t)=t^2$. Найдите наиболее эффективную
оценку неизвестного параметра $\beta_1$ в классе линейных по $y$ и несмещенных оценок.
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
Рассматривается модель $y_t=\beta_1 x_t+\e_t$, где ошибки $\e_t$  --- независимые
случайные величины с $\E(\e_t)=0$ и $\Var(\e_t)=t$. Найдите наиболее эффективную
оценку неизвестного параметра $\beta_1$ в классе линейных по $y$ и несмещенных оценок.
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
Рассматривается модель $y_t=\beta_1 x_t +\e_t$, где ошибки $\e_t$  --- независимые
случайные величины с $\E(\e_t)=0$ и $\Var(\e_t)=t^2$. Найдите наиболее эффективную
оценку неизвестного параметра $\beta_1$ в классе линейных по $y$ и несмещенных оценок.
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
Докажите, что в условиях гетероскедастичности МНК-
оценки остаются несмещенными.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Оценка коэффициентов обобщенного МНК имеет вид $\hb_{GLS}=(X'V^{-1}X)^{-1}X'V^{-1}y$, где $V=\Var(\e)$. Совпадает ли оценка $\hb_{GLS}$ с оценкой обычным МНК в условиях гомоскедастичности?
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Модель $y_i=\beta_1 + \beta_2 x_i +\e_i$ оценивается по трём наблюдениям, $y=(9,3,6)$, $x=(1,2,4)$. Имеется гетероскедастичность вида $\Var(\e_i)=\sigma^2 x_i^2$, ошибки $\e_i$ нормально распределены.
\begin{enumerate}
\item Оцените $\hb$ с помощью МНК проигнорировав гетероскедастичность. Постройте 95\% доверительный интервал для каждого коэффициента, проигнорировав гетероскедастичность
\item Оцените $\hb$ с помощью обобщенного МНК учтя гетероскедастичность. Постройте 95\% доверительный интервал для каждого коэффициента с учётом гетероскедастичности
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Рассмотрим модель $y_i=\beta_1+\beta_2 x_i+\e_i$, где ошибки $\e_i$ некоррелированы, $\E(\e_i)=0$, $\Var(\e_i)=\sigma^2_i$. Предлагается два способа оценить коэффициенты модели:
\begin{enumerate}
\item[WLS.] Взвешенный метод наименьших квадратов. Поделим каждое уравнение $y_i=\beta_1+\beta_2 x_i+\e_i$ на $\sigma_i$. Затем обычным методом наименьших квадратов в преобразованной модели $y_i/\sigma_i=\beta_1\cdot 1/\sigma_i+\beta_2 x_i/\sigma_i+\e_i/\sigma_i$ найдем оценки $\hb_{WLS}$.
\item[GLS.] Обобщенный метод наименьших квадратов. Оценки $\hb_{GLS}$ находим по формуле $\hb_{GLS}=(X'V^{-1}X)^{-1}X'V^{-1}y$, где
\[
V=\Var(\e)=\begin{pmatrix}
\sigma^2_1 & 0 & 0 \\
0 & \ddots & 0 \\
0 & 0 & \sigma^2_n
\end{pmatrix}
\]
\end{enumerate}
\begin{enumerate}
\item Докажите, что в матричном виде преобразование взвешенного МНК записывается как $V^{-1/2}y=V^{-1/2}X\beta+V^{-1/2}\e$.
\item Верно ли, что $\hb_{WLS}=\hb_{GLS}$?
\item Найдите $\E(\hb_{WLS})$, $\Var(\hb_{WLGS})$
\item В явном виде выпишите $\hb_{2,WLS}$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
Рассмотрим модель регрессии $y_i=\beta_1+\beta_2 x_i + \beta_3 z_i+\e_i$, в которой
ошибки $\e_i$ независимы и имеют нормальное распределение $N(0,\sigma^2)$. Для $n = 200$ наблюдений найдите
\begin{enumerate}
\item вероятность того, что статистика Уайта окажется больше 10,
\item ожидаемое значение статистики Уайта,
\item дисперсию статистики Уайта.
\end{enumerate}
\end{problem}

\begin{solution}
$0.0752$, $5$, $10$
\end{solution}

\begin{problem}
Найдите число коэффициентов во вспомогательной регрессии, необходимой для выполнения теста Уайта, если число коэффициентов в исходной регрессии равно $k$, включая свободный член.
\end{problem}

\begin{solution}
$k(k+1)/2$
\end{solution}

\begin{problem}
(Кирилл Фурманов) По 35 наблюдениям сотрудники НИИ оценили уравнение регрессии $y_i = \beta_1 + \beta_2 x_i + \e_i$ и рассчитали остатки $\e_i$. После того они приступили к диагностике возможных недостатков модели, обнаружили гетероскедастичность и решили её побороть.
\begin{enumerate}
\item[(a)] Самый младший научный сотрудник выдвинул предположение, что стандартное отклонение случайной составляющей может быть выражено так: $\sigma_{\e, i} = a x_i$, где $a$ --– неизвестный коэффициент. Каким образом нужно преобразовать исходное уравнение регрессии, чтобы избавиться от гетероскедастичности?
\item[(b)] Профессор решил перепроверить результаты и оценил регрессию:
$$\hat{e}_i^2 = -0.3 + 0.08 x_i - 0.01 x_i^2, R^2 = 0.15$$
Свидетельствует ли полученный профессором результат о наличии гетероскедастичности?
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
 Пусть $y_t = \beta x_t + \e$ где $\E(\e_t) = 0$ и известно, что оценки для параметров $\tilde{\beta} = \left( \sum_{t=1}^n y_t \right)/\left( \sum_{t=1}^n x_t \right)$ являются наилучшими (в смысле минимума дисперсии) среди линейных несмещенных оценок параметра $\beta$. Чему равна в этом случае матрица ковариаций вектора $\e$ с точностью до пропорциональности?
\end{problem}


\begin{solution}
Известно, что оценки параметров, получаемые по обобщённому методу наименьших квадратов, являются наилучшими, поэтому:
$\delta^2 \begin{bmatrix}
x_1 & 0\ldots0 & 0 \\
0 &  & 0 \\
  & \ldots & \\
0 &  & 0 \\
0 & 0\ldots0 & x_n \\
\end{bmatrix}$
\end{solution}

\begin{problem}
Для регрессии $y = X\beta + \e$ с $\E(\e) = 0$, $\Var(\e) = \Sigma \neq \sigma^2 I$, оцененной с помощью обобщённого метода наименьших квадратов, найдите ковариационную матрицу $\Cov(\hat{\beta}_{GLS}, \e)$
\end{problem}


\begin{solution}
\begin{multline}
\Cov(\hat{\beta}_{GLS}, \e) = \Cov \left( (X^T \Sigma^{-1} X)^{-1} X^T \Sigma^{-1} y, \e \right) = \\
\Cov \left( (X^T \Sigma^{-1} X)^{-1} X^T \Sigma^{-1} \e, \e \right) = \\ = (X^T \Sigma^{-1} X)^{-1} X^T \Sigma^{-1} \Cov(\e, \e) =\\
 (X^T \Sigma^{-1} X)^{-1} X^T \Sigma^{-1} \Sigma = (X^T \Sigma^{-1} X)^{-1} X^T
\end{multline}
\end{solution}


\begin{problem}
Найдите наиболее эффективную оценку коэффициента $\beta_1$ для модели $y_i = \beta_1 + \e$, $\E(\e_i) = 0$, $\E(\e_i\e_j) = 0$, $\Var(\e_i) = \sigma_{\e}^2 / x_i$, $x_i > 0$ в классе линейных несмещенных оценок
\end{problem}

\begin{solution}
Для нахождения эффективной оценки воспользуемся взвешенным методом наименьших квадратов. Разделим каждое из уравнений $y_i = \beta_1 + \e$ на корень из дисперсии $\e_i$ с тем, чтобы ошибки в полученных уравнениях имели равные дисперсии (в этом случае можно будет сослаться на т. Гаусса-Маркова). Итак, после деления i-го уравнения на величину $\sqrt{x_i}/\sigma_{\e}$, мы получаем:
$$\begin{bmatrix}
y_1 \sqrt{x_1}/\sigma_{\e} \\
y_2 \sqrt{x_2}/\sigma_{\e} \\
\ldots \\
y_n \sqrt{x_n}/\sigma_{\e} \\
\end{bmatrix} = \beta_1 \begin{bmatrix}
\sqrt{x_1}/\sigma_{\e} \\
\sqrt{x_2}/\sigma_{\e} \\
\ldots \\
\sqrt{x_n}/\sigma_{\e} \\
\end{bmatrix} + \begin{bmatrix}
\e_1 \sqrt{x_1}/\sigma_{\e} \\
\e_2 \sqrt{x_2}/\sigma_{\e} \\
\ldots \\
\e_n \sqrt{x_n}/\sigma_{\e} \\
\end{bmatrix}$$
Поскольку условия т. Гаусса-Маркова для последней модели выполнены, то МНК-оценка для последней модели будет наиболее эффективной. Поэтому
$$\hat{\beta_1} = \frac{\sum_{i=1}^n (y_i \sqrt{x_i}/\sigma_{\e})(\sqrt{x_i}/\sigma_{\e})}{\sum_{i=1}^n (\sqrt{x_1}/\sigma_{\e})} = \frac{\sum_{i=1}^n y_i x_i}{\sum_{i=1}^n x_i^2}$$
\end{solution}


\begin{problem}
Фурманов Кирилл. Исследователь оценил регрессионную модель $y_i = \beta_1 + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \e_i$ и провёл диагностику различных проблемных явлений. Результаты его стараний приведены ниже:

%\todo[inline]{Две картинки с графиками остатков}


Также $VIF_2 = 1.06$, $VIF_3 = 1.07$, $VIF_4 = 1.02$
\begin{enumerate}
\item[(a)] Определите, какие проблемные явления обнаружил исследователь. Обоснуйте свой ответ.
\item[(b)] Найдите коэффициент детерминации для регрессии: $x_{i2} = \gamma_1 + \gamma_2 x_{i3} + \gamma_3 x_{i4} + u_i$
\end{enumerate}
\end{problem}


\begin{solution}
\end{solution}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Новые задачи

\begin{problem}
В модели $y_i=\beta x_i+\e_i$ предполагается гетероскедастичность вида $\Var(\e_i)=\exp(\gamma_1+\gamma_2 x_i)$.
\begin{enumerate}
\item Сформулируйте гипотезу о гомоскедастичности с помощью коэффициентов
\item Выведите в явном виде оценку максимального правдоподобия при предположении о гомоскедастичности
\item Выпишите условия первого порядка для оценки максимального правдоподобия без предположения о гомоскедастичности
\item Выведите в явном виде формулу для LM теста множителей Лагранжа
\end{enumerate}
\end{problem}

\begin{solution}
В предположении о гомоскедастичности, $\gamma_2=0$, оценка правдоподобия совпадает с МНК-оценкой, значит $\hb=\sum y_i x_i/ \sum x_i^2$. И $\hs^2_i=RSS/n$, значит $\hat{\gamma_1}=\ln(RSS/n)$.
\end{solution}


\begin{problem}
Рассмотрим модель $y_i=\beta_1 + \beta_2 x_i + \e_i$, где $y=(1,3,3)'$ и $x=(0,0,1)$.
\begin{enumerate}
\item Найдите $\hb$ и $\hVar(\hb)$ в предположении гомоскедастичности
\item Найдите оценку Уайта ковариационной матрицы $\hVar_{White}(\hb)$
\item Найдите оценку HC3 ковариационной матрицы $\hVar_{HC3}(\hb)$
\item Найдите оценку взвешенного МНК, $\hb_{WLS}$, и оценку её ковариационной матрицы, $\hVar(\hb_{WLS})$, в предположении, что $\Var(\e_i|X)=\sigma^2 \cdot ( 4 + 5x_i)$
\end{enumerate}
\end{problem}

\begin{solution}
Решение средствами пакета \verb|sandwich|
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{df} \hlkwb{<-} \hlkwd{data.frame}\hlstd{(}\hlkwc{y}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{3}\hlstd{,}\hlnum{3}\hlstd{),} \hlkwc{x}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{1}\hlstd{))}

\hlstd{model} \hlkwb{<-} \hlkwd{lm}\hlstd{(}\hlkwc{data}\hlstd{=df, y}\hlopt{~}\hlstd{x)}
\hlkwd{coef}\hlstd{(model)}
\end{alltt}
\begin{verbatim}
## (Intercept)           x 
##           2           1
\end{verbatim}
\begin{alltt}
\hlcom{# residuals}
\hlkwd{resid}\hlstd{(model)}
\end{alltt}
\begin{verbatim}
##             1             2             3 
## -1.000000e+00  1.000000e+00  2.220446e-16
\end{verbatim}
\begin{alltt}
\hlkwd{vcov}\hlstd{(model)}
\end{alltt}
\begin{verbatim}
##             (Intercept)  x
## (Intercept)           1 -1
## x                    -1  3
\end{verbatim}
\begin{alltt}
\hlkwd{vcovHC}\hlstd{(model)} \hlcom{# should fail}
\end{alltt}
\begin{verbatim}
##             (Intercept)   x
## (Intercept)         NaN NaN
## x                   NaN NaN
\end{verbatim}
\begin{alltt}
\hlkwd{vcovHC}\hlstd{(model,} \hlkwc{type} \hlstd{=}\hlstr{"HC0"} \hlstd{)}
\end{alltt}
\begin{verbatim}
##             (Intercept)    x
## (Intercept)         0.5 -0.5
## x                  -0.5  0.5
\end{verbatim}
\begin{alltt}
\hlcom{# help(vcovHC)}
\end{alltt}
\end{kframe}
\end{knitrout}

Решение с ручным подсчётом матриц
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{y} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{3}\hlstd{,}\hlnum{3}\hlstd{)}
\hlstd{X} \hlkwb{<-} \hlkwd{cbind}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{3}\hlstd{),} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{0}\hlstd{,}\hlnum{1}\hlstd{))}

\hlstd{hat_beta} \hlkwb{<-} \hlkwd{solve}\hlstd{(}\hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{X)} \hlopt{%*%} \hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{y}
\hlstd{hat_beta}
\end{alltt}
\begin{verbatim}
##      [,1]
## [1,]    2
## [2,]    1
\end{verbatim}
\begin{alltt}
\hlcom{# by hand Var(hat_beta)}
\hlstd{y_hat} \hlkwb{<-} \hlstd{X} \hlopt{%*%} \hlstd{hat_beta}
\hlstd{e_hat} \hlkwb{<-} \hlstd{y}\hlopt{-}\hlstd{y_hat}
\hlstd{RSS} \hlkwb{<-} \hlkwd{sum}\hlstd{((y}\hlopt{-}\hlstd{y_hat)}\hlopt{^}\hlnum{2}\hlstd{)}
\hlstd{vcov_ols} \hlkwb{<-} \hlstd{RSS}\hlopt{/}\hlstd{(}\hlnum{3}\hlopt{-}\hlnum{2}\hlstd{)}\hlopt{*}\hlkwd{solve}\hlstd{(}\hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{X)}
\hlstd{vcov_ols}
\end{alltt}
\begin{verbatim}
##      [,1] [,2]
## [1,]    1   -1
## [2,]   -1    3
\end{verbatim}
\begin{alltt}
\hlcom{# crossprod(X) is just synonym fort(X) %*% X}

\hlstd{H} \hlkwb{<-} \hlstd{X} \hlopt{%*%} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(X))} \hlopt{%*%} \hlkwd{t}\hlstd{(X)}
\hlstd{H}
\end{alltt}
\begin{verbatim}
##              [,1]         [,2]         [,3]
## [1,] 5.000000e-01 5.000000e-01 1.110223e-16
## [2,] 5.000000e-01 5.000000e-01 1.110223e-16
## [3,] 1.110223e-16 1.110223e-16 1.000000e+00
\end{verbatim}
\begin{alltt}
\hlkwd{diag}\hlstd{(H)}
\end{alltt}
\begin{verbatim}
## [1] 0.5 0.5 1.0
\end{verbatim}
\begin{alltt}
\hlstd{S_hat_white} \hlkwb{<-} \hlkwd{diag}\hlstd{(}\hlkwd{as.vector}\hlstd{(e_hat}\hlopt{^}\hlnum{2}\hlstd{))}
\hlstd{S_hat_HC3} \hlkwb{<-} \hlkwd{diag}\hlstd{(}\hlkwd{as.vector}\hlstd{(e_hat}\hlopt{^}\hlnum{2}\hlstd{)}\hlopt{/}\hlstd{(}\hlnum{1}\hlopt{-}\hlkwd{diag}\hlstd{(H))}\hlopt{^}\hlnum{2}\hlstd{)}
\hlstd{S_hat_HC3} \hlcom{# look at the problem}
\end{alltt}
\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    4    0    0
## [2,]    0    4    0
## [3,]    0    0  Inf
\end{verbatim}
\begin{alltt}
\hlcom{# vcov White}
\hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(X))} \hlopt{%*%} \hlkwd{t}\hlstd{(X)}  \hlopt{%*%} \hlstd{S_hat_white} \hlopt{%*%} \hlstd{X} \hlopt{%*%} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(X))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2]
## [1,]  0.5 -0.5
## [2,] -0.5  0.5
\end{verbatim}
\begin{alltt}
\hlcom{# vcov HC3}
\hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(X))} \hlopt{%*%} \hlkwd{t}\hlstd{(X)}  \hlopt{%*%} \hlstd{S_hat_HC3} \hlopt{%*%} \hlstd{X} \hlopt{%*%} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(X))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2]
## [1,]  NaN  NaN
## [2,]  NaN  NaN
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{solution}


\begin{problem}
Имеются наблюдения

```{r}
x <- c(0,2,2)
y <- c(-1,1,0)
df <- data.frame(x=x,y=y)
pander(df)
```

Предположим, что $y_i=\beta_1 + \beta_2 x_i + \e_i$ и регрессоры неслучайные.

Для удобства приведены матрицы

\[
X'X=
```{r, results='asis'}
X <- model.matrix(y~x, data=df)
XX <- t(X) %*% X
XX_inv <- solve(XX)
XX_X <- XX_inv %*% t(X)
H <- X %*% XX_X
xmatrix(XX)
```
, \;
(X'X)^{-1}=
```{r, results='asis'}
xmatrix(XX_inv)
```
, \;
(X'X)^{-1}X'=
```{r, results='asis'}
xmatrix(XX_X)
```
\]
А также:
\[
H=X(X'X)^{-1}X'=
```{r, results='asis'}
xmatrix(H)
```
, \;
y'y=
```{r, results='asis'}
xmatrix(t(y) %*% y)
```
, \;
X'y=
```{r, results='asis'}
xmatrix(t(X) %*% y)
```
\]

\begin{enumerate}
\item Найдите оценки коэффициентов с помощью МНК и оценку их ковариационной матрицы предполагая независимость и гомоскедастичность ошибок.

\item Найдите две робастных к гетероскедастичности оценки ковариационной матрицы оценок МНК: в форме Уайта и в форме HC3.

\item  Предположим, что дисперсии первых двух наблюдений равны, а дисперсия третьего наблюдения в 4 раза больше. Найдите оценки взвешенного МНК и оценку их ковариационной матрицы.

\item  Предположим, что дисперсии первых двух наблюдений равны, а дисперсия третьего наблюдения в 4 раза больше. Также предположим, что $Corr(\e_2, \e_3)=0.5$, а остальные корреляции между ошибками равны 0. Найдите оценки обобщенного МНК и оценку их ковариационной матрицы.

\item Аккуратно объясните, с какой целью используются робастные оценки ковариационной матрицы (например, оценка Уайта). Ответ <<для борьбы с гетероскедастичностью>> не оценивается. Как конкретно и при каких условиях можно использовать робастные оценки ковариационной матрицы?

\item Аккуратно объясните, с какой целью вместо МНК используется обобщенный МНК. Ответ <<для борьбы с гетероскедастичностью>> не оценивается. Что конкретно даёт обобщенный МНК, чего не даёт обычный МНК и при каких условиях?
\end{enumerate}

\end{problem}

\begin{solution}

\end{solution}


\begin{problem}
Предположим, что $y_i$ независимы, нормально распределены и имеют одинаковое математическое ожидание $\mu$.

\begin{enumerate}
\item  Предложите эффективную оценку для $\mu$, предполагая, что $y_i$ гомоскедастичны
\item  Предложите эффективную оценку для $\mu$, предполагая, что $Var(y_i)=1/i^2$
\end{enumerate}
\end{problem}

\begin{solution}
при гомоскедастичности $\hat{\mu}=\bar{y}$, при гетероскедастичности
\[
\hat{\mu}=\frac{\sum x_i y_i}{\sum x_i^2}=\frac{\sum i\cdot y_i}{\sum i^2}
\]
\end{solution}


\chapter{Ошибки спецификации}


\begin{problem}
По $25$ наблюдениям при помощи метода наименьших квадратов оценена
модель $\hat{y}=\hb_1+\hb_2 x +\hb_3 z$, для которой $RSS = 73$. При помощи вспомогательной регрессии $\hat{\hat{y}}=\hat{\gamma}_1+\hat{\gamma}_2 x +\hat{\gamma}_3 z+\hat{\gamma}_4 \hy^2$, для которой $RSS = 70$, выполните тест Рамсея на уровне значимости 5\%. 
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
По $20$ наблюдениям при помощи метода наименьших квадратов оценена
модель $\hat{y}=\hb_1+\hb_2 x +\hb_3 z$, для которой $R^2 = 0.7$. При помощи вспомогательной регрессии $\hat{\hat{y}}=\hat{\gamma}_1+\hat{\gamma}_2 x +\hat{\gamma}_3 z+\hat{\gamma}_4 \hy^2$, для которой $R^2 = 0.75$, выполните тест Рамсея на уровне значимости 5\%. 
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
По $30$ наблюдениям при помощи метода наименьших квадратов оценена
модель $\hat{y}=\hb_1+\hb_2 x +\hb_3 z$, для которой $RSS = 150$. При помощи вспомогательной регрессии $\hat{\hat{y}}=\hat{\gamma}_1+\hat{\gamma}_2 x +\hat{\gamma}_3 z+\hat{\gamma}_4 \hy^2+\hat{\gamma}_5 \hy^3$, для которой $RSS = 120$, выполните тест Рамсея на уровне значимости 5\%. 
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
По $35$ наблюдениям при помощи метода наименьших квадратов оценена
модель $\hat{y}=\hb_1+\hb_2 x +\hb_3 z$, для которой $R^2 = 0.7$. При помощи вспомогательной регрессии $\hat{\hat{y}}=\hat{\gamma}_1+\hat{\gamma}_2 x +\hat{\gamma}_3 z+\hat{\gamma}_4 \hy^2+\hat{\gamma}_5 \hy^3$, для которой $R^2 = 0.8$, выполните тест Рамсея на уровне значимости 5\%. 
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Используя 80 наблюдений, исследователь оценил две конкурирующие модели: $\hat{y}=\hb_1+\hb_2 x +\hb_3 z$, в которой $RSS_1=36875$ и $\widehat{\ln y}=\hb_1+\hb_2 x +\hb_3 z$, в которой $RSS_2=122$. 

Выполнив преобразование $y^*_i=y_i/\sqrt[n]{\prod y_i}$, исследователь также оценил две вспомогательные регрессии: $\hat{y}^*=\hb_1+\hb_2 x +\hb_3 z$, в которой $RSS^*_1=239$ и $\widehat{\ln y^*}=\hb_1+\hb_2 x +\hb_3 z$, в которой $RSS^*_2=121$.

Завершите тест Бокса-Кокса на уровне значимости 5\%.
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Используя 40 наблюдений, исследователь оценил две конкурирующие модели: $\hat{y}=\hb_1+\hb_2 x +\hb_3 z$, в которой $RSS_1=250$ и $\widehat{\ln y}=\hb_1+\hb_2 x +\hb_3 z$, в которой $RSS_2=12$. 

Выполнив преобразование $y^*_i=y_i/\sqrt[n]{\prod y_i}$, исследователь также оценил две вспомогательные регрессии: $\hat{y}^*=\hb_1+\hb_2 x +\hb_3 z$, в которой $RSS^*_1=20$ и $\widehat{\ln y^*}=\hb_1+\hb_2 x +\hb_3 z$, в которой $RSS^*_2=25$.

Завершите тест Бокса-Кокса на уровне значимости 5\%.
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Почему при реализации теста Бокса-Кокса на компьютере предпочтительнее использовать формулу $y^*_i=\exp(\ln y_i - \sum \ln y_i /n) $, а не формулу $y^*_i=y_i/\sqrt[n]{\prod y_i}$?
\end{problem}

\begin{solution}
чтобы избежать переполнения при подсчете произведения всех $y_i$
\end{solution}


\begin{problem}
Обследовав выборку из 27 домохозяйств, исследователь оценил уравнение регрессии:
\[
\frac{\widehat{Exp}_i}{Size_i}=926+235\frac{1}{Size_i}+0.3\frac{Income_i}{Size_i}
\]
где $Exp_i$ --- месячные затраты $i$-го домохозяйства на питание в рублях, $Income_i$ --- месячный доход домохозяйства (также в рублях),  $Size_i$ --- число членов домохозяйства. Известен коэффициент детерминации, $R^2=0.3$.

\begin{enumerate}
\item Каково, согласно оценённой модели, ожидаемое различие в затратах на питание между двумя домохозяйствами с одинаковым доходом, первое из которых больше второго на одного человека?
\item Известно, что нормировка переменных модели на размер семьи $Size_i$ была проведена с целью устранения гетероскедастичности в модели $Exp_i=\beta_1+\beta_2 Size_i+\beta_3 Income_i+\varepsilon_i$. Какое предположение сделал исследователь о виде гетероскедастичности?
\item Для проверки правильности выбранной спецификации было оценено ещё одно уравнение:
\[
\frac{\widehat{\widehat{Exp}}_i}{Size_i}=513+1499\frac{1}{Size_i}+0.5\frac{Income_i}{Size_i}-0.001\left(\frac{\widehat{Exp}_i}{Size_i}\right)^2
\]
Известно, что $R^2=0.4$. Даёт ли эта проверка основание считать модель исследователя неверно специфицированной? Используйте уровень значимости 1\%
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}






\begin{problem}
Мартовский Заяц и Безумный Шляпник почти всё время пьют чай. Известно, что количество выпитого за день чая (в чашках) зависит от количества пирожных (в штуках) и печенья (в штуках).
Алиса, гостившая у героев в течение 25 дней, заметила, что если оценить зависимость выпитого чая от закуски для Мартовского Зайца и Шляпника, то получится регрессия с $RSS=11.5$:
\[
\widehat{Tea}_i=6+0.5Biscuit_i+1.5Cake_i
\] 
Чтобы понять, удачную ли модель она построила,  Алиса оценила ещё одну регрессию с $RSS=9.5$: 
\[
\widehat{\widehat{Tea}}_i=12.7+0.65Biscuit_i-0.8Cake_i-0.59\widehat{Tea}^2_i+0.03\widehat{Tea}^3_i
\]
Помогите Алисе понять, верную ли спецификацию модели она выбрала
\begin{enumerate}
\item Проведите подходящий тест 
\item Сформулируйте основную и альтернативную гипотезы
\item Алиса решила проверить первоначальную короткую модель на наличие гетероскедастичности с помощью теста Уайта. Выпишите уравнение регрессии, которое она должна оценить.
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}



%


\chapter{Случайные регрессоры}

\begin{problemtext}

Экзогенность, $\E(\e\mid x)=0$

Предопределённость, $\E(\e_t\mid x_t)=0$ для всех $t$
\end{problemtext}


\begin{problem}
Настоящая зависимость между $y_i$ и $x_i$ имеет вид $y_i=\beta x_i +\e_i$, где $\e_i$ независимы и равновероятно принимают значения $-1$ и $+1$. Эконометресса Аврора оценивает коэффициент $\beta$ по двум наблюдениям. Аврора не знает чему равны настоящие $x_i$, вместо них она наблюдает значения $x^*_i=x_i+u_i$, где $u_i$ --- ошибки измерения, поэтому она строит регрессию $y_i$ на $x_i^*$. Ошибки измерения регрессора, $u_i$, независимы и равновероятно принимают значения $-1$ и $+1$. Величины $u_i$ и $\e_j$ независимы.
\begin{enumerate}
\item Выведите явную формулу для оценки $\hb$
\item Чему равно $u_i^2$? Чему равно $\e_i^2$?
\item Найдите $\E(\hb)$, если $x_1=0$ и $x_2=1$
\item Найдите $\E(\hb)$, если $x_1=0$ и $x_2=2$
\item Интуитивно объясните, как меняется смещение с ростом расстояния между $x_1$ и $x_2$
\end{enumerate}
\end{problem}

\begin{solution}
$u_i^2=\e_i^2=1$, $\E(\hb \mid x_1=0, \, x_2=1)=0.2\beta$, $\E(\hb \mid x_1=0, \, x_2=2)=0.8\beta$. Интуитивно объясняем: рисуем прямую по двум точкам. Мы знаем абсциссы точек с точностью $\pm 1$. Если точки близки, то это может сильно менять оценку наклона, если точки далеки, то случайность слабо влияет на наклон.
\end{solution}

\begin{problem}
Известна совместная функция плотности пары величин $X_i$, $Y_i$
\[
f(x,y)=
\]
Найдитe 
\begin{enumerate}
\item $\E(X_i)$, $\E(Y_i)$, $\Var(X_i)$, $\Var(Y_i)$, $\Cov(X_i,Y_i)$
\item $\E(Y_i \mid X_i)$, $\E(X_i \mid Y_i)$
\item Вася оценивает модель $y_i=\beta_1+\beta_2 x_i+\epsilon_i$ по огромному количеству наблюдений, $n>>0$. Чему примерно у него окажутся равны $\hb_1$, $\hb_2$, $\hs^2$, $\widehat{\Var}(\hb_2)$? Чему равно $\E(\hb_2)$? (или оно не будет браться???)
\item Петя оцениваем модель $y_i=\beta_1+\beta_2 x_i+\beta_2 x_i^2+\epsilon_i$. Найдите $\E(\hb_1)$, $\E(\hb_2)$, $\E(\hb_3)$, $\Var(\hb)$ (?)
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Табличка 2 на 2. Найдите $\E(\varepsilon)$, $\E(\varepsilon \mid x)$, $\Cov(\varepsilon,x)$.
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Приведите примеры дискретных случайных величин $\e$ и $x$, таких, что
\begin{enumerate}
\item $\E(\e)=0$, $\E(\e\mid x)=0$, но величины зависимы. Чему в этом случае равно $\Cov(\e,x)$?
\item $\E(\e)=0$, $\Cov(\e,x)=0$, но $\E(\e\mid x)\neq 0$. Зависимы ли эти случайные величины? 
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Эконометресса Агнесса хочет оценить модель $y_i=\beta_1+\beta_2 x_i +\e_i$, но, к сожалению, величина $x_i$ ненаблюдаема. Вместо неё доступна величина  $x_i^*$. Величина $x_i^*$  содержит ошибку измерения, $x_i^*=x_i+a_i$. Известно, что $\Var(x_i)=9$, $\Var(a_i)=4$,   $\Var(\e_i)=1$. Величины $x_i$, $a_i$ и $\e_i$ независимы.

Агнесса оценивает регрессию $\hy_i=\hb_1+\hb_2 x_i^*$ с помощью МНК. 
\begin{enumerate}
\item Найдите $\plim \hb_2$
\item Являются ли оценки, получаемые Агнессой, состоятельными?
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
ошибка измерения и коррелированная инструментальная ?
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Эконометресса Анжелла хочет оценить модель $y_i=\beta_1+\beta_2 x_i +\beta_3 w_i +\e_i$, но, к сожалению, величина $w_i$ ненаблюдаема. Известно, что $\Var(x_i)=9$, $\Var(w_i)=4$,  $\Var(\e_i)=1$ и $\Cov(x_i,w_i)=-2$. Случайная составляющая не коррелированна с регрессорами. 

За неимением $w_i$ Анжелла оценивает регрессию $\hy_i=\hb_1+\hb_2 x_i$ с помощью МНК. 
\begin{enumerate}
\item Найдите $\plim \hb_2$
\item Являются ли оценки, получаемые Агнессой, состоятельными?
\end{enumerate}

%Анжелла сумела найти переменную $z_i$, такую что $\Cov(z_i,\e_i)=0$ $\Cov(z_i,
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
Насяльника хочет оценить модель $y_i=\beta_1+\beta_2 x_i +\e_i$, к сожалению, величина $x_i$ ненаблюдаема. Вместо неё доступны  $x_i^a$, померянный Равшаном, и $x_i^b$, померянный Джумшутом. Естественно, $x_i^a$ и $x_i^b$ содержат ошибку измерения, $x_i^a=x_i+a_i$, $x_i^b=x_i+b_i$. Известно, что $\Var(x_i)=\sigma^2_x$, $\Var(a_i)=\sigma^2_a$,  $\Var(b_i)=\sigma^2_b$,  $\Var(\e_i)=\sigma^2_{\e}$. Величины $x_i$, $a_i$, $b_i$ и $\e_i$ независимы.

\begin{enumerate}
\item Найдите асимптотическое смещение оценки $\hb_2$ для следующих случаев:
\begin{enumerate}
\item С помощью МНК оценивается модель $\hy_i=\hb_1+\hb_2 x_i^a$
\item С помощью МНК оценивается модель $\hy_i=\hb_1+\hb_2 x_i^b$
\item С помощью МНК оценивается модель $\hy_i=\hb_1+\hb_2 \frac{x_i^a+x_i^b}{2}$
\item Методом инструментальных переменных оценивается модель $\hy_i=\hb_1+\hb_2 x_i^a$, в роли инструмента выступает $x_i^b$
\item Методом инструментальных переменных оценивается модель $\hy_i=\hb_1+\hb_2 x_i^b$, в роли инструмента выступает $x_i^a$
\end{enumerate}
\item Какой из методов даёт наибольшее смещение? Наименьшее смещение?
\item Какой из вариантов применения инструментальных переменных предпочтительнее с точки зрения дисперсии получаемой оценки $\hb_2$?
\end{enumerate}

\end{problem}


\begin{solution}
\end{solution}



% time series


\chapter{Временные ряды}


\begin{problem}
Что такое автокорреляция?
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
На графике представлены данные по уровню озера Гур\'{о}н в футах в 1875-1972 годах:  



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{ggplot}\hlstd{(df,}\hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=obs,}\hlkwc{y}\hlstd{=level))}\hlopt{+}\hlkwd{geom_line}\hlstd{()}\hlopt{+}
    \hlkwd{labs}\hlstd{(}\hlkwc{x}\hlstd{=}\hlstr{"Год"}\hlstd{,}\hlkwc{y}\hlstd{=}\hlstr{"Уровень озера (футы)"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-58-1} 

\end{knitrout}

График автокорреляционной и частной автокорреляционной функций:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{ggplot}\hlstd{(acfs.df,}\hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=lag,}\hlkwc{y}\hlstd{=acf,}\hlkwc{fill}\hlstd{=acf.type))}\hlopt{+}
    \hlkwd{geom_histogram}\hlstd{(}\hlkwc{position}\hlstd{=}\hlstr{"dodge"}\hlstd{,}\hlkwc{stat}\hlstd{=}\hlstr{"identity"}\hlstd{)}\hlopt{+}
  \hlkwd{xlab}\hlstd{(}\hlstr{"Лаг"}\hlstd{)}\hlopt{+}\hlkwd{ylab}\hlstd{(}\hlstr{"Корреляция"}\hlstd{)} \hlopt{+}
  \hlkwd{guides}\hlstd{(}\hlkwc{fill}\hlstd{=}\hlkwd{guide_legend}\hlstd{(}\hlkwc{title}\hlstd{=}\hlkwa{NULL}\hlstd{))}\hlopt{+}
  \hlkwd{geom_hline}\hlstd{(}\hlkwc{yintercept}\hlstd{=}\hlnum{1.96}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlkwd{nrow}\hlstd{(df)))}\hlopt{+}
  \hlkwd{geom_hline}\hlstd{(}\hlkwc{yintercept}\hlstd{=}\hlopt{-}\hlnum{1.96}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlkwd{nrow}\hlstd{(df)))}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-59-1} 

\end{knitrout}


\begin{enumerate}
\item Судя по графикам, какие модели класса ARMA или ARIMA имеет смысл оценить?
\item По результатам оценки некоей модели ARMA c двумя параметрами, исследователь посчитал оценки автокорреляционной функции для остатков модели. Известно, что для остатков модели первые три выборочные автокорреляции равны соответственно $0.0046707$, $\ensuremath{-0.0129386}$ и $\ensuremath{-0.0630104}$. С помощью подходящей статистики проверьте гипотезу о том, что первые три корреляции ошибок модели равны нулю.
\end{enumerate}
\end{problem}

\begin{solution}
\begin{enumerate}
\item Процесс $AR(2)$, т.к. две первые частные корреляции значимо отличаются от нуля, а гипотезы о том, что каждая последующая равна нулю не отвергаются.
\item Можно использовать одну из двух статистик
\[
\text{Ljung-Box}=n(n+2)\sum_{k=1}^3\frac{\hat{\rho}_k^2}{n-k}=
0.4288623
\]
\[
\text{Box-Pierce}=n\sum_{k=1}^3\hat{\rho}_k^2=
0.4076341
\]
Критическое значение хи-квадрат распределения с 3-мя степенями свободы для $\alpha=0.05$ равно $\chi^2_{3,crit}=7.8147279$.
Вывод: гипотеза $H_0$ об отсутствии корреляции ошибок модели не отвергается.
\end{enumerate}
\end{solution}





\begin{problem}
Винни-Пух пытается выявить закономерность в количестве придумываемых им каждый день ворчалок.  Винни-Пух решил разобраться, является ли оно стационарным процессом, для этого он оценил регрессию

\[ \Delta \hat{y}_t = \underset{(0.5)}{4.5} - \underset{(0.1)}{0.4}y_{t-1} +\underset{(0.5)}{0.7} \Delta y_{t-1} \]

Из-за опилок в голове Винни-Пух забыл, какой тест ему нужно провести, то ли Доктора Ватсона, то ли Дикого Фуллера. 

\begin{enumerate}
\item Аккуратно сформулируйте основную и альтернативную гипотезы
\item Проведите подходящий тест на уровне значимости 5\%
\item Сделайте вывод о стационарности ряда
\item Почему Сова не советовала Винни-Пуху пользоваться широко применяемым в Лесу $t$-распределением?
\end{enumerate}
\end{problem}

\begin{solution}



\begin{enumerate}
\item $H_0$: ряд содержит единичный корень, $\beta=0$; $H_0$: ряд не содержит единичного корня, $\beta<0$
\item $ADF=-0.4/0.1=-4$, $ADF_{crit}=\ensuremath{-2.89}$, $H_0$ отвергается
\item Ряд стационарен
\item При верной $H_0$ ряд не стационарен, и  $t$-статистика имеет не $t$-распределение, а распределение Дики-Фуллера.
\end{enumerate}
\end{solution}



\begin{problem}
Рассматривается модель $y_t=\beta_1+\beta_2 x_{t1}+\ldots+\beta_k x_{tk}+\e_t$. Ошибки $\e_t$ гомоскедастичны, но в них возможно присутствует автокорреляция первого порядка, $\e_t=\rho \e_{t-1}+u_t$. При известном числе наблюдений $T$ на уровне значимости 5\% сделайте статистический вывод о наличии автокорреляции.
\begin{enumerate}
\item $T=25$, $k=2$, $DW=0.8$
\item $T=30$, $k=3$, $DW=1.6$
\item $T=50$, $k=4$, $DW=1.8$
\item $T=100$, $k=5$, $DW=1.1$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
По 100 наблюдениям была оценена модель линейной регрессии
$y_t=\beta_1+\beta_2 x_t+\e_t$. Оказалось, что $RSS=120$, $\he_1=-1$, $\he_{100}=2$, $\sum_{t=2}^{100} \he_t\he_{t-1}=-50$. Найдите $DW$ и $\rho$.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Применяется ли статистика Дарбина-Уотсона для выявления автокорреляции в следующих моделях
\begin{enumerate}
\item $y_t=\beta_1 x_t + \e_t$
\item $y_t=\beta_1 + \beta_2 x_t + \e_t$
\item $y_t=\beta_1 + \beta_2 y_{t-1} + \e_t$
\item $y_t=\beta_1 + \beta_2 t +\beta_3 y_{t-1} + \e_t$
\item $y_t=\beta_1 t + \beta_2 x_t + \e_t$
\item $y_t=\beta_1 + \beta_2 t +\beta_3 x_t +\beta_4 x_{t-1} + \e_t$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
По 21 наблюдению была оценена модель линейной регрессии
$\underset{(se)}{\hat{y}}=\underset{(0.3)}{1.2}+\underset{(0.18)}{0.9}\cdot y_{t-1}+\underset{(0.01)}{0.1}\cdot t$, $R^2=0.6$, $DW=1.21$. Протестируйте гипотезу об отсутствии автокорреляции ошибок на уровне значимости 5\%.
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
По 24 наблюдениям была оценена модель линейной регрессии
$\underset{(se)}{\hat{y}}=\underset{(0.01)}{0.5}+\underset{(0.02)}{2}\cdot t$, $R^2=0.9$, $DW=1.3$. Протестируйте гипотезу об отсутствии автокорреляции ошибок на уровне значимости 5\%.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
По 32 наблюдениям была оценена модель линейной регрессии
$\underset{(se)}{\hat{y}}=\underset{(2.5)}{10}+\underset{(0.5)}{2.5}\cdot t- \underset{(0.01)}{0.1}\cdot t^2$, $R^2=0.75$, $DW=1.75$. Протестируйте гипотезу об отсутствии автокорреляции ошибок на уровне значимости 5\%.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Рассмотрим модель $y_t=\beta_1+\beta_2 x_{t1}+\ldots+\beta_k x_{tk}+\e_t$, где $\e_t$ подчиняются автокорреляционной схеме первого порядка, т.е.
\begin{enumerate}
\item $\e_t=\rho \e_{t-1}+u_t$, $-1<\rho<1$
\item $\Var(\e_t)=const$, $\E(\e_t)=const$
\item $\Var(u_t)=\sigma^2$, $\E(u_t)=0$
\item Величины $u_t$ независимы между собой
\item Величины $u_t$ и $\e_s$ независимы, если $t\geq s$
\end{enumerate}
Найдите:
\begin{enumerate}
\item $\E(\e_t)$, $\Var(\e_t)$
\item $\Cov(\e_t,\e_{t+h})$
\item $\Corr(\e_t,\e_{t+h})$
\end{enumerate}
\end{problem}

\begin{solution}
\begin{enumerate}
\item $\E(\e_t)=0$, $\Var(\e_t)=\sigma^2/(1-\rho^2)$
\item $\Cov(\e_t,\e_{t+h})=\rho^h\cdot \sigma^2/(1-\rho^2)$
\item $\Corr(\e_t,\e_{t+h})=\rho^h$
\end{enumerate}
\end{solution}



\begin{problem}
Ошибки в модели $y_t=\beta_1+\beta_2 x_{t}+\e_t$ являются автокоррелированными первого порядка, $\e_t=\rho \e_{t-1}+u_t$. Шаман-эконометрист Ойуун выполняет два камлания-преобразования. Поясните смысл камланий:
\begin{enumerate}
\item Камлание А, при $t\geq 2$, Ойуун преобразует уравнение к виду $y_t-\rho y_{t-1}=\beta_1(1-\rho)+ \beta_2(x_t-\rho x_{t-1})+\e_t-\rho \e_{t-1}$
\item Камлание Б, при $t=1$, Ойуун преобразует уравнение к виду $\sqrt{1-\rho^2}y_1=\sqrt{1-\rho^2}\beta_1+\sqrt{1-\rho^2}\beta_2 x_1+\sqrt{1-\rho^2}\e_1$.
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Пусть $y_{t}$ --- стационарный процесс. Верно ли, что стационарны: 
\begin{enumerate}
\item $z_{t}=2y_{t}$ 
\item $z_{t}=y_{t}+1$ 
\item $z_{t}=\Delta y_{t}$ 
\item $z_{t}=2y_{t}+3y_{t-1}$ 
\end{enumerate} 
\end{problem}

\begin{solution}
все линейные комбинации стационарны
\end{solution}





\begin{problem}
Известно, что временной ряд $y_{t}$ порожден стационарным процессом, задаваемым соотношением $y_{t}=1+0.5y_{t-1}+\varepsilon_{t}$. Имеется 1000 наблюдений. Вася построил регрессию $y_{t}$ на константу и $y_{t-1}$. Петя построил регрессию на константу и $y_{t+1}$. Какие примерно оценки коэффициентов они получат? 
\end{problem}

\begin{solution}
Они будут примерно одинаковы. Оценка наклона определяется автоковариационной функцией.
\end{solution}



%%%%%%%%%%%%%%%%%% ARCH-GARCH

\begin{problem}
Рассмотрим следующий AR(1)-ARCH(1) процесс, 
$y_{t}=1+0.5y_{t-1}+\varepsilon_{t}$, $\varepsilon_{t}=\nu_{t}\cdot \sigma_{t}$ \\
$\nu_{t}$ независимые $N(0;1)$ величины. \\
$\sigma^{2}_{t}=1+0.8\varepsilon^{2}_{t-1}$\\
Также известно, что $y_{100}=2$, $y_{99}=1.7$ 
\begin{enumerate}
\item Найдите $\E_{100}(\varepsilon^{2}_{101})$, $\E_{100}(\varepsilon^{2}_{102})$, $\E_{100}(\varepsilon^{2}_{103})$, $\E(\varepsilon^{2}_{t})$ 
\item $\Var(y_{t})$, $\Var(y_{t}|\mathcal{F}_{t-1})$ 
\item Постройте доверительный интервал для $y_{101}$:
\begin{enumerate}
\item проигнорировав условную гетероскедастичность 
\item учтя условную гетерескедастичность 
\end{enumerate} 
\end{enumerate} 
\end{problem}

\begin{solution}
\end{solution}


%%%%%%%%%%%%%%%%% Оператор лага

\begin{problem}
Пусть $x_{t}$, $t=0,1,2,...$ - случайный процесс и $y_{t}=(1+\L )^{t}x_{t}$.
Выразите $x_{t}$ с помощью $y_{t}$ и оператора лага $\L $.
\end{problem}

\begin{solution}
$x_{t}=(1-\L )^{t}y_{t}$
\end{solution}


\begin{problem}
Пусть $ F_{n} $ --- последовательность чисел Фибоначчи. Упростите величину
\[ F_{1}+C^{1}_{5}F_{2}+C^{2}_{5}F_{3}+C^{3}_{5}F_{4}+C^{4}_{5}F_{5}+C^{5}_{5}F_{6} \]
\end{problem}

\begin{solution}
$ F_{n}=\L (1+\L )F_{n} $, значит $ F_{n}=\L ^{k}(1+\L )^{k}F_{n} $ или $ F_{n+k}=(1+\L )^{k}F_{n} $
\end{solution}



\begin{problem}
Пусть $y_{t}$, $t=...-2,-1,0,1,2,...$ - случайный процесс. И $y_{t}=x_{-t}$. Являются ли верными рассуждения?
\begin{enumerate}
\item $\L y_{t}=\L x_{-t}=x_{-t-1}$
\item $\L y_{t}=y_{t-1}=x_{-t+1}$ 
\end{enumerate}
\end{problem}

\begin{solution}
а - неверно, б - верно. 
\end{solution}



%%%%%%%%%%%%%%%% состояние-наблюдение, фильтр Калмана


\begin{problem}
Представьте процесс AR(1),
$y_{t}=0.9y_{t-1}-0.2y_{t-2}+\varepsilon_{t}$,
$\varepsilon\sim$WN(0;1) в виде модели состояние-наблюдение. \\
а) Выбрав в качестве состояний вектор $\left(%
\begin{array}{c}
  y_{t} \\
  y_{t-1} \\
\end{array}%
\right)$ \\
б) Выбрав в качестве состояний вектор $\left(%
\begin{array}{c}
  y_{t} \\
  \hat{y}_{t,1} \\
\end{array}%
\right)$ \\
Найдите дисперсии ошибок состояний 
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Представьте процесс MA(1),
$y_{t}=\varepsilon_{t}+0.5\varepsilon_{t-1}$,
$\varepsilon\sim$WN(0;1) в виде модели состояние-наблюдение. \\
a) $\left(%
\begin{array}{c}
  \varepsilon_{t} \\
  \varepsilon_{t-1} \\
\end{array}%
\right)$ \\
b) $\left(%
\begin{array}{c}
  \varepsilon_{t}+0.5\varepsilon_{t-1} \\
  0.5\varepsilon_{t} 
\end{array}%
\right)$ 
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Представьте процесс ARMA(1,1),
$y_{t}=0.5y_{t-1}+\varepsilon_{t}+\varepsilon_{t-1}$,
$\varepsilon\sim$WN(0;1) в
виде модели состояние-наблюдение. \\
Вектор состояний имеет вид $x_{t},x_{t-1}$, где
$x_{t}=\frac{1}{1-0.5L}\varepsilon_{t}$ 
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Рекурсивные коэффициенты 
\begin{enumerate}
\item Oцените модель вида $y_{t}=a+b_{t}x_{t}+\varepsilon_{t}$,
где $b_{t}=b_{t-1}$. 
\item Сравните графики filtered state и smoothed state. 
\item Сравните финальное состояние $b_{T}$ с коэффициентом в
обычной модели линейной регрессии, $y_{t}=a+bx_{t}+\varepsilon_{t}$. 
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Пусть $u_t$ --- независимые нормальные случайные величины с 
математическим ожиданием $0$ и дисперсией $\sigma^2$. Известно, что $\e_1=u_1$, $\e_t=u_1+u_2+\ldots+u_t$. Рассмотрим модель $y_t=\beta_1+\beta_2 x_t + \e_t$.

\begin{enumerate}
\item Найдите $\Var(\e_t)$, $\Cov(\e_t,\e_s)$, $\Var(\e)$
\item Являются ли ошибки $\e_t$ гетероскедастичными?
\item Являются ли ошибки $\e_t$ автокоррелированными?
\item Предложите более эффективную оценку вектора коэффициентов регрессии по сравнению МНК-оценкой.
\item Результаты предыдущего пункта подтвердите симуляциями Монте-Карло на компьютере.
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Найдите безусловную дисперсию GARCH-процессов 
\begin{enumerate}
\item $\e_t=\sigma_t \cdot \z_t$, $\sigma^2_t=0.1+0.8\sigma^2_{t-1}+0.1\e^2_{t-1}$
\item $\e_t=\sigma_t \cdot \z_t$, $\sigma^2_t=0.4+0.7\sigma^2_{t-1}+0.1\e^2_{t-1}$
\item $\e_t=\sigma_t \cdot \z_t$, $\sigma^2_t=0.2+0.8\sigma^2_{t-1}+0.1\e^2_{t-1}$
\end{enumerate}
\end{problem}

\begin{solution}
$1$, $2$, $2$ 
\end{solution}


\begin{problem}
Являются ли верными следующие утверждения?
\begin{enumerate}
\item GARCH-процесс является процессом белого шума, условная дисперсия которого
изменяется во времени
\item Модель GARCH(1,1) предназначена для прогнозирования меры изменчивости цены
финансового инструмента, а не для прогнозирования самой цены инструмента
\item При помощи GARCH-процесса можно устранять гетероскедастичность
\item Безусловная дисперсия GARCH-процесса изменяется во времени
\item Модель GARCH(1,1) может быть использована для прогнозирования
волатильности финансовых инструментов на несколько торговых недель вперёд     
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Рассмотрим GARCH-процесс $\e_t=\sigma_t \cdot \z_t$, $\sigma^2_t=k+g_1\sigma^2_{t-1}+a_1\e^2_{t-1}$. Найдите
\begin{enumerate}
\item $\E(\z_t)$, $\E(\z_t^2)$, $\E(\e_t)$, $\E(\e_t^2)$
\item $\Var(\z_t)$, $\Var(\e_t)$, $\Var(\e_t \mid \mathcal{F}_{t-1})$
\item $\E(\e_t \mid \mathcal{F}_{t-1})$, $\E(\e_t^2 \mid \mathcal{F}_{t-1})$, $\E(\sigma^2_t \mid \mathcal{F}_{t-1})$
\item $\E(\z_t\z_{t-1})$, $\E(\z_t^2\z_{t-1}^2)$, $\Cov(\e_t,\e_{t-1})$, $\Cov(\e_t^2,\e_{t-1}^2)$
\item $\lim_{h\to\infty} \E(\sigma^2_{t+h} \mid \mathcal{F}_t)$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Используя 500 наблюдений дневных логарифмических доходностей $y_t$ ,
была оценена GARCH(1,1)-модель: $\hy_t=-0.000708+\he_t$, $\e_t=\sigma_t \cdot \z_t$, $\sigma^2_t=0.000455+0.6424\sigma^2_{t-1}+0.2509\e^2_{t-1}$. Также известно, что $\hs^2_{499}=0.002568$, $\he^2_{499}=0.000014$, $\he^2_{500}=0.002178$.
Найдите 
\begin{enumerate}
\item  $\hs^2_{500}$, $\hs^2_{501}$, $\hs^2_{502}$
\item Волатильность в годовом выражении в процентах, соответствующую
наблюдению с номером $t = 500$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Докажите, что в условиях автокорреляции МНК-
оценки остаются несмещенными.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Продавец мороженного оценил динамическую модель объёмов продаж:
\[
\ln \hat{Q}_t=26.7 + 0.2\ln \hat{Q}_{t-1}-0.6\ln P_t
\]
Здесь $Q_t$ --- число проданных в день $t$ вафельных стаканчиков, а $P_t$ --- цена одного стаканчика в рублях. Продавец также рассчитал остатки $\hat{e}_t$.
\begin{enumerate}
\item Чему, согласно полученным оценкам, равна долгосрочная эластичность объёма продаж по цене?
\item Предположим, что продавец решил проверить наличие автокорреляции первого порядка с помощью теста Бройша-Годфри. Выпишите уравнение регрессии, которое он должен оценить.  
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Рассматривается модель $y_t = \mu + \varepsilon_t$, $t = 1,\ldots,T$, где $\varepsilon_t = \rho \varepsilon_{t-1} + u_t$, случайные величины $\varepsilon_0, u_1,\dots,u_T$ независимы, причем $\varepsilon_0 \sim N(0,\sigma^2/(1 - \rho^2))$, $u_t \sim N(0,\sigma^2)$. Имеются наблюдения $y' = (1, 2, 0, 0, 1)$.
\begin{enumerate}
  \item Выпишите функцию правдоподобия 
  \[
  \mathrm{L}(\mu, \rho, \sigma^2) = f_{Y_1}(y_1)\prod_{t=2}^{T}f_{Y_t|Y_{t-1}}(y_t|y_{t-1}).
  \]
  \item Найдите оценки неизвестных параметров модели максимизируя условную функцию правдоподобия 
  \[
  \mathrm{L}(\mu, \rho, \sigma^2|Y_1 = y_1) = \prod_{t=2}^{T}f_{Y_t|Y_{t-1}}(y_t|y_{t-1})
  \] 
\end{enumerate}
\end{problem}

\begin{solution}
1. Поскольку имеют место соотношения $\varepsilon_1 = \rho \varepsilon_0 + u_1$ и $Y_1 =\mu + \varepsilon_1$, то из условия задачи получаем, что $\varepsilon_1 \sim N(0,\sigma^2 / (1 - \rho^2))$
и $Y_1 \sim N(\mu,\sigma^2 / (1 - \rho^2))$. Поэтому
\[
f_{Y_1}(y_1) = \frac{1}{\sqrt{2\pi\sigma^2/(1-\rho^2)}}\exp{\left(-\frac{(y_1 - \mu)^2}{2\sigma^2/(1 - \rho^2)}\right)}.
\]

Далее, найдем $f_{Y_2|Y_1}(y_2|y_1)$. Учитывая, что $Y_2 = \rho Y_1 + (1- \rho) \mu + u_2$, получаем $Y_2|\{Y_1 = y_1\} \sim N(\rho y_1 + (1- \rho) \mu, \sigma^2)$. Значит,
\[
f_{Y_2|Y_1}(y_2|y_1) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{(y_2 - \rho y_1 - (1- \rho) \mu)^2}{2\sigma^2}\right)}.
\]

Действуя аналогично, получаем, что для всех $t \geq 2$ справедлива формула
\[
f_{Y_{t}|Y_{t-1}}(y_{t}|y_{t-1}) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp{\left(-\frac{(y_{t} - \rho y_{t-1} - (1- \rho) \mu)^2}{2\sigma^2}\right)}.
\]

Таким образом, находим функцию правдоподобия
\[
\mathrm{L}(\mu, \rho, \sigma^2) = f_{Y_T,\ldots,Y_1}(y_T,\dots,y_1) = f_{Y_1}(y_1)\prod_{t=2}^{T}f_{Y_t|Y_{t-1}}(y_t|y_{t-1}) \text{,}
\]
где $f_{Y_1}(y_1)$ и $f_{Y_t|Y_{t-1}}(y_t|y_{t-1})$ получены выше.

2. Для нахождения неизвестных параметров модели запишем логарифмическую условную функцию правдоподобия:
\[
l(\mu, \rho, \sigma^2|Y_1 = y_1) = \sum_{t=2}^{T}\log{f_{Y_t|Y_{t-1}}(y_t|y_{t-1})} = 
\]
\[
=-\frac{T-1}{2} \log(2 \pi) - \frac{T-1}{2} \log{\sigma^2} - \frac{1}{2\sigma^2} \sum_{t=2}^{T}(y_t - \rho y_{t-1} - (1 - \rho) \mu)^2 \text{.} 
\]

Найдем производные функции $l(\mu, \rho, \sigma^2|Y_1 = y_1)$ по неизвестным параметрам:
\[
\frac{\partial l}{\partial \mu} = -\frac{1}{2\sigma^2} \sum_{t=2}^{T} 2(y_t - \rho y_{t-1} - (1 - \rho) \mu) \cdot (\rho - 1) \text{,}
\]
\[
\frac{\partial l}{\partial \rho} = -\frac{1}{2\sigma^2} \sum_{t=2}^{T} 2(y_t - \rho y_{t-1} - (1 - \rho) \mu) \cdot (\mu - y_{t-1}) \text{,}
\]
\[
\frac{\partial l}{\partial {\sigma^2}} =  - \frac{T-1}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{t=2}^{T}(y_t - \rho y_{t-1} - (1 - \rho) \mu)^2 \text{.}
\]

Оценки неизвестных параметров модели могут быть получены как решение следующей системы уравнений:
\[
\left\{
  \begin{aligned}
    \frac{\partial l}{\partial \mu} = 0 \text{,} \\
    \frac{\partial l}{\partial \rho} = 0 \text{,} \\
    \frac{\partial l}{\partial {\sigma^2}} = 0 \text{.}
  \end{aligned}
\right.
\]

Из первого уравнения системы получаем, что
\[
\sum_{t=2}^{T}y_{t} - \hat{\rho} \sum_{t=2}^{T}y_{t-1} = (T - 1) (1- \hat{\rho}) \hat{\mu} \text{,}
\]
откуда
\[
\hat{\mu} = \frac{\sum_{t=2}^{T}y_{t} - \hat{\rho} \sum_{t=2}^{T}y_{t-1}}{(T - 1) (1- \hat{\rho})} = \frac{3 - \hat{\rho} \cdot 3}{4\cdot(1-\hat{\rho})} = \frac{3}{4} \text{.}
\]

Далее, если второе уравнение системы переписать в виде
\[
\sum_{t=2}^{T}(y_t - \hat{\mu} - \hat{\rho} (y_{t-1} - \hat{\mu}))(y_{t-1} - \hat{\mu}) = 0 \text{,}
\]
то легко видеть, что
\[
\hat{\rho} = \frac{\sum_{t=2}^{T}(y_t - \hat{\mu})(y_{t-1} - \hat{\mu})}{\sum_{t=2}^{T}(y_{t-1} - \hat{\mu})^2} \text{.}
\]
Следовательно, $\hat{\rho} =-1/11= -0.0909$.

Наконец, из третьего уравнения системы
\[
\hs^2 =\frac{1}{T-1} \sum_{t=2}^{T}(y_t - \hat{\rho} y_{t-1} - (1 - \hat{\rho}) \hat{\mu})^2 \text{.}
\]
Значит, $\hs^2 = 165/242= 0.6818$. Ответы: $\hat{\mu} = 3/4= 0.75$, $\hat{\rho} = -1/11=-0.0909$, $\hs^2 =165/242=0.6818$.
\end{solution}


\begin{problem}
Была оценена AR(2) модель
\[
\hat{y}_t=2.3+0.8 y_{t-1}-0.2 y_{t-2}
\]
Дополнительно известно, что $se(\hb_{y_{t-1}})=0.3$ и $\hat{\rho}_1=0.7$. Найдите $se(\hb_{y_{t-2}})$ и $\hCov(\hb_{y_{t-2}},\hb_{y_{t-1}})$.
\end{problem}

\begin{solution}
Рассмотрим модель без константы. Тогда ковариационная матрица коэффициентов пропорциональна матрице
\[
\begin{pmatrix}
1 & -\hat{\rho}_1 \\
-\hat{\rho}_1 & 1
\end{pmatrix}
\]
\end{solution}



\begin{problem}
Рассмотрите следующие два утверждения:
\begin{itemize}
  \item[(a)] GARCH-процесс является слабо стационарным процессом,
  \item[(b)] GARCH-процесс является процессом с изменяющейся во времени условной дисперсией.
\end{itemize}
Поясните смысл каждого из них. Объясните, почему между ними нет противоречия.
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
Предложите способ, при помощи которого из моделей GARCH(1,1) и GARCH(2,1) можно выбрать лучшую.
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Опишите тест, при помощи которого можно выявить необходимость использовать GARCH-модель.
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
Рассматривается GARCH(1,1)-процесс $\sigma_t^2 = 1 + 0.8 \cdot \sigma_{t-1}^2 + 0.1 \cdot \varepsilon_{t-1}^2$. Известно, что $\sigma_T^2 = 9$, $\varepsilon_T = -2$. Найдите
\begin{itemize}
  \item[(a)] $\mathbb{E}[\sigma_{T+1}^2|\mathcal{F}_T$],
  \item[(b)] $\mathbb{E}[\sigma_{T+2}^2|\mathcal{F}_T$],
  \item[(c)] $\mathbb{E}[\sigma_{T+3}^2|\mathcal{F}_T$].
\end{itemize}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Рассмотрите два ряда цен интересующих вас финансовых инструментов, действующих в одной отрасли. Примером могут выступать цены обыкновенных акций Сбербанка и ВТБ. По данным для выбранных инструментов, содержащим не менее 250 наблюдений (за одни и тот же промежуток времени), рассчитайте при помощи GARCH-модели историческую волатильность в годовом выражении в процентах.
\begin{itemize}
  \item[(a)] В одних координатных осях постройте графики полученных волатильностей.
  \item[(b)] На основании графика, построенного в пункте (a), сделайте качественный вывод относительно риска каждого финансового инструмента.
  \item[(c)] Для каждого из выбранных инструментов постройте прогноз волатильности (в годовом выражении в процентах) на три торговых дня вперед.
\end{itemize}
\end{problem}

\begin{solution}
\end{solution}

%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%% Здесь начинаются новые задачи
%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%


\begin{problem}
Имеются данные $y=(1,\, 2,\, 0,\,  0,\, 2,\, 1)$. Предполагая модель с автокоррелированной ошибкой, $y_t=\mu+\e_t$, где $\e_t=\rho \e_{t-1}+u_t$ с помощью трёх тестов проверьте гипотезы
$H_0$: $\rho=0$, 
$H_0$: $\mu=0$, 
$H_0$: $\begin{cases}
\rho=0 \\
\mu = 0 \\
\sigma^2=1 
\end{cases}$
\end{problem}


\begin{solution}

\end{solution}


\begin{problem}
Процесс $x_t$ --- это процесс $y_t$, наблюдаемый с ошибкой, т.е. $x_t=y_t+\nu_t$. Ошибки $\nu_t$ являются белым шумом и не коррелированы с $y_t$. 
\begin{enumerate}
\item Является ли процесс $x_t$ MA(1) процессом, если $y_t$ ---  MA(1) процесс? Если да, то как связаны их автокорреляционные функциии?
\item Является ли процесс $x_t$ стационарным AR(1) процессом, если $y_t$ ---  стационарный AR(1) процесс? Если да, то как связаны их автокорреляционные функциии?
\end{enumerate}
\end{problem}

\begin{solution}

\end{solution}

\begin{problem}
Пусть $\e_t$ --- белый шум. Рассмотрим процесс $y_t=2+0.5y_{t-1}+\e_t$ с различными начальными условиями, указанными ниже.

\begin{enumerate}
\item Найдите $\E(y_t)$, $\Var(y_t)$ и определите, является ли процесс  стационарным, если:
\begin{enumerate}
\item $y_1=0$
\item $y_1=4$
\item $y_1=4+\e_1$
\item $y_1=4+\frac{2}{\sqrt{3}}\e_1$
\end{enumerate}
\item Как точно следует понимать фразу <<процесс $y_t=2+0.5y_{t-1}+\e_t$ является стационарным>>?
\end{enumerate}

\end{problem}


\begin{solution}
Процесс стационарен только при $y_1=4+\frac{2}{\sqrt{3}}\e_1$. Фразу нужно понимать как <<у стохастического разностного уравнения $y_t=2+0.5y_{t-1}+\e_t$ есть стационарное решение>>.
\end{solution}


\begin{problem}
Рассмотрим модель $y_t=\beta x_t +\e_t$, где $\e_1=u_1$ и $\e_t=u_t+u_{t-1}$ при $t\geq 2$. Случайные величины $u_i$ независимы с $\E(u_i)=0$ и $\Var(u_i)=\sigma^2$. 
\begin{enumerate}
\item Найдите $\Var(\e_t)$
\item Являются ли ошибки $\e_t$ гетероскедастичными?
\item Найдите $\Cov(\e_i,\e_j)$
\item Являются ли ошибки $\e_t$ автокоррелированными?
\item Как выглядит матрица $\Var(\e)$?
\item Рассмотрим оценку
\[
\hb=\frac{\sum x_i y_i}{\sum x_i^2}
\]
Является ли она несмещенной для $\beta$? Является ли она эффективной в классе линейных по $y$ несмещенных оценок?
\item Если приведенная $\hb$ не является эффективной, то приведите формулу для эффективной оценки.
\end{enumerate}
\end{problem}


\begin{solution}
\begin{enumerate}
\item $\E(\e_t)=0$, $\Var(\e_1)=\sigma^2$, $\Var(\e_t)=2\sigma^2$ при $t\geq 2$.  Гетероскедастичная.
\item $\Cov(e_t,e_{t+1})=\sigma^2$. Автокоррелированная.
\item $\hb$ --- несмещенная, неэффективная
\item Более эффективной будет $\hb_{gls}=(X'V^{-1}X)^{-1}X'V^{-1}y$, где 
\[
X=\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{pmatrix}
\]

Матрица $V$ известна с точностью до константы $\sigma^2$, но в формуле для $\hb_{gls}$ неизвестная $\sigma^2$ сократится.

Другой способ построить эффективную оценку --- применить МНК к преобразованным наблюдениям, т.е. $\hb_{gls}=\frac{\sum x'_i y'_i}{\sum x_i^{\prime 2}}$, где $y'_1=y_1$, $x'_1=x_1$, $y'_t=y_t-y_{t-1}$, $x'_t=x_t-x_{t-1}$ при $t\geq 2$.
\end{enumerate}
\end{solution}


% Нетрадиционная медицина


\chapter{Метод опорных векторов}


\begin{problem}
Имеются три наблюдения $A$, $B$ и $C$:

\begin{tabular}{ccc}
 & $x$ & $y$ \\ 
\hline 
$A$ & 1 & -2 \\ 
$B$ & 2 & 1 \\ 
$C$ & 3 & 0 \\ 
\end{tabular} 

\begin{enumerate}
\item Найдите расстояние $AB$ и косинус угла $ABC$
\item Найдите расстояние $AB$ и косинус угла $ABC$ в расширенном пространстве с помощью гауссовского ядра с $\sigma=1$.
\item Найдите расстояние $AB$ и косинус угла $ABC$ в расширенном пространстве с помощью полиномиального ядра второй степени
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Переход из двумерного пространства в расширяющее задан функцией 
\[
f : (x_1,x_2) \to (1,x_1,x_2,3x_1 x2, 2x_1^2, 4x_2^2)
\]
Найдите соответствующую ядерную функцию
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Ядерная функция имеет вид 
\[
K(x,y)=x_1^2y_1^2+x_2^2y_2^2+2x_1x_2y_1y_2
\]
Как может выглядеть функция $f:\R^2\to\R^3$ переводящие исходные векторы в расширенное пространство?
\end{problem}

\begin{solution}
$f(x_1,x_2)=(x_1^2,x_2^2,\sqrt{2}x_1x_2)$
\end{solution}


\begin{problem}
На плоскости имеются точки двух цветов. Красные: $(1,1)$, $(1,-1)$ и синие: $(-1,1)$, $(-1,-1)$. 
\begin{enumerate}
\item Найдите разделяющую гиперплоскость методом опорных векторов при разных $C$. 
\item Укажите опорные вектора.
\end{enumerate}
 
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
На плоскости имеются точки двух цветов. Красные: $(1,1)$, $(1,-1)$ и синие: $(-1,1)$, $(-1,-1)$ и $(2,0)$. 
\begin{enumerate}
\item Найдите разделяющую гиперплоскость методом опорных векторов при разных $C$.
\item Укажите опорные вектора
\end{enumerate}
 
\end{problem}

\begin{solution}
\end{solution}

\chapter{Деревья и Random Forest}


\begin{problem}
Для случайных величин  $X$ и $Y$ найдите индекс Джини и энтропию


\begin{tabular}{ccc}
$X$ & $0$ & $1$ \\ 
\hline 
$\P()$ & $0.2$ & $0.8$ \\ 
\end{tabular},
\begin{tabular}{cccc}
$Y$ & $0$ & $1$ & $5$ \\ 
\hline 
$\P()$ & $0.2$ & $0.3$ & $0.5$ \\ 
\end{tabular} 
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Случайная величина $X$ принимает значение $1$ с вероятностью $p$ и значение $0$ с вероятностью $1-p$.
\begin{enumerate}
\item Постройте график зависимости индекса Джини и энтропии от $p$
\item При каком $p$ энтропия и индекс Джини будут максимальны?
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Кот Леопольд анкетировал 20 мышей по трём вопросам: $x$ --- <<Одобряете ли Вы непримиримую к котам позицию Белого и Серого?>>, $y$ --- <<Известно ли Вам куда пропала моя любимая кошка Мурка?>> и $z$ --- <<Известны ли Вам настоящие имена Белого и Серого?>> Результаты опроса в таблице:
\begin{kframe}
\begin{alltt}
\hlkwd{set.seed}\hlstd{(}\hlnum{1975}\hlstd{)}
\hlstd{x} \hlkwb{<-} \hlkwd{sample}\hlstd{(}\hlkwd{c}\hlstd{(}\hlstr{"yes"}\hlstd{,}\hlstr{"no"}\hlstd{),}\hlkwc{size}\hlstd{=}\hlnum{20}\hlstd{,}\hlkwc{rep}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\hlstd{y} \hlkwb{<-} \hlkwd{sample}\hlstd{(}\hlkwd{c}\hlstd{(}\hlstr{"yes"}\hlstd{,}\hlstr{"no"}\hlstd{),}\hlkwc{size}\hlstd{=}\hlnum{20}\hlstd{,}\hlkwc{rep}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\hlstd{z} \hlkwb{<-} \hlkwd{sample}\hlstd{(}\hlkwd{c}\hlstd{(}\hlstr{"yes"}\hlstd{,}\hlstr{"no"}\hlstd{),}\hlkwc{size}\hlstd{=}\hlnum{20}\hlstd{,}\hlkwc{rep}\hlstd{=}\hlnum{TRUE}\hlstd{)}
\hlkwd{library}\hlstd{(xtable)}
\hlkwd{xtable}\hlstd{(}\hlkwd{data.frame}\hlstd{(x,y,z))}
\end{alltt}
\end{kframe}% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:14 2015
\begin{table}[ht]
\centering
\begin{tabular}{rlll}
  \hline
 & x & y & z \\ 
  \hline
1 & no & no & yes \\ 
  2 & no & yes & yes \\ 
  3 & yes & yes & yes \\ 
  4 & yes & yes & no \\ 
  5 & no & no & no \\ 
  6 & no & yes & yes \\ 
  7 & no & no & yes \\ 
  8 & no & no & no \\ 
  9 & yes & no & yes \\ 
  10 & yes & no & yes \\ 
  11 & no & no & no \\ 
  12 & yes & yes & yes \\ 
  13 & no & yes & yes \\ 
  14 & no & yes & no \\ 
  15 & yes & no & no \\ 
  16 & yes & no & yes \\ 
  17 & no & no & no \\ 
  18 & no & yes & no \\ 
  19 & no & yes & no \\ 
  20 & yes & no & no \\ 
   \hline
\end{tabular}
\end{table}



\begin{enumerate}
\item Какой фактор нужно использовать при прогнозировании $y$, чтобы минимизировать энтропию?
\item Какой фактор нужно использовать при прогнозировании $y$, чтобы минимизировать индекс Джини?
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}







% байесовский подход



% Приложения


\chapter{Линейная алгебра}


\begin{problem}
Найдите каждую из следующих матриц в каждой из следующих степеней $\frac{1}{2}$, $\frac{1}{3}$, $-\frac{1}{2}$, $-\frac{1}{3}$, $-1$, $100$.
\begin{enumerate}
\item
\ensuremath{% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:14 2015
\begin{pmatrix}{}
    1 &   1 \\ 
    1 &   2 \\ 
  \end{pmatrix}
}
\item
\ensuremath{% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:14 2015
\begin{pmatrix}{}
    4 &   1 \\ 
    1 &   2 \\ 
  \end{pmatrix}
}
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Найдите ортогональную проекцию и ортогональную составляющую (перпендикуляр) вектора $u_1$ на линейное подпространство $L = \mathcal{L}(u_2)$, порождённое вектором $u_2$, если
\begin{enumerate}
\item $\begin{matrix} u_1 = (1 & 1 & 1 & 1), u_2 = (1 & 0 & 0 & 1) \end{matrix}$
\item $\begin{matrix} u_1 = (2 & 2 & 2 & 2), u_2 = (1 & 0 & 0 & 1) \end{matrix}$
\item $\begin{matrix} u_1 = (1 & 1 & 1 & 1), u_2 = (7 & 0 & 0 & 7) \end{matrix}$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Найдите обратные матрицы ко всем матрицам, представленным ниже.
\begin{enumerate}
\item
\ensuremath{% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:14 2015
\begin{pmatrix}{}
    1 &   1 &   0 \\ 
    0 &   1 &   1 \\ 
    0 &   0 &   1 \\ 
  \end{pmatrix}
}
\item
\ensuremath{% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:14 2015
\begin{pmatrix}{}
    1 &   0 &   0 \\ 
    1 &   1 &   0 \\ 
    0 &   1 &   1 \\ 
  \end{pmatrix}
}
\item
\ensuremath{% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:14 2015
\begin{pmatrix}{}
    0 &   0 &   1 \\ 
    1 &   0 &   0 \\ 
    0 &   1 &   0 \\ 
  \end{pmatrix}
}
\item
\ensuremath{% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:14 2015
\begin{pmatrix}{}
  0 & 0 & a \\ 
  1 & 0 & 0 \\ 
  0 & 1 & 0 \\ 
  \end{pmatrix}
}
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Найдите ранг следующих матриц в зависимости от значений параметра $\lambda$.

\begin{enumerate}
\item $\begin{pmatrix} \lambda & 1 & 1 \\ 1 & \lambda & 1 \\ 1 & 1 & \lambda \end{pmatrix}$
\item $\begin{pmatrix} 1-\lambda & 1-2\lambda \\ 1+\lambda & 1+3\lambda \end{pmatrix}$
\item $\begin{pmatrix} 1 & \lambda & -1 & 2 \\ 2 & -1 & \lambda & 5 \\ 1 & 10 & -6 & 1 \end{pmatrix}$
\item $\begin{pmatrix} \lambda & 1 & -1 & -1 \\ 1 & \lambda & -1 & -1 \\ 1 & 1 & -\lambda & -1
\\ 1 & 1 & -1 & -\lambda \end{pmatrix}$
\end{enumerate}
\end{problem}


\begin{solution}
\end{solution}



\begin{problem}
Пусть $i = (1,\dots,1)'$ --- вектор из $n$ единиц и $\pi=i(i'i)^{-1}i'$. Найдите:
\begin{enumerate}
\item $\tr(\pi)$ и $\rk(\pi)$
\item $\tr(I-\pi)$ и $\rk(I-\pi)$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Пусть $X$ --- матрица размера ${n \times k}$, где $n > k$, и пусть $\rk(X) = k$. Верно ли, что матрица $P = X(X'X)^{-1}X'$ симметрична и идемпотентна?
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Пусть $X$ --- матрица размера ${n \times k}$, где $n > k$, и пусть $\rk(X) = k$. Верно ли, что каждый столбец матрицы $P = X(X'X)^{-1}X'$ является собственным вектором матрицы $P$, отвечающим собственному значению 1?
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Пусть $X$ --- матрица размера ${n \times k}$, где $n > k$, пусть $\rk(X) = k$ и $P = X(X'X)^{-1}X'$. Верно ли, что каждый вектор-столбец $u$, такой что $X'u=0$, является собственным вектором матрицы $P$, отвечающим собственному значению 0?
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
  Верно ли, что для любых матриц $A$ размера $m\times n$ и матриц $B$ размера 
${n \times m}$ выполняется равенство $\tr(AB) = \tr(BA)$?
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Верно ли, что собственные значения симметричной и идемпотентной матрицы могут быть только нулями и единицами?
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Пусть $P$ --- матрица размера ${n \times n}$, $P'= P$, $P^2 = P$. Верно ли, что $\rk(P) = \tr(P)$?
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
  Верно ли, что для симметричной матрицы собственные векторы, отвечающие различным собственным значениям, ортогональны?
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Найдите собственные значения и собственные векторы матрицы $P = X(X'X)^{-1}X'$, если 

\begin{enumerate}

\item $X = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \end{pmatrix}$

\item $X = \begin{pmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{pmatrix}$

\item $X = \begin{pmatrix} 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 1 & 0  \\ 1 & 1 & 1 \end{pmatrix}$

\item $X = \begin{pmatrix} 1 & 0 & 0 & 0 \\ 1 & 1 & 0 & 0 \\ 1 & 1 & 1 & 0  \\ 1 & 1 & 1 & 1 \end{pmatrix}$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Приведите пример таких $A$ и $B$, что $\det(AB)\neq \det(BA)$.
\end{problem}

\begin{solution}
Например, $A=(1,2,3)$, $B=(1,0,1)'$
\end{solution}

\begin{problem}
Для матриц-проекторов $\pi=\v1(\v1'\v1)^{-1}\v1'$ и $P=X(X'X)^{-1}X'$ найдите $\tr(\pi)$, $\tr(P)$, $\tr(I-\pi)$, $\tr(I-P)$.
\end{problem}

\begin{solution}
$\tr(I)=n$, $\tr(\pi)=1$, $\tr(P)=k$ 
\end{solution}


\begin{problem}
Выпишите в явном виде матрицы $X'X$, $(X'X)^{-1}$ и $X'y$, если

$y=\left(
\begin{array}{c}
y_1 \\
y_2 \\
\vdots \\
y_n 
\end{array}\right)$ и
$X=\left(
\begin{array}{cc}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_n 
\end{array}\right)$ 
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Выпишите в явном виде матрицы $\pi$, $\pi y$, $\pi \e$, $I-\pi$, если $\pi=\v1(\v1'\v1)^{-1}\v1'$.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Формула Фробениуса. Матрицу $A$ размера $(n+m)\times (n+m)$ разрезали на 4 части: $A=\begin{pmatrix}
A_{11} & A_{12} \\
A_{21} & A_{22} 
\end{pmatrix}$. Кусок $A_{11}$ имеет размер $n\times n$ и обратим, кусок $A_{22}$ имеет размер $m\times m$. Известно, что $A$ --- обратима и $A^{-1}=B$. На аналогичные по размеру и расположению части разрезали матрицу $B=\begin{pmatrix}
B_{11} & B_{12} \\
B_{21} & B_{22} 
\end{pmatrix}$.
\begin{enumerate}
\item Каковы размеры кусков $A_{12}$ и $A_{21}$?
\item Чему равно $B_{22}(A_{22}-A_{21}A_{11}^{-1}A_{12})$?
\end{enumerate}
\end{problem}

\begin{solution}
$n\times m$, $m\times n$, $I$ 
\end{solution}



\begin{problem}
Спектральное разложение. Симметричная матрица $A$ размера $n\times n$ имеет $n$ собственных чисел $\lambda_1$, \ldots, $\lambda_n$ с собственными векторами $u_1$, \ldots, $u_n$. Докажите, что $A$ можно представить в виде $A=\sum \lambda_i u_i u_i'$.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Найдите определитель, след, собственные значения, собственные векторы и число
обусловленности матрицы $A$. Также найдите $A^{-1}$, $A^{-1/2}$ и $A^{1/2}$.
\begin{enumerate}
\item $A=\begin{pmatrix}
0.2 & 0 \\
0 & 0.1
\end{pmatrix}$ 
\item $A=\begin{pmatrix}
2 & 1 \\
1 & 2
\end{pmatrix}$ 

\item $A=\begin{pmatrix}
4 & 1 \\
1 & 4
\end{pmatrix}$ 

\item $A=\begin{pmatrix}
2 & 1 & 1 \\
1 & 2 & 1 \\
1 & 1 & 2
\end{pmatrix}$ 

\item $A=\begin{pmatrix}
3 & 2 & 1 \\
2 & 3 & 2 \\
1 & 2 & 3
\end{pmatrix}$ 

\item $A=\begin{pmatrix}
0 & 0 & 1 \\
0 & 1 & 0 \\
1 & 0 & 0
\end{pmatrix}$ 

\item $A=\begin{pmatrix}
4 & -1 & 1 \\
-1 & 4 & -1 \\
1 & -1 & 4
\end{pmatrix}$ 

\item $A=\begin{pmatrix}
6 & -2 & 2 \\
-2 & 5 & 0 \\
2 & 0 & 7
\end{pmatrix}$ 
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
В этом упражнении исследуется связь определителя, следа и собственных значений. Везде имеются ввиду действительные собственные значения с учетом кратности.
\begin{enumerate}
\item Приведите пример матрицы для которой след равен сумме собственных значений.  
\item Приведите пример матрицы для которой след не равен сумме собственных значений. 
\item Верно ли, что для симметричной матрицы след всегда равен сумме собственных значений? 
\item Приведите пример матрицы для которой определитель равен произведению собственных значений.  
\item Приведите пример матрицы для которой определитель не равен произведению собственных значений. 
\item Верно ли, что для симметричной матрицы определитель всегда равен произведению собственных значений? 
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}

\chapter{Случайные векторы}


\begin{problem}
Пусть $y=(y_1, y_2, y_3, y_4, y_5)'$ --- случайный вектор доходностей пяти ценных бумаг. Известно, что $\E(y')=(5, 10, 20, 30, 40)$, $\Var(y_1)=0$, $\Var(y_2)=10$, $\Var(y_3)=20$, $\Var(y_4)=40$, $\Var(y_5)=40$ и
\[
\Corr(y)=\begin{pmatrix}
0 & 0 & 0 & 0 & 0 \\
0 & 1 & 0.3 & -0.2 & 0.1 \\
0 & 0.3 & 1 & 0.3 & -0.2 \\
0 & -0.2 & 0.3& 1 & 0.3 \\
0 & 0.1 & -0.2 & 0.3 & 1 
\end{pmatrix}
\]
С помощью компьютера найдите ответы на вопросы:
\begin{enumerate}
\item Какая ценная бумага является безрисковой?
\item Найдите ковариационную матрицу $\Var(y)$
\item Найдите ожидаемую доходность и дисперсию доходности портфеля, доли ценных бумаг в котором равны соответственно:
\begin{enumerate}
\item $\alpha=(0.2, 0.2, 0.2, 0.2, 0.2)'$
\item $\alpha=(0.0, 0.1, 0.2, 0.3, 0.4)'$
\item $\alpha=(0.0, 0.4, 0.3, 0.2, 0.1)'$
\end{enumerate}
\item Составьте из данных бумаг пять некоррелированных портфелей 
\end{enumerate}
\end{problem}

\begin{solution}
\newpage
\end{solution}


\begin{problem}
Пусть $i = (1,\dots,1)'$ --- вектор из $n$ единиц, $\pi=i(i'i)^{-1}i'$ и $\e = (\e_1,\dots,\e_n)'\sim N (0,I)$.
\begin{enumerate}
\item Найдите $\E(\e'\pi\e)$, $\E(\e'(I-\pi)\e)$ и $\E(\e\e')$
\item Как распределены случайные величины $\e'\pi\e$ и $\e'(I-\pi)\e$?
\item Запишите выражения $\e'\pi\e$ и $\e'(I-\pi)\e$, используя знак суммы
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}

Пусть $X = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \end{pmatrix}$, $P = X(X'X)^{-1}X'$, случайные величины $\e_1, \e_2, \e_3, \e_4$ независимы и одинаково распределены $\sim N (0,1)$.
\begin{enumerate}
\item Найдите распределение случайной величины $\e'P\e$, где $\e = \begin{pmatrix} \e_1 & \e_2 & \e_3 & \e_4 \end{pmatrix}'$
\item Найдите $\E(\e'P\e)$
\item При помощи таблиц найдите такое число $q$, что $\P(\e'P\e > q)=0.1$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}

Пусть $X = \begin{pmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{pmatrix}$, $P = X(X'X)^{-1}X'$, случайные величины $\e_1, \e_2, \e_3, \e_4$ независимы и одинаково распределены $\sim N (0,1)$.
\begin{enumerate}
\item Найдите распределение случайной величины $\e'P\e$, где $\e = \begin{pmatrix} \e_1 & \e_2 & \e_3 & \e_4 \end{pmatrix}'$
\item Найдите $\E(\e'P\e)$
\item При помощи таблиц найдите такое число $q$, что $\mathbb{P}(\e'P\e > q)=0.1$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}

Пусть $X = \begin{pmatrix} 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 1 & 0  \\ 1 & 1 & 1 \end{pmatrix} $, $P = X(X'X)^{-1}X'$, случайные величины $\e_1, \e_2, \e_3, \e_4$ независимы и одинаково распределены $\sim N (0,1)$.
\begin{enumerate}
\item Найдите распределение случайной величины $\e'P\e$, где $\e = \begin{pmatrix} \e_1 & \e_2 & \e_3 & \e_4 \end{pmatrix}'$.
\item Найдите $\E(\e'P\e)$.
\item При помощи таблиц найдите такое число $q$, что $\mathbb{P}(\e'P\e > q)=0.1$. 
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
Пусть  $x = \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}$, $\E(x) = \begin{pmatrix} 1 \\ 2 \end{pmatrix}$, $\Var(x) = \begin{pmatrix} 2 & 1 \\ 1 & 2 \end{pmatrix}$. Найдите $\E(y)$, $\Var(y)$ и $\E(z)$, если
\begin{enumerate}
\item $y = x - \E(x)$
\item $y = \Var(x)x$
\item $y = \Var(x)(x - \E(x))$
\item $y = \Var(x)^{-1}(x - \E(x))$
\item $y = \Var(x)^{-1/2}(x - \E(x))$
\item $z = (x - \E(x))'\Var(x)(x - \E(x))$
\item $z = (x - \E(x))'\Var(x)^{-1}(x - \E(x))$
\item $z = x'\Var(x)x$
\item $z = x'\Var(x)^{-1}x$
\end{enumerate}

\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Пусть  $x = \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}$, $\E(x) = \begin{pmatrix} 1 \\ 4 \end{pmatrix}$, $\Var(x) = \begin{pmatrix} 4 & 1 \\ 1 & 4 \end{pmatrix}$. Найдите $\E(y)$, $\Var(y)$ и $\E(z)$, если
\begin{enumerate}
\item $y = x - \E(x)$
\item $y = \Var(x)x$
\item $y = \Var(x)(x - \E(x))$
\item $y = \Var(x)^{-1}(x - \E(x))$
\item $y = \Var(x)^{-1/2}(x - \E(x))$
\item $z = (x - \E(x))'\Var(x)(x - \E(x))$
\item $z = (x - \E(x))'\Var(x)^{-1}(x - \E(x))$
\item $z = x'\Var(x)x$
\item $z = x'\Var(x)^{-1}x$
\end{enumerate}

\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
Известно, что случайные величины $x_1$, $x_2$ и $x_3$ имеют следующие характеристики: 
\begin{enumerate}
\item $\E(x_1) = 5$, $\E(x_2) = 10$, $\E(x_3) = 8$
\item $\Var  (x_1) = 6$, $\Var  (x_2) = 14$, $\Var  (x_3) = 1$
\item $\Cov  (x_1, x_2) = 3$, $\Cov  (x_1, x_3) = 1$, $\Cov  (x_2, x_3) = 0$
\end{enumerate}
Пусть случайные величины $y_1$, $y_2$ и $y_3$, представляют собой линейные комбинации случайных величин $X_1$, $X_2$ и $X_3$:
$$y_1 = x_1 + 3x_2 - 2x_3$$
$$y_2 = 7x_1 - 4x_2 + x_3$$
$$y_3 = -2x_1 - x_2 + 4x_3$$

\begin{enumerate}
\item Выпишите математическое ожидание и ковариационную матрицу случайного вектора $x =  \begin{pmatrix}
x_1 & x_2 & x_3\\
\end{pmatrix} ^T$
\item Напишите матрицу $A$, которая позволяет перейти от случайного вектора $x =  \begin{pmatrix}
x_1 & x_2 & x_3\\
\end{pmatrix} ^T$ к случайному вектору $y =  \begin{pmatrix}
y_1 & y_2 & y_3\\
\end{pmatrix} ^T$
\item С помощью матрицы $A$ найдите математическое ожидание и ковариационную матрицу случайного вектора $y =  \begin{pmatrix}
y_1 & y_2 & y_3\\
\end{pmatrix} ^T$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}





\begin{problem}
Пусть $\xi_1, \xi_2, \xi_3$ --- случайные величины, такие что $\Var (\xi_1) = 2$, $\Var (\xi_2) = 3$, $\Var (\xi_3) = 4$, $\Cov (\xi_1, \xi_2) = 1$, $\Cov (\xi_1, \xi_3) = -1$, $\Cov (\xi_2, \xi_3) = 0$. Пусть $\xi =  \begin{pmatrix}
\xi_1 & \xi_2 & \xi_3 \\
\end{pmatrix} ^T$. Найдите $\Var (\xi)$ и $\Var (\xi_1 + \xi_2 + \xi_3)$.
\end{problem}

\begin{solution}
По определению ковариационной матрицы:

$\Var (\xi) =  \begin{pmatrix}
\Var (\xi_1) & \Cov (\xi_1, \xi_2) & \Cov (\xi_1, \xi_3) \\
\Cov (\xi_2, \xi_1) & \Var (\xi_2) & \Cov (\xi_2, \xi_3) \\
\Cov (\xi_3, \xi_1) & \Cov (\xi_3, \xi_2) & \Var (\xi_3) \\
\end{pmatrix}  =  \begin{pmatrix}
2 & 1 & -1 \\
1 & 3 & 0 \\
-1 & 0 & 4 \\
\end{pmatrix} $

\begin{multline}
\Var (\xi_1 + \xi_2 + \xi_3)  = \Var   \begin{pmatrix} 
 \begin{pmatrix}
1 & 1 & 1 \\
\end{pmatrix}  &  \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\xi_3 \\
\end{pmatrix} 
\end{pmatrix}  = \\  
\begin{pmatrix}
1 & 1 & 1 \\
\end{pmatrix}  \Var   \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\xi_3 \\
\end{pmatrix}   \begin{pmatrix}
1 \\
1 \\
1 \\
\end{pmatrix}  = \\
 \begin{pmatrix}
1 & 1 & 1 \\
\end{pmatrix}   \begin{pmatrix}
2 & 1 & -1 \\
1 & 3 & 0 \\
-1 & 0 & 4 \\
\end{pmatrix}   \begin{pmatrix}
1 \\
1 \\
1 \\
\end{pmatrix}  = 9
\end{multline}
\end{solution}




\begin{problem}
  Пусть $h =  \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix} $; $\E (h) =  \begin{pmatrix}
1\\
2\\
\end{pmatrix} $; $\Var (h) =  \begin{pmatrix}
2 & 1 \\
1 & 2 \\
\end{pmatrix} $; $z_1 =  \begin{pmatrix}
\eta_1 \\
\eta_2 \\
\end{pmatrix}  =  \begin{pmatrix}
0 & 0 \\
0 & 1 \\
\end{pmatrix}   \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix} $. Найдите $\E (z_1)$ и $\Var (z_1)$.
\end{problem}

\begin{solution}
\begin{multline}
\E (z_1) = \E   \begin{pmatrix}
 \begin{pmatrix}
0 & 0 \\
0 & 1 \\
\end{pmatrix}  &  \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix}  \\
\end{pmatrix}  =  \begin{pmatrix}
0 & 0 \\
0 & 1 \\
\end{pmatrix}  \E   \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix}  = \\
 \begin{pmatrix}
0 & 0 \\
0 & 1 \\
\end{pmatrix}   \begin{pmatrix}
1\\
2\\
\end{pmatrix}  =  \begin{pmatrix}
0\\
2\\
\end{pmatrix} 
\end{multline}

$\Var (z_1) = \Var   \begin{pmatrix}
 \begin{pmatrix}
0 & 0 \\
0 & 1 \\
\end{pmatrix}  &  \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix}  \\
\end{pmatrix}  =  \begin{pmatrix}
0 & 0 \\
0 & 1 \\
\end{pmatrix}  \Var   \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix}   \begin{pmatrix}
0 & 0 \\
0 & 1 \\
\end{pmatrix} ^T =  \begin{pmatrix}
0 & 0 \\
0 & 1 \\
\end{pmatrix}   \begin{pmatrix}
2 & 1 \\
1 & 2 \\
\end{pmatrix}   \begin{pmatrix}
0 & 0 \\
0 & 1 \\
\end{pmatrix} ^T = \\
\begin{pmatrix}
0 & 0 \\
0 & 1 \\
\end{pmatrix}   \begin{pmatrix}
0 & 1 \\
0 & 2 \\
\end{pmatrix}  =  \begin{pmatrix}
0 & 0 \\
0 & 2 \\
\end{pmatrix} $
\end{solution}








\begin{problem}
Пусть $h =  \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix} $; $\E (h) =  \begin{pmatrix}
1\\
2\\
\end{pmatrix} $; $\Var (h) =  \begin{pmatrix}
2 & 1 \\
1 & 2 \\
\end{pmatrix} $; $z_2 =  \begin{pmatrix}
\eta_1 \\
\eta_2 \\
\end{pmatrix}  =  \begin{pmatrix}
0 & 0 \\
0 & 1 \\
\end{pmatrix}   \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix}  +  \begin{pmatrix}
1\\
1\\
\end{pmatrix} $. Найдите $\E (z_2)$ и $\Var (z_2)$
\end{problem}

\begin{solution}
$\E (z_2) = \E   \begin{pmatrix}
 \begin{pmatrix}
0 & 0 \\
0 & 1 \\
\end{pmatrix}  &  \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix}  & + &  \begin{pmatrix}
1\\
1\\
\end{pmatrix}  \\
\end{pmatrix}  =  \begin{pmatrix}
0 & 0 \\
0 & 1 \\
\end{pmatrix}  \E   \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix}  +  \begin{pmatrix}
1\\
1\\
\end{pmatrix}  =  \begin{pmatrix}
0 & 0 \\
0 & 1 \\
\end{pmatrix}   \begin{pmatrix}
1\\
2\\
\end{pmatrix}  +  \begin{pmatrix}
1\\
1\\
\end{pmatrix}  =  \begin{pmatrix}
0\\
2\\
\end{pmatrix} +  \begin{pmatrix}
1\\
1\\
\end{pmatrix}  =  \begin{pmatrix}
1\\
3\\
\end{pmatrix} $

Поскольку $z_2 = z_1 +  \begin{pmatrix}
1\\
1\\
\end{pmatrix} $, где $z_1$ --- случайный вектор из предыдущей задачи, то $\Var (z_2) = \Var (z_1)$. Сдвиг случайного вектора на вектор-константу не меняет его ковариационную матрицу. 
\end{solution}







\begin{problem}
Пусть $h =  \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix} $; $\E (h) =  \begin{pmatrix}
1\\
2\\
\end{pmatrix} $; $\Var (h) =  \begin{pmatrix}
2 & 1 \\
1 & 2 \\
\end{pmatrix} $; $z_3 =  \begin{pmatrix}
\eta_1 \\
\eta_2 \\
\end{pmatrix}  =  \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix}  -  \begin{pmatrix}
\E \xi_1 \\
\E \xi_2 \\
\end{pmatrix} $. Найдите $\E (z_3)$ и $\Var (z_3)$
\end{problem}

\begin{solution}
\textit{В данном примере проиллюстрирована процедура центрирования случайного вектора --- процедура вычитания из случайного вектора его математического ожидания.}

$\E (z_3) = \E   \begin{pmatrix}
 \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix}  & - &  \begin{pmatrix}
\E  \xi_1 \\
\E  \xi_2 \\
\end{pmatrix}  \\
\end{pmatrix}  = \E   \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix}  - \E   \begin{pmatrix}
\E  \xi_1 \\
\E  \xi_2 \\
\end{pmatrix}  =  \begin{pmatrix}
\E  \xi_1 \\
\E  \xi_2 \\
\end{pmatrix}  -  \begin{pmatrix}
\E  \xi_1 \\
\E  \xi_2 \\
\end{pmatrix}  =  \begin{pmatrix}
0\\
0\\
\end{pmatrix} $

Заметим, что вектор $z_3$ отличается от вектора $z_1$ (из задачи 15) сдвигом на вектор-константу $ \begin{pmatrix}
\E  \xi_1 \\
\E  \xi_2 \\
\end{pmatrix} $, поэтому $\Var (z_3) = \Var (z_1)$.
\end{solution}






\begin{problem}
Пусть $r_1$, $r_2$ и $r_3$ --- годовые доходности трёх рисковых финансовых инструментов. Пусть $\alpha_1$, $\alpha_2$ и $\alpha_3$ --- доли, с которыми данные инструменты входят в портфель инвестора. Считаем, что $\sum_{i=1}^3 \alpha_i = 1$ и $\alpha_i \geqslant 0$ для всех $i=1,2,3$. Пусть $r =  \begin{pmatrix}
r_1 & r_2 & r_3\\
\end{pmatrix} ^T$, $\E (r) =  \begin{pmatrix}
a_1 & a_2 & a_3\\
\end{pmatrix} ^T$, $\Var (r) =  \begin{pmatrix}
c_{11} & c_{12} & c_{13} \\
c_{21} & c_{22} & c_{23} \\
c_{31} & c_{32} & c_{33} \\
\end{pmatrix} $. Параметры $\left\lbrace a_i \right\rbrace$ и $\left\lbrace c_i \right\rbrace$ известны.

\begin{enumerate}
\item Найдите годовую доходность портфеля П инвестора
\item Докажите, что дисперсия доходности портфеля П равна $\sum_{i=1}^3 \sum_{j=1}^3 \alpha_i c_{ij} \alpha_j$
\item Для случая $\alpha_1 = 0.1$, $\alpha_2 = 0.5$, $\alpha3 = 0.4$, $\E (r) =  \begin{pmatrix}
a_1 & a_2 & a_3\\
\end{pmatrix} ^T =  \begin{pmatrix}
0.10 & 0.06 & 0.05 \\
\end{pmatrix} ^T$, 
\[
\Var (r) =  \begin{pmatrix}
0.04 & 0 & -0.005\\
0 & 0.01 & 0\\
-0.005 & 0 & 0.0025\\
\end{pmatrix} 
\]
найдите $\E (\text{П})$ и $\Var (\text{П})$
\end{enumerate}
\end{problem}


\begin{solution}
\end{solution}


\begin{problem}
Пусть $h =  \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix} $; $\E (h) =  \begin{pmatrix}
1\\
2\\
\end{pmatrix} $; $\Var (h) =  \begin{pmatrix}
2 & 1 \\
1 & 2 \\
\end{pmatrix} $; $z_3 =  \begin{pmatrix}
\eta_1 \\
\eta_2 \\
\end{pmatrix}  =  \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix}  -  \begin{pmatrix}
\E \xi_1 \\
\E \xi_2 \\
\end{pmatrix} $; $z_4 = \Var (h)^{-1/2} z_3$. Найдите $\E (z_4)$ и $\Var (z_4)$
\end{problem}

\begin{solution}
\end{solution}



\begin{problem}
Пусть $h =  \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix} $; $\E (h) =  \begin{pmatrix}
1\\
2\\
\end{pmatrix} $; $\Var (h) =  \begin{pmatrix}
3 & 1 \\
1 & 3 \\
\end{pmatrix} $; $z_3 =  \begin{pmatrix}
\eta_1 \\
\eta_2 \\
\end{pmatrix}  =  \begin{pmatrix}
\xi_1 \\
\xi_2 \\
\end{pmatrix}  -  \begin{pmatrix}
\E \xi_1 \\
\E \xi_2 \\
\end{pmatrix} $; $z_4 = \Var (h)^{-1/2} z_3$. Найдите $\E (z_4)$ и $\Var (z_4)$
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Случайные величины $w_1$ и $w_2$ независимы с нулевым ожиданием и единичной дисперсией. Из них составлено два вектора, 
$w=\left(
\begin{array}{c}
w_1 \\
w_2
\end{array}\right)$
и 
$z=\left(
\begin{array}{c}
-w_2 \\
w_1
\end{array}\right)$
\begin{enumerate}
\item Являются ли векторы $w$ и $z$ перпендикулярными?
\item Найдите $\E(w)$, $\E(z)$
\item Найдите $\Var(w)$, $\Var(z)$, $\Cov(w,z)$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Есть случайный вектор $w=(w_1, w_2, \ldots, w_n)'$. 
\begin{enumerate}
\item Возможно ли, что $E(w)=0$ и $\sum w_i=0$?
\item Возможно ли, что $E(w)\neq 0$ и $\sum w_i=0$?
\item Возможно ли, что $E(w)=0$ и $\sum w_i \neq 0$?
\item Возможно ли, что $E(w)\neq 0$ и $\sum w_i \neq 0$?
\end{enumerate}
\end{problem}

\begin{solution}
Каждый из вариантов возможен
\end{solution}


\begin{problem}
Известна ковариационная матрица вектора $\e=(\e_1,\e_2)$,
\[
\Var(\e)=\left(
\begin{matrix}
9 & -1 \\
-1 & 9
\end{matrix}
\right)
\]

Найдите четыре различных матрицы $A$, таких что вектор $v=A\e$ имеет некоррелированные компоненты с единичной дисперсией, то есть $\Var(A\e)=I$.
\end{problem}

\begin{solution}
\end{solution}


\chapter{Многомерное нормальное распределение} % и квадратичные формы}
\chaptermark{Многомерное нормальное}

\begin{problem}
Пусть $\e = (\e_1, \e_2, \e_3)'\sim N (0,I)$ и матрица $A$ представлена ниже. Найдите $\E(\e'A\e)$ и распределение случайной величины $\e'A\e$.

\begin{enumerate}
\item 
$\begin{pmatrix} 2/3 & -1/3 & 1/3 \\ -1/3 & 2/3 & 1/3 \\ 1/3 & 1/3 & 2/3 \end{pmatrix}$

\item 
$\begin{pmatrix} 2/3 & -1/3 & -1/3 \\ -1/3 & 2/3 & -1/3 \\ -1/3 & -1/3 & 2/3 \end{pmatrix}$

\item 
$\begin{pmatrix} 1/3 & 1/3 & -1/3 \\ 1/3 & 1/3 & -1/3 \\ -1/3 & -1/3 & 1/3 \end{pmatrix}$

\item
$\begin{pmatrix} 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \\ 1/3 & 1/3 & 1/3 \end{pmatrix}$

\item
$\begin{pmatrix} 1/2 & 0 & 1/2 \\ 0 & 1 & 0 \\ 1/2 & 0 & 1/2 \end{pmatrix}$

\item
$\begin{pmatrix} 1/2 & 0 & -1/2 \\ 0 & 1 & 0 \\ -1/2 & 0 & 1/2 \end{pmatrix}$

\item
$\begin{pmatrix} 1/2 & -1/2 & 0 \\ -1/2 & 1/2 & 0 \\ 0 & 0 & 1 \end{pmatrix}$

\item
$\begin{pmatrix} 1/2 & 1/2 & 0 \\ 1/2 & 1/2 & 0 \\ 0 & 0 & 0 \end{pmatrix}$

\item 
$\begin{pmatrix} 0.8 & 0.4 & 0 \\ 0.4 & 0.2 & 0 \\ 0 & 0 & 1 \end{pmatrix}$

\item 
$\begin{pmatrix} 0.2 & -0.4 & 0 \\ -0.4 & 0.8 & 0 \\ 0 & 0 & 0 \end{pmatrix}$

\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Пусть $i = (1,\dots,1)'$ --- вектор из $n$ единиц, $\pi=i(i'i)^{-1}i'$ и $\e = (\e_1,\dots,\e_n)'\sim N (0,I)$.
\begin{enumerate}
\item Найдите $\E(\e'\pi\e)$, $\E(\e'(I-\pi)\e)$ и $\E(\e\e')$
\item Как распределены случайные величины $\e'\pi\e$ и $\e'(I-\pi)\e$?
\item Запишите выражения $\e'\pi\e$ и $\e'(I-\pi)\e$, используя знак суммы
\end{enumerate}
\end{problem}


\begin{solution}
\end{solution}


\begin{problem}

Пусть $X = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \end{pmatrix}$, $P = X(X'X)^{-1}X'$, случайные величины $\e_1, \e_2, \e_3, \e_4$ независимы и одинаково распределены $\sim N (0,1)$.
\begin{enumerate}
\item Найдите распределение случайной величины $\e'P\e$, где $\e = \begin{pmatrix} \e_1 & \e_2 & \e_3 & \e_4 \end{pmatrix}'$
\item Найдите $\E(\e'P\e)$
\item При помощи таблиц найдите такое число $q$, что $\P(\e'P\e > q)=0.1$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}

Пусть $X = \begin{pmatrix} 1 & 1 \\ 1 & 2 \\ 1 & 3 \\ 1 & 4 \end{pmatrix}$, $P = X(X'X)^{-1}X'$, случайные величины $\e_1, \e_2, \e_3, \e_4$ независимы и одинаково распределены $\sim N (0,1)$.
\begin{enumerate}
\item Найдите распределение случайной величины $\e'P\e$, где $\e = \begin{pmatrix} \e_1 & \e_2 & \e_3 & \e_4 \end{pmatrix}'$
\item Найдите $\E(\e'P\e)$
\item При помощи таблиц найдите такое число $q$, что $\mathbb{P}(\e'P\e > q)=0.1$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}

Пусть $X = \begin{pmatrix} 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 1 & 0  \\ 1 & 1 & 1 \end{pmatrix} $, $P = X(X'X)^{-1}X'$, случайные величины $\e_1, \e_2, \e_3, \e_4$ независимы и одинаково распределены $\sim N (0,1)$.
\begin{enumerate}
\item Найдите распределение случайной величины $\e'P\e$, где $\e = \begin{pmatrix} \e_1 & \e_2 & \e_3 & \e_4 \end{pmatrix}'$.
\item Найдите $\E(\e'P\e)$.
\item При помощи таблиц найдите такое число $q$, что $\mathbb{P}(\e'P\e > q)=0.1$. 
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Пусть $\e = (\e_1, \e_2, \e_3)'\sim N (0,I)$. Найдите $\E(\e'P\e)$ и распределение случайной величины $\e'P\e$, если $P = X(X'X)^{-1}X'$ и матрица $X'$ представлена ниже.
\begin{enumerate}
\item
\ensuremath{% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:14 2015
\begin{pmatrix}{}
    1 &   1 &   1 \\ 
  \end{pmatrix}
}
\item
\ensuremath{% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:14 2015
\begin{pmatrix}{}
    1 &   2 &   3 \\ 
  \end{pmatrix}
}
\item
\ensuremath{% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:14 2015
\begin{pmatrix}{}
    1 &   1 &   1 \\ 
    0 &   0 &   1 \\ 
  \end{pmatrix}
}
\item
\ensuremath{% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:14 2015
\begin{pmatrix}{}
    1 &   1 &   1 \\ 
    1 &   2 &   3 \\ 
  \end{pmatrix}
}
\item
\ensuremath{% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Thu Feb 19 12:54:14 2015
\begin{pmatrix}{}
    1 &   1 &   1 \\ 
    0 &   1 &   1 \\ 
    0 &   0 &   1 \\ 
  \end{pmatrix}
}
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Пусть $\e = \begin{pmatrix} \e_1 \\ \e_2 \\ \e_3 \end{pmatrix} \sim N (0,\sigma^2 I)$, $i = (1,\dots,1)'$ --- вектор из $n$ единиц, $\pi=i(i'i)^{-1}i'$,
$X$ --- матрица размера ${n \times k}$, $P = X(X'X)^{-1}X'$. Найдите:
\begin{enumerate}
\item $\E(\e'(P - \pi)\e)$ 
\item $\E(\e'(I - \pi)\e)$
\item $\E(\e'P \e)$
\item $\E(\sum_{i=1}^n {(\e_i - \bar{\e})}^2)$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Пусть $\e = (\e_1, \e_2, \e_3)'\sim N (0,4I)$, $A = \begin{pmatrix} 4 & 1 & 1 \\ 1 & 3 & 1 \\ 1 & 1 & 2 \end{pmatrix}$. Найдите:



\begin{enumerate}
\item $\E(\e'A\e)$
\item $\E(\e'(I - A)\e)$ 
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Пусть $x =  \begin{bmatrix}
x_1 & x_2\\
\end{bmatrix} ^T$ --- случайный вектор, имеющий двумерное нормальное распределение с математическим ожиданием $\mu =  \begin{bmatrix}
1 & 2\\
\end{bmatrix} ^T$ и ковариационной матрицей $\Sigma = 
\begin{bmatrix}
2 & 1 \\
1 & 2 \\
\end{bmatrix} $. 

\begin{enumerate}
\item Найдите $\Sigma^{-1}$
\item Найдите $\Sigma^{-1/2}$
\item Найдите математическое ожидание и ковариационную матрицу случайного вектора $y = \Sigma^{-1/2} \cdot (x - \mu)$
\item Какое распределение имеет вектор $y$ из предыдущего пункта?
\item Найдите распределение случайной величины $q = (x- \mu)^T \cdot \Sigma^{-1} \cdot (x - \mu)$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Пусть $z =  \begin{bmatrix}
z_1 & z_2 & z_3\\
\end{bmatrix} ^T \sim N(0, I_{3x3})$, $b =  \begin{bmatrix}
1 & 2 & 3\\
\end{bmatrix} ^T$,

$A =  \begin{bmatrix}
1 & 1 & 1 \\
0 & 1 & 1 \\
0 & 0 & 1 \\
\end{bmatrix} $, $K =  \begin{bmatrix}
1 & 0 & 0 \\
0 & 1/2 & 1/2 \\
0 & 1/2 & 1/2 \\
\end{bmatrix} $.

\begin{enumerate}
\item Найдите $\E x$ и $\Var (x)$ случайного вектора $x = A \cdot z + b$
\item Найдите распределение случайного вектора $x$
\item Найдите $\E q$ случайной величины $q = z^T \cdot K \cdot z$
\item Найдите распределение случайной величины $q$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Известно, что $\e\sim N(0,I)$, $\e=(\e_1,\e_2,\e_3)'$. Матрица $A=\left(\begin{matrix}
2/3 & -1/3 & -1/3 \\ 
-1/3 & 2/3 & -1/3 \\ 
-1/3 & -1/3 & 2/3
\end{matrix}\right)$.
\begin{enumerate}
\item Найдите $\E(\e'A\e)$
\item Как распределена случайная величина $\e' A\e$?
\end{enumerate}
\end{problem}

\begin{solution}
по $\chi^2$-распределению 
\end{solution}


\begin{problem}
Известно, что $\e\sim N(0,A)$, $\e=(\e_1,\e_2)'$. Матрица $A=\left(\begin{matrix}
4 & 1 \\ 
1 & 4 
\end{matrix}\right)$, матрица $B=\left(\begin{matrix}
-1 & 3 \\ 
2 & 1 
\end{matrix}\right)$
\begin{enumerate}
\item Как распределен вектор $h=B\e$?
\item Найдите $A^{-1/2}$
\item Как распределен вектор $u=A^{-1/2}\e$?
\end{enumerate}
\end{problem}

\begin{solution}
$u\sim N(0,I)$ 
\end{solution}

\chapter{Задачи по программированию}

\begin{problemtext}
Все наборы данных доступны по ссылке \url{https://github.com/bdemeshev/em301/wiki/Datasets}.
\end{problemtext}

\begin{problem}
Начиная с какого знака в числе $\pi=3.1415\ldots$ можно обнаружить твой номер телефона? Первый $10$ миллионов знаков числа $\pi$ можно найти на сайте \url{http://code.google.com/p/pc2012-grupo-18-turma-b/downloads/list}. Если не хватает, то миллиард знаков, файл размера примерно в 1 гигабайт, доступен по ссылке \url{http://stuff.mit.edu/afs/sipb/contrib/pi/}. Настоящие челябинцы рассчитывают $\pi$ самостоятельно. Краткая история о том, как маньяки считали $\pi$ до 10 миллиардов знаков и потеряли полгода из-за сбоев компьютерного железа, \url{http://www.numberworld.org/misc_runs/pi-10t/details.html}. 
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Отряд Иосифа Флавия из 40 воинов, защищающий город Йодфат, блокирован в пещере превосходящими силам римлян. Чтобы не сдаться врагу, воины стали по кругу и договорились, что сами будут убивать каждого третьего, пока не погибнут все. При этом двое воинов, оставшихся последними в живых, должны были убить друг друга. Хитренький Иосиф Флавий, командующий этим отрядом, хочет определить, где нужно встать ему и его товарищу, чтобы остаться последними. Не для того, чтобы убить друг друга, а чтобы сдать крепость римлянам. Напишите программу, которая для $n$ воинов вставших в круг определяет, какие двое останутся последними, если будут убивать каждого $k$-го.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Напишите программу, которая печатает сама себя.
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Задача Макар-Лиманова. У торговца 55 пустых стаканчиков, разложенных в несколько стопок. Пока нет покупателей он развлекается: берет верхний стаканчик из каждой стопки и формирует из них новую стопку. Потом снова берет верхний стаканчик из каждой стопки и формирует из них новую стопку и т.д.
\begin{enumerate}
\item Напишите функцию \verb|makar_step|. На вход функции подаётся вектор количества стаканчиков в каждой стопке до перекладывания. На выходе функция возвращает количества стаканчиков в каждой стопке после одного перекладывания.
\item Изначально стаканчики были разложены в две стопки, из 25 и 30 стаканчиков. Как разложатся стаканчики если покупателей не будет достаточно долго?
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Напишите программу, которая находит сумму элементов побочной диагонали квадратной матрицы. 
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Напишите функцию, которая по матрице $X$ и вектору $y$ для модели $Y=X\beta+\e$ вычисляет значение статистики Дарбина-Уотсона.
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Напишите функцию, которая по матрице $X$ и вектору $y$ для модели $Y=X\beta+\e$ вычисляет оценки дисперсии коэффициентов, скорректированные на гетероскедастичность по формуле Уайта 
\[
\widehat{\Var}_{White}(\hb_j)=\frac{\sum_{i=1}^{n}\he_i^2\hat{u}_{ij}^2}{RSS_j},
\]
где $\hat{u}_{ij}$ --- остатки в линейной регрессии фактора $x_j$ на остальные регрессоры, а $RSS_j$ --- сумма квадратов остатков в этой регрессионной модели.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Напишите функцию, которая по матрице $X$ и вектору $y$ для модели $Y=X\beta+\e$ вычисляет оценки ковариационной матрицы коэффициентов, скорректированную на гетероскедастичность по формуле Уайта: 
\[
\widehat{\Var}_{White}(\hb_{OLS})=(X'X)^{-1} \left( \sum_{i=1}^n \he_i^2 X_{i\cdot}'X_{i\cdot} \right) (X'X)^{-1},
\] 
где $X_{i\cdot}$ --- $i$-ая строка матрицы $X$.
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Напишите программу, которая по заданной матрице регрессоров $X$
возвращает матрицу $Z$ , столбцами которой являются все столбцы матрицы X ,
<<квадраты>> столбцов матрицы $X$, а также перекрестные <<произведения>> столбцов
матрицы $X$.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Напишите функцию, которая по матрице $X$ и вектору $y$ возвращает
значение статистики Уайта.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Напишите функцию, которая по матрице $X$, вектору $y$ и уровню
значимости реализует тест Уайта.
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Напишите функцию, которая по матрице $X$, вектору $y$ и количеству лагов $L$ находит оценку ковариационной матрицу  коэффициентов, скорректированную на гетероскедастичность и автокорреляцию по формуле Невье-Веста:
\[
\widehat{\Var}_{NW}(\hb_{OLS})=(X'X)^{-1}\hat{S}(X'X)^{-1},
\]
где
\[
\hat{S}=\sum_{t=1}^{n}\he_t^2 X_{t\cdot}'X_{t\cdot}+
                     \sum_{j=1}^L w_j \left(\sum_{t=j+1}^n  \he_t \he_{t-j}(X_{t\cdot}'X_{t-j\cdot}+X_{t-j\cdot}'X_{t\cdot})  \right),
\]
где $\e_t$ --- остатки в регрессии $y=X\beta+\e$, а $X_{t\cdot}$ --- строка номер $t$ матрицы $X$.
Напишите две версии данной функции, для разных способов рассчета весов $w_j$:
\begin{enumerate}
\item $w_j=1-j/L$
\item 
\[
w_j=\begin{cases}
1-\frac{6j^2}{(1+L)^2}+\frac{6j^3}{(1+L)^3},\, \text{если}\,  j\leq (1+L)/2 \\
2\left(1-\frac{j}{1+L} \right)^2,\, \text{если}\, j>(1+L)/2
\end{cases}
\]
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}



\restorechapter
%\chapter{Таблицы}

%<<child='../chapters/250_tables.Rnw'>>=
%@




\end{document}
