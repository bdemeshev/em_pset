\Opensolutionfile{solution_file}[sols_080]
% в квадратных скобках фактическое имя файла

\chapter{Гетероскедастичность}


\begin{problem}
Что такое гетероскедастичность? Гомоскедастичность?

\begin{sol}
Рассмотрим вопрос на примере парной регрессии $y_i = \beta_1 + \beta_2 x_i + \e_i$. Можно выделить условную и безусловную гетероскедастичность. Безусловная — $\Var(\e_i)\neq const$. Условная — $\Var(\e_i | x_i) \neq const$.

 Условная и безусловная дисперсии связаны соотношением:

\[
\Var(\e_i) = \E(\Var(\e_i|x_i)) + \Var(\E(\e_i|x_i))
\]

То есть при условии $\E(\e_i |x_i)=0$ из условной гомоскедастичности следует безусловная.

Удобно изучать гетероскедастичность при парадигме стохастических регрессоров, а именно, предполагать, что наблюдения представляют собой случайную выборку. В этой ситуации получаются одновременно условно гетероскедастичные и безусловно гомоскедастичные ошибки.
\end{sol}
\end{problem}


\begin{problem}
В модели $y_i = \beta_1 + \beta_2 x_i + \e_i$ присутствует гетероскедастичность вида $\Var(\e_i)=\sigma^2 x^2_i$. Как надо преобразовать исходные регрессоры и зависимую переменную, чтобы устранить гетероскедастичность?


\begin{sol}
Поделить зависимую переменную и каждый регрессор, включая единичный столбец, на $|x_i|$.
\end{sol}
\end{problem}



\begin{problem}
В модели $y_i = \beta_1 + \beta_2 x_i + \e_i$ присутствует гетероскедастичность вида $\Var(\e_i)=\lambda |x_i|$. Как надо преобразовать исходные регрессоры и зависимую переменную, чтобы устранить гетероскедастичность?


\begin{sol}
Поделить зависимую переменную и каждый регрессор, включая единичный столбец, на $\sqrt{|x_i|}$.
\end{sol}
\end{problem}



\begin{problem}
Известно, что после деления каждого уравнения регрессии $y_i = \beta_1 + \beta_2 x_i + \e_i$ на $x_i^2$ гетероскедастичность ошибок была устранена. Какой вид имела дисперсия ошибок, $\Var(\e_i)$?


\begin{sol}
$\Var(\e_i)=cx_i^4$
\end{sol}
\end{problem}



\begin{problem}
Известно, что после деления каждого уравнения регрессии $y_i = \beta_1 + \beta_2 x_i + \e_i$ на $\sqrt{x_i}$ гетероскедастичность ошибок была устранена. Какой вид имела дисперсия ошибок, $\Var(\e_i)$?

\begin{sol}
$\Var(\e_i)=c x_i$
\end{sol}
\end{problem}



\begin{problem}
Диаграмма рассеяния стоимости квартиры в Москве (в 1000\$) и общей площади квартиры имеет вид:

<<"flats_scatter", include=FALSE>>=
tikz("../R_plots/flats_scatter.tikz", standAlone = FALSE, bareBones = TRUE)
ggplot(flats, aes(x = totsp, y = price)) + geom_point() +
    labs(x = "Общая площадь, кв. м.",
    y = "Цена квартиры, 1000\\$")
invisible(dev.off())
@

<<eval=FALSE>>=
ggplot(flats, aes(x = totsp, y = price)) + geom_point() +
    labs(x = "Общая площадь, кв. м.",
    y = "Цена квартиры, 1000\\$")
@

\begin{figure}
\begin{tikzpicture}[scale = 0.025]
\input{R_plots/flats_scatter.tikz}
\end{tikzpicture}
\end{figure}


Какие подходы к оцениванию зависимости имеет смысл посоветовать исходя из данного графика?



\begin{sol}
По графику видно, что с увеличением общей площади увеличивается разброс цены. Поэтому разумно, например, рассмотреть следующие подходы:
\begin{enumerate}
\item Перейти к логарифмам, т.е. оценивать модель $\ln price_i=\beta_1+\beta_2 \ln totsp_i +\varepsilon_i$
\item Оценивать квантильную регрессию. В ней угловые коэффициенты линейной зависимости будут отличаться для разных квантилей переменной $price$.
\item Обычную модель линейной регрессии с гетероскедастичностью вида $Var(\varepsilon_i)=\sigma^2 totsp_i^2$
\end{enumerate}
\end{sol}
\end{problem}




\begin{problem}
По наблюдениям $x=(1,2,3)'$, $y=(2,-1,3)'$ оценивается модель $y=\b_1+\b_2 x+\e$. Ошибки $\e$ гетероскедастичны и известно, что $\Var(\e_i)=\sigma^2 \cdot x_i^2$.
\begin{enumerate}
\item Найдите оценки $\hb_{ols}$ с помощью МНК и их ковариационную матрицу
\item Найдите оценки $\hb_{gls}$ с помощью обобщенного МНК и их ковариационную матрицу
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}





\begin{problem}
Для линейной регрессии $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i + \e_i$ была
выполнена сортировка наблюдений по возрастанию переменной $x$. Исходная модель оценивалась по разным частям выборки:

\begin{tabular}{c|cccc}
Выборка & $\hb_1$ & $\hb_2$ & $\hb_3$ & $RSS$ \\
\hline
$i=1,\ldots, 30$ & $1.21$ & $1.89$ & $2.74$ & $48.69$ \\
$i=1,\ldots, 11$ & $1.39$ & $2.27$ & $2.36$ & $10.28$ \\
$i=12,\ldots, 19$ & $0.75$ & $2.23$ & $3.19$ & $5.31$ \\
$i=20,\ldots, 30$ & $1.56$ & $1.06$ & $2.29$ & $14.51$ \\
\end{tabular}

Известно, что ошибки в модели являются независимыми нормальными случайными величинами с нулевым математическим ожиданием. Протестируйте
ошибки на гетероскедастичность на уровне значимости 5\%.



\begin{sol}
Протестируем гетероскедастичность ошибок при помощи теста Голдфельда-
Квандта. $H_0: \Var(\e_i)=\sigma^2$, $H_a: \Var(\e_i)=f(x_i)$

\begin{enumerate}
\item Тестовая статистика $GQ=\frac{RSS_3/(n_3-k)}{RSS_1/(n_1-k)}$, где $n_1=11$ — число наблюдений в первой подгруппе, $n_3=11$ — число наблюдений в
последней подгруппе, $k=3$ — число факторов в модели, считая единичный столбец.
\item Распределение тестовой статистики при верной $H_0$: $GQ\sim F_{n_3-k,n_1-k}$
\item Наблюдаемое значение $GQ_{obs}=1.41$
\item Область, в которой $H_0$ не отвергается: $GQ\in [0;3.44]$
\item Статистический вывод: поскольку $GQ_{obs} \in [0;3.44]$, то на основании имеющихся наблюдений на уровне значимости 5\% основная гипотеза $H_0$ не может быть отвергнута. Таким образом, тест Голдфельда-Квандта не выявил гетероскедастичность.
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
Для линейной регрессии $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i + \e_i$ была
выполнена сортировка наблюдений по возрастанию переменной $x$. Исходная модель оценивалась по разным частям выборки:

\begin{tabular}{c|cccc}
Выборка & $\hb_1$ & $\hb_2$ & $\hb_3$ & $RSS$ \\
\hline
$i=1,\ldots, 50$ & $1.16$ & $1.99$ & $2.97$ & $174.69$ \\
$i=1,\ldots, 21$ & $0.76$ & $2.25$ & $3.18$ & $20.41$ \\
$i=22,\ldots, 29$ & $0.85$ & $1.81$ & $3.32$ & $3.95$ \\
$i=30,\ldots, 50$ & $1.72$ & $1.41$ & $2.49$ & $130.74$ \\
\end{tabular}

Известно, что ошибки в модели являются независимыми нормальными случайными величинами с нулевым математическим ожиданием. Протестируйте
ошибки на гетероскедастичность на уровне значимости 1\%.


\begin{sol}
Протестируем гетероскедастичность ошибок при помощи теста Голдфельда-
Квандта. $H_0: \Var(\e_i)=\sigma^2$, $H_a: \Var(\e_i)=f(x_i)$

\begin{enumerate}
\item Тестовая статистика $GQ=\frac{RSS_3/(n_3-k)}{RSS_1/(n_1-k)}$, где $n_1=21$ — число наблюдений в первой подгруппе, $n_3=21$ — число наблюдений в
последней подгруппе, $k=3$ — число факторов в модели, считая единичный столбец.
\item Распределение тестовой статистики при верной $H_0$: $GQ\sim F_{n_3-k,n_1-k}$
\item Наблюдаемое значение $GQ_{obs}=6.49$
\item Область, в которой $H_0$ не отвергается: $GQ\in [0;3.12]$
\item Статистический вывод: поскольку $GQ_{obs} \notin [0;3.12]$, то на основании имеющихся наблюдений на уровне значимости 1\% основная гипотеза $H_0$ отвергается. Таким образом, тест Голдфельда-Квандта выявил гетероскедастичность.
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
Для линейной регрессии $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i + \e_i$ была
выполнена сортировка наблюдений по возрастанию переменной $x$. Исходная модель оценивалась по разным частям выборки:

\begin{tabular}{c|cccc}
Выборка & $\hb_1$ & $\hb_2$ & $\hb_3$ & $RSS$ \\
\hline
$i=1,\ldots, 30$ & $0.96$ & $2.25$ & $3.44$ & $52.70$ \\
$i=1,\ldots, 11$ & $1.07$ & $2.46$ & $2.40$ & $5.55$ \\
$i=12,\ldots, 19$ & $1.32$ & $1.01$ & $2.88$ & $11.69$ \\
$i=20,\ldots, 30$ & $1.04$ & $2.56$ & $4.12$ & $16.00$ \\
\end{tabular}

Известно, что ошибки в модели являются независимыми нормальными случайными величинами с нулевым математическим ожиданием. Протестируйте
ошибки на гетероскедастичность на уровне значимости 5\%.


\begin{sol}
Протестируем гетероскедастичность ошибок при помощи теста Голдфельда-
Квандта. $H_0: \Var(\e_i)=\sigma^2$, $H_a: \Var(\e_i)=f(x_i)$

\begin{enumerate}
\item Тестовая статистика $GQ=\frac{RSS_3/(n_3-k)}{RSS_1/(n_1-k)}$, где $n_1=11$ — число наблюдений в первой подгруппе, $n_3=11$ — число наблюдений в
последней подгруппе, $k=3$ — число факторов в модели, считая единичный столбец.
\item Распределение тестовой статистики при верной $H_0$: $GQ\sim F_{n_3-k,n_1-k}$
\item Наблюдаемое значение $GQ_{obs}=2.88$
\item Область, в которой $H_0$ не отвергается: $GQ\in [0;3.44]$
\item Статистический вывод: поскольку $GQ_{obs} \in [0;3.44]$, то на основании имеющихся наблюдений на уровне значимости 5\% основная гипотеза $H_0$ не может быть отвергнута. Таким образом, тест Голдфельда-Квандта не выявил гетероскедастичность.
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
Для линейной регрессии $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i + \e_i$ была выполнена сортировка наблюдений по возрастанию переменной $x$. Исходная модель оценивалась по разным частям выборки:

\begin{tabular}{c|cccc}
Выборка & $\hb_1$ & $\hb_2$ & $\hb_3$ & $RSS$ \\

\hline
$i=1,\ldots, 50$ & $0.93$ & $2.02$ & $3.38$ & $145.85$ \\
$i=1,\ldots, 21$ & $1.12$ & $2.01$ & $3.32$ & $19.88$ \\
$i=22,\ldots, 29$ & $0.29$ & $2.07$ & $2.24$ & $1.94$ \\
$i=30,\ldots, 50$ & $0.87$ & $1.84$ & $3.66$ & $117.46$ \\
\end{tabular}

Известно, что ошибки в модели являются независимыми нормальными случайными величинами с нулевым математическим ожиданием. Протестируйте
ошибки на гетероскедастичность на уровне значимости 5\%.

\begin{sol}
Протестируем гетероскедастичность ошибок при помощи теста Голдфельда-
Квандта. $H_0: \Var(\e_i)=\sigma^2$, $H_a: \Var(\e_i)=f(x_i)$

\begin{enumerate}
\item Тестовая статистика $GQ=\frac{RSS_3/(n_3-k)}{RSS_1/(n_1-k)}$, где $n_1=21$ — число наблюдений в первой подгруппе, $n_3=21$ — число наблюдений в
последней подгруппе, $k=3$ — число факторов в модели, считая единичный столбец.
\item Распределение тестовой статистики при верной $H_0$: $GQ\sim F_{n_3-k,n_1-k}$
\item Наблюдаемое значение $GQ_{obs}=5.91$
\item Область, в которой $H_0$ не отвергается: $GQ\in [0;2.21]$
\item Статистический вывод: поскольку $GQ_{obs} \notin [0;2.21]$, то на основании имеющихся наблюдений на уровне значимости 5\% основная гипотеза $H_0$ отвергается. Таким образом, тест Голдфельда-Квандта выявил гетероскедастичность.
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
Рассмотрим линейную регрессию $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i + \e_i$ по 50 наблюдениям. При оценивании с помощью МНК были получены результаты: $\hb_1=1.21$, $\hb_2=1.11$, $\hb_3=3.15$, $R^2=0.72$.

Оценена также вспомогательная регрессия: $\he^2_i=\delta_1+\delta_2 x_i +\delta_3 z_i+\delta_4 x_i^2+\delta_5 z_i^2+\delta_6 x_i z_i + u_i$. Результаты оценивания следующие: $\hat{\delta}_1=1.50$, $\hat{\delta}_2=-2.18$,  $\hat{\delta}_3=0.23$,  $\hat{\delta}_4=1.87$,  $\hat{\delta}_5=-0.56$,  $\hat{\delta}_6=-0.09$,  $R^2_{aux}=0.36$


Известно, что ошибки в модели являются независимыми нормальными случайными величинами с нулевым математическим ожиданием. Протестируйте
ошибки на гетероскедастичность на уровне значимости 5\%.


\begin{sol}
Протестируем гетероскедастичность ошибок при помощи теста Уайта. $H_0: \Var(\e_i)=\sigma^2$, $H_a: \Var(\e_i)=\delta_1+\delta_2 x_i +\delta_3 z_i+\delta_4 x_i^2+\delta_5 z_i^2+\delta_6 x_i z_i$.
\begin{enumerate}
\item Тестовая статистика $W=n\cdot R^2_{aux}$, где $n$ — число наблюдений, $R^2_{aux}$ — коэффициент детерминации для вспомогательной регрессии.
\item Распределение тестовой статистики при верной $H_0$: $W\sim \chi^2_{k_{aux}-1}$, где $k_{aux}=6$ — число регрессоров во вспомогательной регрессии, считая константу.
\item Наблюдаемое значение тестовой статистики: $W_{obs}=18$
\item Область, в которой $H_0$ не отвергается: $W\in [0;W_{crit}]=[0;11.07]$
\item Статистический вывод: поскольку $W_{obs} \notin [0;11.07]$, то на основании имеющихся наблюдений на уровне значимости 5\% основная гипотеза $H_0$ отвергается. Таким образом, тест Уайта выявил гетероскедастичность.
\end{enumerate}
\end{sol}
\end{problem}




\begin{problem}
Объясните, с какой целью используются стандартные ошибки в форме Уайта. Приведите развернутый ответ. Верно ли, что стандартные ошибки в форме Уайта позволяют
\begin{enumerate}
\item устранить гетероскедастичность?
\item корректно тестировать гипотезы относительно коэффициентов регрессии в условиях гетероскедастичности?
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Объясните, с какой целью используются стандартные ошибки в форме Невье–Веста. Приведите развернутый ответ. Верно ли, что стандартные ошибки в форме Невье–Веста позволяют
\begin{enumerate}
\item устранить гетероскедастичность?
\item корректно тестировать гипотезы относительно коэффициентов регрессии в условиях гетероскедастичности?
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Рассматривается модель $y_t=\beta_1+\e_t$, где ошибки $\e_t$  — независимые
случайные величины с $\E(\e_t)=0$ и $\Var(\e_t)=t$. Найдите наиболее эффективную
оценку неизвестного параметра $\beta_1$ в классе линейных по $y$ и несмещенных оценок.

\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Рассматривается модель $y_t=\beta_1+\e_t$, где ошибки $\e_t$  — независимые
случайные величины с $\E(\e_t)=0$ и $\Var(\e_t)=t^2$. Найдите наиболее эффективную
оценку неизвестного параметра $\beta_1$ в классе линейных по $y$ и несмещенных оценок.

\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Рассматривается модель $y_t=\beta_1 x_t+\e_t$, где ошибки $\e_t$  — независимые
случайные величины с $\E(\e_t)=0$ и $\Var(\e_t)=t$. Найдите наиболее эффективную
оценку неизвестного параметра $\beta_1$ в классе линейных по $y$ и несмещенных оценок.

\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Рассматривается модель $y_t=\beta_1 x_t +\e_t$, где ошибки $\e_t$  — независимые
случайные величины с $\E(\e_t)=0$ и $\Var(\e_t)=t^2$. Найдите наиболее эффективную
оценку неизвестного параметра $\beta_1$ в классе линейных по $y$ и несмещенных оценок.

\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Докажите, что в условиях гетероскедастичности МНК-
оценки остаются несмещенными.


\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Оценка коэффициентов обобщенного МНК имеет вид $\hb_{GLS}=(X'V^{-1}X)^{-1}X'V^{-1}y$, где $V=\Var(\e)$. Совпадает ли оценка $\hb_{GLS}$ с оценкой обычным МНК в условиях гомоскедастичности?


\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Модель $y_i=\beta_1 + \beta_2 x_i +\e_i$ оценивается по трём наблюдениям, $y=(9,3,6)$, $x=(1,2,4)$. Имеется гетероскедастичность вида $\Var(\e_i)=\sigma^2 x_i^2$, ошибки $\e_i$ нормально распределены.
\begin{enumerate}
\item Оцените $\hb$ с помощью МНК проигнорировав гетероскедастичность. Постройте 95\% доверительный интервал для каждого коэффициента, проигнорировав гетероскедастичность
\item Оцените $\hb$ с помощью обобщенного МНК учтя гетероскедастичность. Постройте 95\% доверительный интервал для каждого коэффициента с учётом гетероскедастичности
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Рассмотрим модель $y_i=\beta_1+\beta_2 x_i+\e_i$, где ошибки $\e_i$ некоррелированы, $\E(\e_i)=0$, $\Var(\e_i)=\sigma^2_i$. Предлагается два способа оценить коэффициенты модели:
\begin{enumerate}
\item[WLS.] Взвешенный метод наименьших квадратов. Поделим каждое уравнение $y_i=\beta_1+\beta_2 x_i+\e_i$ на $\sigma_i$. Затем обычным методом наименьших квадратов в преобразованной модели $y_i/\sigma_i=\beta_1\cdot 1/\sigma_i+\beta_2 x_i/\sigma_i+\e_i/\sigma_i$ найдем оценки $\hb_{WLS}$.
\item[GLS.] Обобщенный метод наименьших квадратов. Оценки $\hb_{GLS}$ находим по формуле $\hb_{GLS}=(X'V^{-1}X)^{-1}X'V^{-1}y$, где
\[
V=\Var(\e)=\begin{pmatrix}
\sigma^2_1 & 0 & 0 \\
0 & \ddots & 0 \\
0 & 0 & \sigma^2_n
\end{pmatrix}
\]
\end{enumerate}
\begin{enumerate}
\item Докажите, что в матричном виде преобразование взвешенного МНК записывается как $V^{-1/2}y=V^{-1/2}X\beta+V^{-1/2}\e$.
\item Верно ли, что $\hb_{WLS}=\hb_{GLS}$?
\item Найдите $\E(\hb_{WLS})$, $\Var(\hb_{WLGS})$
\item В явном виде выпишите $\hb_{2,WLS}$
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
Рассмотрим модель регрессии $y_i=\beta_1+\beta_2 x_i + \beta_3 z_i+\e_i$, в которой
ошибки $\e_i$ независимы и имеют нормальное распределение $N(0,\sigma^2)$. Для $n = 200$ наблюдений найдите
\begin{enumerate}
\item вероятность того, что статистика Уайта окажется больше 10,
\item ожидаемое значение статистики Уайта,
\item дисперсию статистики Уайта.
\end{enumerate}


\begin{sol}
$0.0752$, $5$, $10$
\end{sol}
\end{problem}


\begin{problem}
Найдите число коэффициентов во вспомогательной регрессии, необходимой для выполнения теста Уайта, если число коэффициентов в исходной регрессии равно $k$, включая свободный член.


\begin{sol}
$k(k+1)/2$
\end{sol}
\end{problem}


\begin{problem}
По 35 наблюдениям сотрудники НИИ оценили уравнение регрессии $y_i = \beta_1 + \beta_2 x_i + \e_i$ и рассчитали остатки $\e_i$. После того они приступили к диагностике возможных недостатков модели, обнаружили гетероскедастичность и решили её побороть.
\begin{enumerate}
\item[(a)] Самый младший научный сотрудник выдвинул предположение, что стандартное отклонение случайной составляющей может быть выражено так: $\sigma_{\e, i} = a x_i$, где $a$ --– неизвестный коэффициент. Каким образом нужно преобразовать исходное уравнение регрессии, чтобы избавиться от гетероскедастичности?
\item[(b)] Профессор решил перепроверить результаты и оценил регрессию:
$$\hat{e}_i^2 = -0.3 + 0.08 x_i - 0.01 x_i^2, R^2 = 0.15$$
Свидетельствует ли полученный профессором результат о наличии гетероскедастичности?
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Пусть $y_t = \beta x_t + \e$ где $\E(\e_t) = 0$ и известно, что оценки для параметров $\tilde{\beta} = \left( \sum_{t=1}^n y_t \right)/\left( \sum_{t=1}^n x_t \right)$ являются наилучшими (в смысле минимума дисперсии) среди линейных несмещенных оценок параметра $\beta$. Чему равна в этом случае матрица ковариаций вектора $\e$ с точностью до пропорциональности?



\begin{sol}
Известно, что оценки параметров, получаемые по обобщённому методу наименьших квадратов, являются наилучшими, поэтому:
$\delta^2
\begin{bmatrix}
x_1     & 0      & \cdots & 0 \\
0       & x_2    & \cdots & 0 \\
\vdots  & \vdots & \ddots & \vdots \\
0       & 0      & \cdots & x_n \\
\end{bmatrix}$
\end{sol}
\end{problem}


\begin{problem}
Для регрессии $y = X\beta + \e$ с $\E(\e) = 0$, $\Var(\e) = \Sigma \neq \sigma^2 I$, оцененной с помощью обобщённого метода наименьших квадратов, найдите ковариационную матрицу $\Cov(\hat{\beta}_{GLS}, \e)$



\begin{sol}
\begin{multline*}
\Cov(\hat{\beta}_{GLS}, \e) = \Cov \left( (X' \Sigma^{-1} X)^{-1} X' \Sigma^{-1} y, \e \right) = \\
= \Cov \left( (X' \Sigma^{-1} X)^{-1} X' \Sigma^{-1} \e, \e \right) = \\
= (X' \Sigma^{-1} X)^{-1} X' \Sigma^{-1} \Cov(\e, \e) =\\
= (X' \Sigma^{-1} X)^{-1} X' \Sigma^{-1} \Sigma = (X' \Sigma^{-1} X)^{-1} X'
\end{multline*}
\end{sol}
\end{problem}



\begin{problem}
Найдите наиболее эффективную оценку коэффициента $\beta_1$ для модели $y_i = \beta_1 + \e$, $\E(\e_i) = 0$, $\E(\e_i\e_j) = 0$, $\Var(\e_i) = \sigma_{\e}^2 / x_i$, $x_i > 0$ в классе линейных несмещенных оценок


\begin{sol}
Для нахождения эффективной оценки воспользуемся взвешенным методом наименьших квадратов. Разделим каждое из уравнений $y_i = \beta_1 + \e$ на корень из дисперсии $\e_i$ с тем, чтобы ошибки в полученных уравнениях имели равные дисперсии (в этом случае можно будет сослаться на т. Гаусса-Маркова). Итак, после деления i-го уравнения на величину $\sqrt{x_i}/\sigma_{\e}$, мы получаем:
$$\begin{bmatrix}
y_1 \sqrt{x_1}/\sigma_{\e} \\
y_2 \sqrt{x_2}/\sigma_{\e} \\
\ldots \\
y_n \sqrt{x_n}/\sigma_{\e} \\
\end{bmatrix} = \beta_1 \begin{bmatrix}
\sqrt{x_1}/\sigma_{\e} \\
\sqrt{x_2}/\sigma_{\e} \\
\ldots \\
\sqrt{x_n}/\sigma_{\e} \\
\end{bmatrix} + \begin{bmatrix}
\e_1 \sqrt{x_1}/\sigma_{\e} \\
\e_2 \sqrt{x_2}/\sigma_{\e} \\
\ldots \\
\e_n \sqrt{x_n}/\sigma_{\e} \\
\end{bmatrix}$$
Поскольку условия т. Гаусса-Маркова для последней модели выполнены, то МНК-оценка для последней модели будет наиболее эффективной. Поэтому
$$\hat{\beta_1} = \frac{\sum_{i=1}^n (y_i \sqrt{x_i}/\sigma_{\e})(\sqrt{x_i}/\sigma_{\e})}{\sum_{i=1}^n (\sqrt{x_1}/\sigma_{\e})} = \frac{\sum_{i=1}^n y_i x_i}{\sum_{i=1}^n x_i^2}$$
\end{sol}
\end{problem}



\begin{problem}
Исследователь оценил регрессионную модель $y_i = \beta_1 + \beta_2 x_{i2} + \beta_3 x_{i3} + \beta_4 x_{i4} + \e_i$ и провёл диагностику различных проблемных явлений. Результаты его стараний приведены ниже:



<<"residual_plot", include=FALSE>>=
tikz("../R_plots/residual_plot.tikz", standAlone = FALSE, bareBones = TRUE)
n <- 200
x2 <- rnorm(n)
x3 <- rnorm(n)
x4 <- rnorm(n)
eps <- abs(x2)*rnorm(n)
y <- 2 + 3 * x2 - 3 * x3 + 1 * x4 + eps
model <- lm(y ~ x2 + x3 + x4)
resid <- resid(model)
plot1 <- qplot(x2, abs(resid),
  xlab = "Переменная $x_2$",
  ylab = "Модуль остатков")
plot2 <- qplot(head(resid, -1),
       tail(resid, -1),
       xlab = "Остаток $\\varepsilon_{t-1}$",
       ylab = "Остаток $\\varepsilon_{t}$")
grid.arrange(plot1, plot2, ncol = 2)
invisible(dev.off())
@

<<eval=FALSE>>=
n <- 200
x2 <- rnorm(n)
x3 <- rnorm(n)
x4 <- rnorm(n)
eps <- abs(x2)*rnorm(n)
y <- 2 + 3 * x2 - 3 * x3 + 1 * x4 + eps
model <- lm(y ~ x2 + x3 + x4)
resid <- resid(model)
plot1 <- qplot(x2, abs(resid),
  xlab = "Переменная $x_2$",
  ylab = "Модуль остатков")
plot2 <- qplot(head(resid, -1),
       tail(resid, -1),
       xlab = "Остаток $\\varepsilon_{t-1}$",
       ylab = "Остаток $\\varepsilon_{t}$")
grid.arrange(plot1, plot2, ncol = 2)
@



\begin{figure}
\begin{tikzpicture}[scale = 0.025]
\input{R_plots/residual_plot.tikz}
\end{tikzpicture}
\end{figure}

$VIF_2 = 1.06$, $VIF_3 = 1.07$, $VIF_4 = 1.02$
\begin{enumerate}
\item[(a)] Определите, какие проблемные явления обнаружил исследователь. Обоснуйте свой ответ.
\item[(b)] Найдите коэффициент детерминации для регрессии: $x_{i2} = \gamma_1 + \gamma_2 x_{i3} + \gamma_3 x_{i4} + u_i$
\end{enumerate}



\begin{sol}
\end{sol}
\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Новые задачи

\begin{problem}
В модели $y_i=\beta x_i+\e_i$ предполагается гетероскедастичность вида $\Var(\e_i)=\exp(\gamma_1+\gamma_2 x_i)$ и нормальность ошибок.
\begin{enumerate}
\item Сформулируйте гипотезу о гомоскедастичности с помощью коэффициентов
\item Выведите в явном виде оценку максимального правдоподобия при предположении о гомоскедастичности
\item Выпишите условия первого порядка для оценки максимального правдоподобия без предположения о гомоскедастичности
\item Выведите в явном виде формулу для LM теста множителей Лагранжа
\end{enumerate}


\begin{sol}
В предположении о гомоскедастичности, $\gamma_2=0$, оценка правдоподобия совпадает с МНК-оценкой, значит $\hb=\sum y_i x_i/ \sum x_i^2$. И $\hs^2_i=RSS/n$, значит $\hat{\gamma_1}=\ln(RSS/n)$.
\end{sol}
\end{problem}



\begin{problem}
Рассмотрим модель $y_i=\beta_1 + \beta_2 x_i + \e_i$, где $y=(1,3,3)'$ и $x=(0,0,1)$.
\begin{enumerate}
\item Найдите $\hb$ и $\hVar(\hb)$ в предположении гомоскедастичности
\item Найдите оценку Уайта ковариационной матрицы $\hVar_{White}(\hb)$
\item Найдите оценку HC3 ковариационной матрицы $\hVar_{HC3}(\hb)$
\item Найдите оценку взвешенного МНК, $\hb_{WLS}$, и оценку её ковариационной матрицы, $\hVar(\hb_{WLS})$, в предположении, что $\Var(\e_i|X)=\sigma^2 \cdot ( 4 + 5x_i)$
\end{enumerate}


\begin{sol}
Решение средствами пакета \verb|sandwich|
<<>>=
df <- data.frame(y=c(1, 3, 3), x=c(0, 0, 1))

model <- lm(data = df, y~x)
coef(model)
# residuals
resid(model)


vcov(model)
vcovHC(model) # should fail
vcovHC(model, type = "HC0" )
# help(vcovHC)
@

Решение с ручным подсчётом матриц
<<>>=
y <- c(1, 3, 3)
X <- cbind(rep(1, 3), c(0, 0, 1))

hat_beta <- solve(t(X) %*% X) %*% t(X) %*% y
hat_beta


# by hand Var(hat_beta)
y_hat <- X %*% hat_beta
e_hat <- y - y_hat
RSS <- sum((y - y_hat)^2)
vcov_ols <- RSS/(3 - 2) * solve(t(X) %*% X)
vcov_ols

# crossprod(X) is just synonym for t(X) %*% X

H <- X %*% solve(crossprod(X)) %*% t(X)
H

diag(H)

S_hat_white <- diag(as.vector(e_hat^2))
S_hat_HC3 <- diag(as.vector(e_hat^2)/(1 - diag(H))^2)
S_hat_HC3 # look at the problem

# vcov White
solve(crossprod(X)) %*% t(X)  %*% S_hat_white %*% X %*% solve(crossprod(X))
# vcov HC3
solve(crossprod(X)) %*% t(X)  %*% S_hat_HC3 %*% X %*% solve(crossprod(X))
@
\end{sol}
\end{problem}



\begin{problem}
Имеются наблюдения

<<results="asis">>=
x <- c(0, 2, 2)
y <- c(-1, 1, 0)
df <- data.frame(x = x, y = y)
xtable(df)
@

Предположим, что $y_i=\beta_1 + \beta_2 x_i + \e_i$ и регрессоры неслучайные.

Для удобства приведены матрицы

\[
X'X=
<<results='asis', echo=FALSE>>=
X <- model.matrix(y ~ x, data=df)
XX <- t(X) %*% X
XX_inv <- solve(XX)
XX_X <- XX_inv %*% t(X)
H <- X %*% XX_X
xmatrix(XX)
@
, \;
(X'X)^{-1}=
<<results='asis', echo=FALSE>>=
xmatrix(XX_inv)
@
, \;
(X'X)^{-1}X'=
<<results='asis', echo=FALSE>>=
xmatrix(XX_X)
@
\]
А также:
\[
H=X(X'X)^{-1}X'=
<<results='asis', echo=FALSE>>=
xmatrix(H)
@
, \;
y'y=
<<results='asis', echo=FALSE>>=
xmatrix(t(y) %*% y)
@
, \;
X'y=
<<results='asis', echo=FALSE>>=
xmatrix(t(X) %*% y)
@
\]

\begin{enumerate}
\item Найдите оценки коэффициентов с помощью МНК и оценку их ковариационной матрицы предполагая независимость и гомоскедастичность ошибок.

\item Найдите две робастных к гетероскедастичности оценки ковариационной матрицы оценок МНК: в форме Уайта и в форме HC3.

\item  Предположим, что дисперсии первых двух наблюдений равны, а дисперсия третьего наблюдения в 4 раза больше. Найдите оценки взвешенного МНК и оценку их ковариационной матрицы.

\item  Предположим, что дисперсии первых двух наблюдений равны, а дисперсия третьего наблюдения в 4 раза больше. Также предположим, что $\Corr(\e_2, \e_3)=0.5$, а остальные корреляции между ошибками равны 0. Найдите оценки обобщенного МНК и оценку их ковариационной матрицы.

\item Аккуратно объясните, с какой целью используются робастные оценки ковариационной матрицы, например, оценка Уайта. Ответ «для борьбы с гетероскедастичностью» не оценивается. Как конкретно и при каких условиях можно использовать робастные оценки ковариационной матрицы?

\item Аккуратно объясните, с какой целью вместо МНК используется обобщенный МНК. Ответ «для борьбы с гетероскедастичностью» не оценивается. Что конкретно даёт обобщенный МНК, чего не даёт обычный МНК и при каких условиях?
\end{enumerate}



\begin{sol}

\end{sol}
\end{problem}



\begin{problem}
Предположим, что $y_i$ независимы, нормально распределены и имеют одинаковое математическое ожидание $\mu$.

\begin{enumerate}
\item  Предложите эффективную оценку для $\mu$, предполагая, что $y_i$ гомоскедастичны
\item  Предложите эффективную оценку для $\mu$, предполагая, что $Var(y_i)=1/i^2$
\end{enumerate}


\begin{sol}
при гомоскедастичности $\hat{\mu}=\bar{y}$, при гетероскедастичности
\[
\hat{\mu}=\frac{\sum \tilde{x}_i \tilde{y}_i}{\sum \tilde{x}_i^2}=\frac{\sum i^2\cdot y_i}{\sum i^2}
\]
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим модель со стохастическими регрессорами $y=X\b + \e$. При этом $\E(\e|X)=0$, как и положено, однако ошибки $\e$ хитро зависят друг от друга, и поэтому $\Var(\e|X)$ есть некоторая известная недиагональная матрица $V$. Несмотря на нарушение предпосылок теоремы Гаусса-Маркова Чак Норрис использует обычный МНК для получения оценок~$\hb$.

Найдите $\E(\hb|X)$, $\Var(\hb|X)$ и $\Cov(\hy, \he | X)$
\begin{sol}
$\E(\hb|X)=\b$
\end{sol}
\end{problem}





\begin{problem}
Рассмотрим модель $y_i = 2 + 3x_i + \e_i$. Предположим, что $x_i \sim \cN(5;9)$ и $\e_i \sim \cN(0;1)$ и независимы. Исследователь Вениамин утверждает, что эту модель можно записать в виде
\[
y_i^* = \beta_1 + \beta_2 x_i^* + \e_i^*,
\]
где $y_i^*=\exp(y_i)$, $x_i^*=\exp(x_i)$ и регрессор $x_i^*$ некоррелирован с ошибкой $\e_i^*$.

Если Вениамин прав, то найдите:

\begin{enumerate}
\item Коэффициенты $\beta_1$ и $\beta_2$
\item Условную по $x_i^*$ и безусловную дисперсию ошибок $\e_i^*$
\item Почему при гетероскедастичности иногда помогает переход к логарифмам?
\end{enumerate}

Подсказка: Если величина $Z$ имеет нормальное распределение $\cN(\mu, \sigma^2)$, то случайная величина $W=\exp(Z)$ имеет лог-нормальное распределение и $\E(W)=\exp(\mu + \sigma^2/2)$, $\Var(W)=(\exp(\sigma^2)-1)\exp(2\mu+\sigma^2)$.


\begin{sol}

\end{sol}
\end{problem}

\Closesolutionfile{solution_file}
