\Opensolutionfile{solution_file}[solutions/sols_040]
% в квадратных скобках фактическое имя файла

\chapter{МНК с матрицами и вероятностями}


\begin{problem}
Пусть $y = X\beta + \e$ — регрессионная модель.
\begin{enumerate}
\item Сформулируйте теорему Гаусса-Маркова.
\item Верно ли, что оценка $\hb = (X'X)^{-1}X'y$ несмещённая?
\item В условиях теоремы Гаусса-Маркова найдите ковариационную матрицу $\hb$.
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item
В случае нестохастических регрессоров:

Пусть дана регрессионная модель $y=X\beta+\e$ с $k$ регрессорами, включая свободный член, и $n$ наблюдениями, и
\begin{enumerate}
\item регрессионная модель правильно специфицирована;
\item $\rk(X)=k$;
\item $X$ не являются стохастическими;
\item $\E(\e)=0$;
\item $\Var(\e)=\sigma^2 I$;
\end{enumerate}
то $\hat\beta=(X'X)^{-1}X'y$ являются лучшими оценками в классе линейных несмещённых оценок, то есть BLUE-оценками.

В случае стохастических регрессоров:

Пусть дана регрессионная модель $y=X\beta+\e$ с $k$ регрессорами, включая свободный член, и $n$ наблюдениями, и
\begin{enumerate}
\item регрессионная модель правильно специфицирована;
\item $\rk(X)=k$;
\item $\E(\e|X)=0$;
\item $\Var(\e|X)=\sigma^2 I$;
\end{enumerate}
то $\hat\beta=(X'X)^{-1}X'y$ являются лучшими оценками в классе линейных несмещённых оценок, то есть BLUE-оценками.

\item
Да, верно. В самом деле,
\begin{multline*}
\E(\hat\beta)=\E((X'X)^{-1}X'y)=(X'X)^{-1}X'\E(y)=(X'X)^{-1}X'\E(X\beta+\e)=\\
=(X'X)^{-1}X'X\E(\beta)+(X'X)^{-1}X'\underbrace{\E(\e)}_{=0}=\beta
\end{multline*}


\item
\begin{multline*}
\Var(\hat\beta)=\Var((X'X)^{-1}X'y)=(X'X)^{-1}X'\Var(y)((X'X)^{-1}X')'=\\
=(X'X)^{-1}X'\Var(X\beta+\e)X(X'X)^{-1}
\end{multline*}
Так как $\beta$ является константой, то $\Var(X\beta)=0$. Тогда
\begin{multline*}
\Var(\hat\beta)=(X'X)^{-1}X'\Var(X\beta+\e)X(X'X)^{-1}=\\=(X'X)^{-1}X'\Var(\e)X(X'X)^{-1}=
(X'X)^{-1}X'\sigma^2X(X'X)^{-1}=\\=\sigma^2(X'X)^{-1}X'X(X'X)^{-1}=\sigma^2(X'X)^{-1}
\end{multline*}
\end{enumerate}

\end{sol}
\end{problem}


\begin{problem}
Пусть $y = X\beta + \e$ — регрессионная модель и $\tilde{\beta} = ((X'X)^{-1}X'+ A)y$ — несмещённая оценка вектора неизвестных параметров $\beta$. Верно ли, что $AX=0$?


\begin{sol}
Да, в общем случае (кроме случая $\beta=0$ это верно. Так как $\tilde\beta$ является несмещённой, то $\E(\tilde\beta)=\beta$.
\begin{multline*}
\E(\tilde\beta)=\E(((X'X)^{-1}X'+A)y)=\E[((X'X)^{-1}X'+A)(X\beta+\e)]=\\=
\E(((X'X)^{-1}X'+A)X\beta)+((X'X)^{-1}X'+A)\underbrace{\E(\e)}_{=0}=\\
=\E((X'X)^{-1}X'X\beta+AX\beta)=\beta+AX\beta
\end{multline*}
\[\E(\tilde\beta)=\beta\]
\[\beta+AX\beta=\beta\]
\[AX\beta=0\]
Значит, либо $AX=0$, либо $\beta=0$.

Заметим, что при $\beta=0$ при любом $AX$ оценка $\tilde\beta$ будет несмещённой.
\end{sol}
\end{problem}





\begin{problem}
Пусть $y = X\beta + \e$ — регрессионная модель, $X = \begin{pmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 1 \end{pmatrix}$, $y = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}$, $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix}$, $\e = \begin{pmatrix} \e_1 \\ \e_2 \\ \e_3 \end{pmatrix}$, $\E(\e)$ = 0, $\Var(\e) = \sigma^2 I$. Найдите коэффициент корреляции $\Corr(\hb_1,\hb_2)$.
<<"040_01", echo=FALSE>>=
X <- matrix(as.integer(c(1, 1, 1, 0, 1, 1)), nrow = 3,
  byrow = FALSE, dimnames = NULL)
y <- c(1, 2, 3)
@


\begin{sol}
\[X'X=\left(\begin{array}{cc}
3 & 2 \\
2 & 2
\end{array}\right) \]
\[(X'X)^{-1}=\left(\begin{array}{cc}
1 & -1 \\
-1 & 1.5
\end{array}\right) \]

\[\Var(\hat\beta_1)=\Var(\hat\beta)_{[1,1]}=\sigma^2\]
\[\Var(\hat\beta_2)=\Var(\hat\beta)_{[2,2]}=1.5\sigma^2\]
\[\Cov(\hat\beta_1,\hat\beta_2)=\Var(\hat\beta)_{[1,2]}=-\sigma^2\]

\[\corr(\hat\beta_1,\hat\beta_2)=\frac{\Cov(\hat\beta_1,\hat\beta_2)}{\sqrt{\Var(\hat\beta_1)}\Var(\hat\beta_2)}=\\
\\\frac{\sigma^2}{\sigma\cdot\sqrt{1.5}\sigma}=\sqrt{\frac{2}{3}}=\frac{\sqrt6}{3}\]
Показательно, что значения $y$ здесь не используются.

Ответ: $\frac{\sqrt6}{3}$.
\end{sol}
\end{problem}



\begin{problem}
Пусть $y = X\beta + \e$ — регрессионная модель, где $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$. Пусть $Z = XD$, где $D = \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}$. Рассмотрите «новую» регрессионную модель $y = Z\alpha + u$, где $\alpha = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}$. Определите, как выражаются «новые» МНК-коэффициенты через «старые».
<<"040_02", echo=FALSE, results='asis'>>=
alpha <- matrix(as.integer(c(1, 0, 0, 1, 1, 0, 0, 1, 1)),
  nrow = 3, byrow = FALSE, dimnames = NULL)
@


\begin{sol}
\[\hat\beta=(X'X)^{-1}X'y\]
\[X'X\hat\beta=X'y\]
\[\hat\alpha=(D'X'XD)^{-1}D'X'y\]
поэтому
\[\hat\alpha=(D'X'XD)^{-1}D'X'y=(D'X'XD)^{-1}D'X'X\hat\beta=(D'X'XD)^{-1}D'X'XDD^{-1}\hat\beta=D^{-1}\hat\beta\]
\[D^{-1}=\left(\begin{array}{ccc}
1 & -1 & 1\\
0 & 1 & -1\\
0 & 0 & 1
\end{array}\right)\]

\[\begin{cases}
\hat\alpha_1=\beta_1-\beta_2+\beta_3\\
\hat\alpha_2=\beta_2-\beta_3\\
\hat\alpha_3=\beta_3
\end{cases}\]
\end{sol}
\end{problem}



\begin{problem}
Пусть $y = X\beta + \e$ — регрессионная модель, где $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$. Пусть $Z = XD$, где $D = \begin{pmatrix} 1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}$. Рассмотрите «новую» регрессионную модель $y = Z\alpha + u$, где $\alpha = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}$. Определите, как выражаются «новые» МНК-коэффициенты через «старые».
<<"040_03", echo=FALSE, results='asis'>>=
alpha2 <- matrix(as.integer(c(1, 0, 0, 1, 1, 0, 1, 1, 1)), nrow = 3,
  ncol = 3, byrow = FALSE, dimnames = NULL)
@


\begin{sol}
\[\hat\beta=(X'X)^{-1}X'y\]
\[X'X\hat\beta=X'y\]
\[\hat\alpha=(D'X'XD)^{-1}D'X'y\]
поэтому
\[\hat\alpha=(D'X'XD)^{-1}D'X'y=(D'X'XD)^{-1}D'X'X\hat\beta=(D'X'XD)^{-1}D'X'XDD^{-1}\hat\beta=D^{-1}\hat\beta\]
\[D^{-1}=\left(\begin{array}{ccc}
1 & -1 & 0\\
0 & 1 & -1\\
0 & 0 & 1
\end{array}\right)\]

\[\begin{cases}
\hat\alpha_1=\beta_1-\beta_2\\
\hat\alpha_2=\beta_2-\beta_3\\
\hat\alpha_3=\beta_3
\end{cases}\]
\end{sol}
\end{problem}



\begin{problem}
Пусть $y = X\beta + \e$ — регрессионная модель, где $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$. Пусть $Z = XD$, где $D = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}$. Рассмотрите «новую» регрессионную модель $y = Z\alpha + u$, где $\alpha = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}$. Определите, как выражаются «новые» МНК-коэффициенты через «старые».
<<"040_04", echo=FALSE, results='asis'>>=
alpha3 <- matrix(as.integer(c(1, 0, 0, 0, 1, 0, 0, 1, 1)), nrow = 3,
  ncol = 3, byrow = FALSE, dimnames = NULL)
@


\begin{sol}
\[\hat\beta=(X'X)^{-1}X'y\]
\[X'X\hat\beta=X'y\]
\[\hat\alpha=(D'X'XD)^{-1}D'X'y\]
поэтому
\[\hat\alpha=(D'X'XD)^{-1}D'X'y=(D'X'XD)^{-1}D'X'X\hat\beta=(D'X'XD)^{-1}D'X'XDD^{-1}\hat\beta=D^{-1}\hat\beta\]
\[D^{-1}=\left(\begin{array}{ccc}
1 & 0 & 0\\
0 & 1 & -1\\
0 & 0 & 1
\end{array}\right)\]

\[\begin{cases}
\hat\alpha_1=\beta_1\\
\hat\alpha_2=\beta_2-\beta_3\\
\hat\alpha_3=\beta_3
\end{cases}\]
\end{sol}
\end{problem}



\begin{problem}
Пусть $y = X\beta + \e$ — регрессионная модель. Верно ли, что $\he'\hy=0$ и $\hy'\he=0$?

\begin{sol}
Да, верно.
\begin{multline*}
\he'\hy=(y-\hy)'\hy=(y-X\hat\beta)'X\hat\beta=(y-X(X'X)^{-1}X'y)'X(X'X)^{-1}X'y=\\=
((I-X(X'X)^{-1}X')y)'X(X'X)^{-1}X'y=y'(I-X(X'X)^{-1}X')X(X'X)^{-1}X'y=\\
=y'(X(X'X)^{-1}X'-X(X'X)^{-1}X'X(X'X)^{-1}X')y=y'(X(X'X)^{-1}X'-X(X'X)^{-1}X')y=0
\end{multline*}

Да, верно.
\[\hy'\he=(\he'\hy)'=0\]
так как выше доказано, что $he'\hy=0$.
\end{sol}
\end{problem}


\begin{problem}
Пусть $y = X\beta + \e$ — регрессионная модель, где $\E(\e)=0$, $\Var(\e)=\sigma_{\e}^2 I$. Пусть $A$ — неслучайная матрица размера $k \times k$, $\det(A) \not= 0.$ Совершается преобразование регрессоров по правилу $Z=XA$. В преобразованных регрессорах уравнение выглядит так: $y = Z\gamma + u$, где $\E(u)=0$, $\Var(u)=\sigma_{u}^2 I.$

\begin{enumerate}
\item Как связаны между собой МНК-оценки $\hb$ и $\hat{\gamma}$?
\item Как связаны между собой векторы остатков регрессий?
\item Как связаны между собой прогнозные значения, полученные по двум регрессиям?
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item
\begin{multline*}
\hat{\gamma} = (Z'Z)^{-1}Z'y = A^{-1}(X'X)^{-1}(A')^{-1}A'X'y =\\
 A^{-1}(X'X)^{-1} X'y = A^{-1}\hb
\end{multline*}
\item $\hat{u} = y - Z\hat{\gamma} = y - XAA^{-1}\hb = y - X\hb = \he$
\item Пусть $z^0 = \begin{pmatrix} 1 & z_1^0 & \dots & z_{k-1}^0 \end{pmatrix}$ — вектор размера $1 \times k$ и $x^0 = \begin{pmatrix} 1 & x_1^0 & \dots & x_{k-1}^0 \end{pmatrix}$ — вектор размера $1 \times k$. Оба эти вектора представляют собой значения факторов. Тогда $z^0 = x^0 A$ и прогнозное значение для регрессии с преобразованными факторами равно $z^0 \hat{\gamma} = x^0 AA^{-1} \hb = x^0 \hb$ прогнозному значению для регрессии с исходными факторами.
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
Рассмотрим оценку вида $\tilde{\beta} = ((X'X)^{-1} + \gamma I)X'y$ для вектора коэффициентов регрессионного уравнения $y = X\beta + \e$, удовлетворяющего условиям классической регрессионной модели. Найдите $\E(\tilde{\beta})$ и $\Var(\tilde{\beta}).$


\begin{sol}
\begin{enumerate}
\item
\begin{multline*}
\E(\tilde{\beta}) = ((X'X)^{-1} + \gamma I)X'\E(y) = \\
 ((X'X)^{-1} + \gamma I)X'X\beta = \beta + \gamma X'X\beta
\end{multline*}
\item
\begin{multline*}
\Var(\tilde{\beta}) = \Var(((X'X)^{-1} + \gamma I)X'y) = \\
 \Var(((X'X)^{-1} + \gamma I)X'\e) = \\
 (((X'X)^{-1} + \gamma I)X')\Var(\e)(((X'X)^{-1} + \gamma I)X')'=  \\
  (((X'X)^{-1} + \gamma I)X')\sigma_{\e}^2 I(((X'X)^{-1} + \gamma I)X')'= \\
  \sigma_{\e}^2((X'X)^{-1} + \gamma I)X'X((X'X)^{-1} + \gamma I) = \\
  \sigma_{\e}^2((X'X)^{-1} + \gamma I)(I + \gamma X'X) =\\
   \sigma_{\e}^2((X'X)^{-1} + 2\gamma I + \gamma ^2 X'X)
\end{multline*}
\end{enumerate}
\end{sol}
\end{problem}




\begin{problem}
Верно ли, что при невырожденном преобразовании факторов $R^2$ не меняется? А именно, пусть заданы две регрессионные модели: $y = X\beta + \e$ и $y = Z\alpha + u$, где $y$ — вектор размера $n \times 1$, $X$ и $Z$ — матрицы размера $n \times k$, $\beta$ и $\alpha$ — вектора рамзера $k \times 1$, $\e$ и $u$ — вектора размера $n \times 1$, а также $Z=XD$, $\det(D) \not= 0.$ Верно ли, что коэффициенты детерминации представленных выше моделей равны между собой?

\begin{sol}
Да, верно.
\[R^2=\frac{ESS}{TSS}=\frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}\]
\[TSS=\sum_{i=1}^n(y_i-\bar y)^2=y'\left(I-\frac{\overrightarrow{1}'}{\overrightarrow{1}^2}\right)y\]
не зависит от $X$.
\[RSS_a=\sum_{i=1}^n(y_i-\hy_i)^2=y'(I-X(X'X)^{-1}X')y\]
\[RSS_b=y'(I-Z(Z'Z)^{-1}Z')y=y'(I-XD(D'X'XD)^{-1}D'X')y=\]
так как $D$ является квадратной и невырожденной, то используя формулу $(AB)^{-1}=B^{-1}A^{-1}$, получим:
\begin{multline*}
RSS_b=y'(I-XD(D'X'XD)^{-1}D'X')y=y'(I-XDD^{-1}(X'X)^{-1}D'^{-1}D'X')y=\\=y'(I-X(X'X)^{-1}X')y=RSS_a
\end{multline*}

Значит,
\[R^2_a=1-\frac{RSS_a}{TSS_a}=1-\frac{RSS_b}{TSS_b}=R^2_b\]
\end{sol}
\end{problem}



\begin{problem}
Верно ли, что при невырожденном преобразовании факторов $RSS$ не меняется. А именно, пусть заданы две регрессиионные модели: $y = X\beta + \e$ и $y = Z\alpha + u$, где $y$ — вектор размера $n \times 1$, $X$ и $Z$ — матрицы размера $n \times k$, $\beta$ и $\alpha$ — вектора рамзера $k \times 1$, $\e$ и $u$ — вектора размера $n \times 1$, а также $Z=XD$, $\det(D) \not= 0.$ Верно ли, что сумма квадратов остатков в представленных выше моделях равны между собой?

\begin{sol}
Да, верно.
\[RSS_1=\sum_{i=1}^n(y_i-\hy_i)^2=y'(I-X(X'X)^{-1}X')y\]
\[RSS_2=y'(I-Z(Z'Z)^{-1}Z')y=y'(I-XD(D'X'XD)^{-1}D'X')y=\]
так как $D$ является квадратной и невырожденной, то используя формулу $(AB)^{-1}=B^{-1}A^{-1}$, получим:
\begin{multline*}
RSS_2=y'(I-XD(D'X'XD)^{-1}D'X')y=y'(I-XDD^{-1}(X'X)^{-1}D'^{-1}D'X')y=\\=y'(I-X(X'X)^{-1}X')y=RSS_1
\end{multline*}
\end{sol}
\end{problem}



\begin{problem}
Пусть регрессионная модель $y_i = \beta_1 + \beta_2 x_{i2} + \beta_3 x_{i3} + \e_i$, $i = 1, \ldots, n$, задана в матричном виде при помощи уравнения $y = X \beta + \e$, где $\beta =  \begin{pmatrix}
\beta_1 & \beta_2 & \beta_3\\
\end{pmatrix} '$. Известно, что $\E \e = 0$ и $\Var (\e) = 4 \cdot I$. Известно также, что:

$y =  \begin{pmatrix}
1 \\
2 \\
3 \\
4 \\
5 \\
\end{pmatrix} $, $X =  \begin{pmatrix}
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 1 & 0 \\
1 & 1 & 1 \\
\end{pmatrix} $

Для удобства расчётов ниже приведены матрицы:

$X' X =  \begin{pmatrix}
5 & 3 & 1 \\
3 & 3 & 1 \\
1 & 1 & 1 \\
\end{pmatrix} $ и $(X' X)^{-1} =  \begin{pmatrix}
0.5 & -0.5 & 0 \\
-0.5 & 1 & -0.5 \\
0 & -0.5 & 1.5 \\
\end{pmatrix} $.

Найдите:
\begin{enumerate}
\item $\Var (\e_1)$;
\item $\Var (\beta_1)$;
\item $\Var (\hb_1)$;
\item $\hVar(\hb_1)$;
\item $\E (\hb_1^2) - \beta_1^2$;
\item $\Cov (\hb_2, \hb_3)$;
\item $\hCov(\hb_2, \hb_3)$;
\item $\Var (\hb_2 - \hb_3)$;
\item $\hVar(\hb_2 - \hb_3)$;
\item $\Var (\beta_2 - \beta_3)$;
\item $\Corr (\hb_2, \hb_3)$;
\item $\hCorr(\hb_2, \hb_3)$;
\item $\E (\hs^2)$;
\item $\hs^2$.
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item $\Var(\e_1)=\Var(\e)_{(1,1)}=4\cdot I_{(1,1)}=4$
\item $\Var(\beta_1)=0$, так как $\beta_1$ — детерминированная величина.
\item $\Var(\hb_1)=\sigma^2(X'X)^{-1}_{(1,1)}=0.5\sigma^2=0.5\cdot 4=2$
\item $\hVar(\hb_1)=\hat\sigma^2(X'X)^{-1}_{(1,1)}=0.5\hat\sigma^2_{(1,1)}=0.5\frac{RSS}{5-3}=0.25RSS=0.25y'(I-X(X'X)^{-1}X')y=0.25\cdot 1=0.25$

$\hat\sigma^2=\frac{RSS}{n-k}=\frac12$.

\item Так как оценки МНК являются несмещёнными, то $\E(\hb)=\beta$, значит:
\[
\E(\hb_1)-\beta_1^2=\E(\hb_1)-(\E(\hb_1))^2=\hVar(\hb_1)=0.25
\]

\item $\Cov(\hb_2,\hb_3)=\sigma^2(X'X)^{-1}_{(2,3)}=4\cdot\left(-\frac12\right)=-2$
\item $\hCov(\hb_2,\hb_3)=\hVar(\hat\beta)_{(2,3)}=\hat\sigma^2(X'X)^{-1}_{(2,3)}=\frac{1}{2}\cdot\left(-\frac12\right)=-\frac14$

\item $\Var(\hb_2-\hb_3)=\Var(\hb_2)+\Var(\hb_3)+2\Cov(\hb_2,\hb_3)=\sigma^2((X'X)^{-1}_{(2,2)}+(X'X)^{-1}_{(3,3)}+2(X'X)^{-1}_{(2,3)}=4(1+1.5+2\cdot(-0.5))=6$

\item $\hVar(\hb_2-\hb_3)=\hVar(\hb_2)+\hVar(\hb_3)+2\hCov(\hb_2,\hb_3)=\hat\sigma^2((X'X)^{-1}_{(2,2)}+(X'X)^{-1}_{(3,3)}+2(X'X)^{-1}_{(2,3)}=\frac{1}{2}\cdot1.5=0.75$

\item $\Var(\beta_2-\beta_3)=0$

\item $\corr(\hb_2,\hb_3)=\frac{\Cov(\hb_2,\hb_3)}{\sqrt{\Var(\hb_2)\Var(\hb_3)}}=\frac{-2}{\sqrt{4\cdot6}}=-\frac{\sqrt6}{6}$

\item $\hCorr(\beta_2,\beta_3)=\frac{\hCov(\hb_2,\hb_3)}{\sqrt{\hVar(\hb_2)\hVar(\hb_3)}}=\frac{-\frac14}{\sqrt{\frac12\cdot\frac34}}=-\frac{\sqrt6}{6}$

\item $(n-k)\frac{\hat\sigma^2}{\sigma^2}\sim\chi^2_{n-k}$.
\[
\E\left((n-k)\frac{\hat\sigma^2}{\sigma^2}\right)=n-k
\]
\[
\E\left(\frac{\hat\sigma^2}{2}\right)=1
\]
\[
\E(\hat\sigma^2)=2
\]

\item $\hat\sigma^2=\frac{RSS}{n-k}=\frac12$

\end{enumerate}

\end{sol}
\end{problem}



% 4.13
\begin{problem}
Пусть $y_i = \beta_1 + \beta_2 x_{i2} + \beta_3 x_{i3} + \e_i$ — регрессионная модель, где $X = \begin{pmatrix} 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 1 \end{pmatrix}$, $y = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \\ 5 \end{pmatrix}$, $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$, $\e = \begin{pmatrix} \e_1 \\ \e_2 \\ \e_3 \\ \e_4 \\ \e_5  \end{pmatrix}$, ошибки $\e_i$ независимы и нормально распределены с $\E(\e)$ = 0, $Var(\e) = \sigma^2 I$. Для удобства расчётов даны матрицы: $X'X = \begin{pmatrix} 5 & 2 & 1 \\ 2 & 2 & 1\\ 1 & 1 & 1 \end{pmatrix}$ и $(X'X)^{-1}= \begin{pmatrix} 0.3333 & -0.3333 & 0.0000 \\ -0.3333 & 1.3333 & -1.0000 \\ 0.0000 & -1.0000 & 2.0000 \end{pmatrix}$


<<"040_05", echo=FALSE, results='asis'>>=
x <- matrix(as.integer(c(1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1)),
  nrow = 5, ncol = 3, byrow = FALSE, dimnames = NULL)
y <- matrix(as.integer(c(1, 2, 3, 4, 5)), nrow = 5,
  ncol = 1, byrow = FALSE, dimnames = NULL)
xx <- matrix(as.integer(c(5, 2, 1, 2, 2, 1, 1, 1, 1)),
  nrow = 3, ncol = 3, byrow = FALSE, dimnames = NULL)
x_x <- solve(xx)
@


\begin{enumerate}
\item Укажите число наблюдений.
\item Укажите число регрессоров в модели, учитывая свободный член.
\item Найдите $TSS = \sum_{i=1}^n (y_i - \bar y)^2$.
\item Найдите $RSS = \sum_{i=1}^n (y_i - \hy_i)^2$.
\item Методом МНК найдите оценку для вектора неизвестных коэффициентов.
\item Чему равен $R^2$ в модели? Прокомментируйте полученное значение с точки зрения качества оценённого уравнения регрессии.
\item Сформулируйте основную и альтернативную гипотезы, которые соответствуют тесту на значимость переменной $x_2$ в уравнении регрессии.
\item Протестируйте на значимость переменную $x_2$ в уравнении регрессии на уровне значимости $10\%$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики.
\item Укажите распределение тестовой статистики при верной $H_0$.
\item Вычислите наблюдаемое значение тестовой статистики.
\item Укажите границы области, где основная гипотеза не отвергается.
\item Сделайте статистический вывод о значимости переменной $x_2$.
\end{enumerate}
\item Найдите $P$-значение, соответствующее наблюдаемому значению тестовой статистики $(T_{obs})$ из предыдущего пункта. На основе полученного $P$-значения сделайте вывод о значимости переменной $x_2$.
\item На уровне значимости $10\%$ проверьте гипотезу $H_0: \beta_2 = 1$ против альтернативной $H_a: \beta_2 \not= 1$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики.
\item Укажите распределение тестовой статистики при верной $H_0$.
\item Вычислите наблюдаемое значение тестовой статистики.
\item Укажите границы области, где основная гипотеза не отвергается.
\item Сделайте статистический вывод.
\end{enumerate}
\item На уровне значимости $10\%$ проверьте гипотезу $H_0: \beta_2 = 1$ против альтернативной $H_a: \beta_2 > 1$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики.
\item Укажите распределение тестовой статистики при верной $H_0$.
\item Вычислите наблюдаемое значение тестовой статистики.
\item Укажите границы области, где основная гипотеза не отвергается.
\item Сделайте статистический вывод.
\end{enumerate}
\item На уровне значимости $10\%$ проверьте гипотезу $H_0: \beta_2 = 1$ против альтернативной $H_a: \beta_2 < 1$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики.
\item Укажите распределение тестовой статистики при верной $H_0$.
\item Вычислите наблюдаемое значение тестовой статистики.
\item Укажите границы области, где основная гипотеза не отвергается.
\item Сделайте статистический вывод.
\end{enumerate}
\item Сформулируйте основную гипотезу, которая соответствует тесту на значимость регрессии «в целом».
\item На уровне значимости $5\%$ проверьте гипотезу о значимости регрессии «в целом»:
\begin{enumerate}
\item Приведите формулу для тестовой статистики.
\item Укажите распределение тестовой статистики при верной $H_0$.
\item Вычислите наблюдаемое значение тестовой статистики.
\item Укажите границы области, где основная гипотеза не отвергается.
\item Сделайте статистический вывод.
\end{enumerate}
\item Найдите $P$-значение, соответствующее наблюдаемому значению тестовой статистики $(T_{obs})$ из предыдущего пункта. На основе полученного $P$-значения сделайте вывод о значимости регрессии «в целом».
\item На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_2 + \beta_3 = 2$ против альтернативной $H_a: \beta_2 + \beta_3 \not= 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики.
\item Укажите распределение тестовой статистики при верной $H_0$.
\item Вычислите наблюдаемое значение тестовой статистики.
\item Укажите границы области, где основная гипотеза не отвергается.
\item Сделайте статистический вывод.
\end{enumerate}
\item На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_2 + \beta_3 = 2$ против альтернативной $H_a: \beta_2 + \beta_3 > 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики.
\item Укажите распределение тестовой статистики при верной $H_0$.
\item Вычислите наблюдаемое значение тестовой статистики.
\item Укажите границы области, где основная гипотеза не отвергается.
\item Сделайте статистический вывод.
\end{enumerate}
\item На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_2 + \beta_3 = 2$ против альтернативной $H_a: \beta_2 + \beta_3 < 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики.
\item Укажите распределение тестовой статистики при верной $H_0$.
\item Вычислите наблюдаемое значение тестовой статистики.
\item Укажите границы области, где основная гипотеза не отвергается.
\item Сделайте статистический вывод.
\end{enumerate}
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item $n = 5$
\item $k = 3$
\item $TSS = 10$
\item $RSS = 2$
\item $\hb = \begin{pmatrix} \hb_1 \\ \hb_2 \\ \hb_3 \end{pmatrix} = (X'X)^{-1}X'y = \begin{pmatrix} 2 \\ 2 \\ 1 \end{pmatrix}$
\item $R^2 = 1 - \frac {RSS}{TSS} = 0.8.$ $R^2$ высокий, построенная эконометрическая модель хорошо описывает данные
\item Основная гипотеза — $H_0: \beta_2 = 0$, альтернативная гипотеза — $H_a: \beta_2 \not= 0$
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 - \beta_2}{\sqrt{\Var(\hb_2)}} = \frac {\hb_2 - \beta_2}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hb_2 - 0}{\sqrt{\Var(\hb_2)}} = \frac {\hb_2 - 0}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-0}{\sqrt{{\frac{2}{5-3}}1.3333}} = 1.7321$
\item Нижняя граница $= -2.920$, верхняя граница $= 2.920$
\item Поскольку $T_{obs} = 1.7321$, что принадлежит промежутку от -2.920 до 2.920, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item $p-value(T_{obs}) = \P (|T|>|T_{obs}|) = 2F_{T}(|T_{obs}|)$, где $F_{T}(|T_{obs}|)$ — функция распределения $t-$распределения с $n - k = 5 - 3 = 2$ степенями свободы в точке $|T_{obs}|$. $p-value(T_{obs}) = 2tcdf(-|T_{obs}|, n - k) = 2tcdf(-1.7321,2) = 0.2253$. Поскольку $P$-значение превосходит уровень значимости $10\%$, то основная гипотеза — $H_0: \beta_2 = 0$ не может быть отвергнута
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 - \beta_2}{\sqrt{\Var(\hb_2)}} = \frac {\hb_2 - \beta_2}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hb_2 - 1}{\sqrt{\Var(\hb_2)}} = \frac {\hb_2 - 1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-1}{\sqrt{{\frac{2}{5-3}}1.3333}} = 0.8660$
\item Нижняя граница $= -2.920$, верхняя граница $= 2.920$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от -2.920 до 2.920, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 - \beta_2}{\sqrt{\Var(\hb_2)}} = \frac {\hb_2 - \beta_2}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hb_2 - 1}{\sqrt{\Var(\hb_2)}} = \frac {\hb_2 - 1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-1}{\sqrt{{\frac{2}{5-3}}1.3333}} = 0.8660$
\item Нижняя граница $= -\infty$, верхняя граница $= 1.8856$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $-\infty$ до $1.8856$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 - \beta_2}{\sqrt{\Var(\hb_2)}} = \frac {\hb_2 - \beta_2}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hb_2 - 1}{\sqrt{\Var(\hb_2)}} = \frac {\hb_2 - 1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-1}{\sqrt{{\frac{2}{5-3}}1.3333}} = 0.8660$
\item Нижняя граница $= -1.8856$, верхняя граница $= +\infty$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $-1.8856$ до $+\infty$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item Основная гипотеза — $H_0: \beta_2 = \beta_3 = 0$, альтернативная гипотеза — $H_a: |\beta_2| + |\beta_3| > 0$
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{R^2}{1 - R^2} \cdot \frac{n-k}{k}; n = 5; k = 3$
\item $T \sim F(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{R^2}{1 - R^2} \cdot \frac{n-k}{k} = \frac{0.8}{1 - 0.8} \cdot \frac{5-3}{2} = 4$
\item Нижняя граница $= 0$, верхняя граница $= 19$
\item Поскольку $T_{obs} = 4$, что принадлежит промежутку от $0$ до $19$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$. Следовательно, регрессия в целом незначима. Напомним, что $R^2 = 0.8$, то есть он высокий. Но при этом регрессия в целом незначима. Такой эффект может возникать при малом объёме выборки, например, таком, как в данной задаче
\end{enumerate}
\item $p-value(T_{obs}) = \P (|T|>|T_{obs}|) = 2F_{T}(|T_{obs}|)$, где $F_{T}(|T_{obs}|)$ — функция распределения $F-$распределения c $k = 3$ и $n - k = 5 - 3 = 2$ степенями свободы в точке $T_{obs}$. $p-value(T_{obs}) = 1 - fcdf(-|T_{obs}|, n - k) = 1 - fcdf(4,2) = 0.2$. Поскольку $P$-значение превосходит уровень значимости $10\%$, то основная гипотеза — $H_0: \beta_2 = \beta_3 = 0$ не может быть отвергнута. Таким образом, регрессия в целом незначима
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 + \hb_3 - (\beta_2 + \beta_3)}{\sqrt{\hVar(\hb_2 + \hb_3)}}$, где $\hVar(\hb_2 + \hb_3) = \hVar(\hb_2) + \hVar(\hb_3) + 2\hCov(\hb_2;\hb_3) = \hs^2 [(X'X)^{-1}]_{22} + 2\hs^2 [(X'X)^{-1}]_{23} + \hs^2 [(X'X)^{-1}]_{33}= \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33})$
\item $T \sim t(n-k); n = 5; k = 3$
\item $\hVar(\hb_2 + \hb_3) = \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33}) = \frac{2}{5 - 3} (1.3333 + 2(-1.0000) + 2.0000) = 1.3333.$ Тогда $T_{obs} = \frac{\hb_2 + \hb_3 - 2}{\sqrt{\hVar(\hb_2 + \hb_3)}} = \frac{2 + 1 - 2}{\sqrt{1.3333}} = 0.8660$
\item Нижняя граница $= - 4.3027$, верхняя граница $= 4.3027$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $- 4.3027$ до $4.3027$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 + \hb_3 - (\beta_2 + \beta_3)}{\sqrt{\hVar(\hb_2 + \hb_3)}}$, где $\hVar(\hb_2 + \hb_3) = \hVar(\hb_2) + \hVar(\hb_3) + 2\hCov(\hb_2;\hb_3) = \hs^2 [(X'X)^{-1}]_{22} + 2\hs^2 [(X'X)^{-1}]_{23} + \hs^2 [(X'X)^{-1}]_{33}= \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33})$
\item $T \sim t(n-k); n = 5; k = 3$
\item $\hVar(\hb_2 + \hb_3) = \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33}) = \frac{2}{5 - 3} (1.3333 + 2(-1.0000) + 2.0000) = 1.3333.$ Тогда $T_{obs} = \frac{\hb_2 + \hb_3 - 2}{\sqrt{\hVar(\hb_2 + \hb_3)}} = \frac{2 + 1 - 2}{\sqrt{1.3333}} = 0.8660$
\item Нижняя граница $= - \infty$, верхняя граница $= 2.9200$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $- \infty$ до $2.9200$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 + \hb_3 - (\beta_2 + \beta_3)}{\sqrt{\hVar(\hb_2 + \hb_3)}}$, где $\hVar(\hb_2 + \hb_3) = \hVar(\hb_2) + \hVar(\hb_3) + 2\hCov(\hb_2;\hb_3) = \hs^2 [(X'X)^{-1}]_{22} + 2\hs^2 [(X'X)^{-1}]_{23} + \hs^2 [(X'X)^{-1}]_{33}= \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33})$
\item $T \sim t(n-k); n = 5; k = 3$
\item $\hVar(\hb_2 + \hb_3) = \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33}) = \frac{2}{5 - 3} (1.3333 + 2(-1.0000) + 2.0000) = 1.3333.$ Тогда $T_{obs} = \frac{\hb_2 + \hb_3 - 2}{\sqrt{\hVar(\hb_2 + \hb_3)}} = \frac{2 + 1 - 2}{\sqrt{1.3333}} = 0.8660$
\item Нижняя граница $= - 2.9200$, верхняя граница $= + \infty$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $-  2.9200$ до $+ \infty$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$
\end{enumerate}
\end{enumerate}
\end{sol}
\end{problem}




% 4.14
\begin{problem}
Пусть $y = X\beta + \e$ — регрессионная модель, где $X = \begin{pmatrix} 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 1 \end{pmatrix}$, $y = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \\ 5 \end{pmatrix}$, $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$, $\e = \begin{pmatrix} \e_1 \\ \e_2 \\ \e_3 \\ \e_4 \\ \e_5  \end{pmatrix}$, $\E(\e)$ = 0, $Var(\e) = \sigma^2 I$.

<<"040_06", echo=FALSE, results='asis'>>=
x <- matrix(as.integer(c(1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1)),
  nrow = 5, ncol = 3, byrow = FALSE, dimnames = NULL)
y <- matrix(as.integer(c(1, 2, 3, 4, 5)), nrow = 5,
  ncol = 1, byrow = FALSE, dimnames = NULL)
@

На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_1 + \beta_2 = 2$ против альтернативной $H_a: \beta_1 + \beta_2 \not= 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики.
\item Укажите распределение тестовой статистики  при верной $H_0$.
\item Вычислите наблюдаемое значение тестовой статистики.
\item Укажите границы области, где основная гипотеза не отвергается.
\item Сделайте статистический вывод.
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item $T = \frac{\hb_1 + \hb_2 - (\beta_1 + \beta_2)}{\sqrt{\hVar(\hb_1 + \hb_2)}}$, где $\hVar(\hb_1 + \hb_2) = \hVar(\hb_1) + \hVar(\hb_2) + 2\hCov(\hb_1;\hb_2) = \hat{\sigma}^2 [(X'X)^{-1}]_{11} + 2\hat{\sigma}^2 [(X'X)^{-1}]_{12} + \hat{\sigma}^2 [(X'X)^{-1}]_{22}= \frac{RSS}{n - k}([(X'X)^{-1}]_{11} + 2[(X'X)^{-1}]_{12} + [(X'X)^{-1}]_{22}), \beta_1 + \beta_2=2$
\item $T \sim t_{n-k}; n = 5; k = 3$
\item Смотри матрицы в номере 4.12. $\hb=(X'X)^{-1}X'y=(1.5\;2.0\;1.5)'$. $\hVar(\hb_1 + \hb_2) =\frac12(0.5+1+2\cdot(-0.5))=\frac14$. $T=\frac{1.5+2-2}{\sqrt{\frac14}}=3$.
\item Так как проверяется знак «равно» в гипотезе, то нижняя граница $-\infty$, а верхняя граница $+\infty$.
\item $t_{0.95,2}=2.9199<T$, значит, гипотеза отвергается на уровне значимости 5\%.
\end{enumerate}
\end{sol}
\end{problem}



% 4.15
\begin{problem}
По 13 наблюдениям Вася оценил модель со свободным членом, пятью количественными регрессорами и двумя качественными. Качественные регрессоры Вася правильно закодировал с помощью дамми-переменных. Одна качественная переменная принимала четыре значения, другая — пять.

\begin{enumerate}
\item Найдите $RSS$, $R^2$.
\item Как выглядит матрица $X(X'X)^{-1}X'$?
\item Почему 13 — несчастливое число?
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item $RSS=0$, $R^2=1$, так как в регрессии у Васи 13 (1+5+(4-1)+(5-1)) переменных, которые точно подстраиваются под данные.
\item Эта матрица является единичной размер 13 на 13, что нетрудно понять из формулы $RSS=y'(I-X(X'X)^{-1}X')y$.
\item На сегодняшний день среди исследователей нет единого мнения о происхождении трискайдекафобии (боязнь числа 13).

По одной из версий, число 13 может считаться «плохим» уже только потому, что оно больше 12, числа, которое является священным у многих народов.

Кроме того, существует библейское предание, косвенно связанное с числом 13 — на тайной вечере Иуда Искариот, апостол, предавший Иисуса, сидел за столом тринадцатым. С этим преданием связывают самую распространенную в XIX веке примету, связанную с числом 13 — если за обеденным столом собрались 13 человек, один из них умрет в течение года после трапезы. Позже в христианстве распространилось апокрифическое убеждение, что Сатана был 13-м ангелом.

Источник: \href{https://ru.wikipedia.org/wiki/%D0%A2%D1%80%D0%B8%D1%81%D0%BA%D0%B0%D0%B9%D0%B4%D0%B5%D0%BA%D0%B0%D1%84%D0%BE%D0%B1%D0%B8%D1%8F}{https://ru.wikipedia.org/wiki/Трискайдекафобия}
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
В рамках классической линейной модели найдите все математические ожидания и все ковариационные матрицы всех пар случайных векторов: $\ve$, $y$, $\hy$, $\he$, $\hb$. В частности, найдите $\E(\e)$, $\E(y)$, \ldots и $\Cov(\e,y)$, $\Cov(\e,\hy)$, \ldots


\begin{sol}
$\Var(\hb)=\sigma^2 (X'X)^{-1}$
\end{sol}
\end{problem}



\begin{problem}
Найдите $\E(\sum (\e_i-\bar{\e})^2 )$, $\E(RSS)$.


\begin{sol}
$(n-1)\sigma^2$, $(n-k)\sigma^2$
\end{sol}
\end{problem}



\begin{problem}
Используя матрицы $H=X(X'X)^{-1}X'$ и $\pi=\v1(\v1'\v1)^{-1}\v1'$ запишите $RSS$, $TSS$ и $ESS$ в матричной форме.

\begin{sol}
$TSS=y'(I-\pi)y$, $RSS=y'(I-H)y$, $ESS=y'(H-\pi)y$
\end{sol}
\end{problem}




\begin{problem}
Найдите $\E(TSS)$, $\E(ESS)$. Надо быть морально готовым к тому, что они выйдут громоздкие.

\begin{sol}
$\E(TSS)=(n-1)\sigma^2+\beta'X'(I-\pi)X\beta$
\end{sol}
\end{problem}



\begin{problem}
Вася строит регрессию $y$ на некий набор объясняющих переменных и константу. А на самом деле $y_i=\beta_1+\e_i$. Чему равно $\E(TSS)$, $\E(RSS)$, $\E(ESS)$ в этом случае?


\begin{sol}
$(n-1)\sigma^2$, $(n-k)\sigma^2$, $(k-1)\sigma^2$
\end{sol}
\end{problem}




\begin{problem}
Рассмотрим классическую линейную модель. Являются ли векторы $\he$ и $\hy$ перпендикулярными? Найдите $\Cov(\he,\hy)$.


\begin{sol}
Вспомним, что $\Var(y)=\Var(X\beta+\e)=\Var(\e)=\sigma^2$
\[
\Cov(\he,\hy)=\Cov(y-\hy,\hy)=\Cov(y-X(X'X)^{-1}X'y,y)=\Var(y)-\Cov(X(X'X)^{-1}X'y,y)
\]
\[
\Cov(\he,\hy)=\sigma^2-X(X'X)^{-1}X'\Var(y)X(X'X)^{-1}X'=\sigma^2(I-X(X'X)^{-1}X')
\]
и если $X(X'X)^{-1}X'$ не равна единичной, то векторы $\he$ и $\hy$ не являются перпендикулярными. Это может произойти только в случае, когда $RSS=0$, то есть в случае, когда существующие переменные абсолютно точно описывают данные.
\end{sol}
\end{problem}


\begin{problem}
Чему в классической модели регрессии равны $\E(\e)$, $\E(\he)$? Верно ли что $\sum \e_i$ равна 0? Верно ли что $\sum \he_i$ равна 0?

\begin{sol}
$\E(\e)=0$, $\E(\he)=0$, $\sum \e_i$ может оказаться равной нулю только случайно, в нормальной модели это происходит с вероятностью 0, $\sum \he_i=0$ в модели со свободным членом
\end{sol}
\end{problem}



\begin{problem}
Найдите на картинке все перпендикулярные векторы. Найдите на картинке все прямоугольные треугольники. Сформулируйте для них теоремы Пифагора.

\tdplotsetmaincoords{70}{110}
\begin{tikzpicture}[tdplot_main_coords]
\coordinate (hY) at (0,2.7,0);
\coordinate (Y) at (-2,2,2);
\coordinate (bY) at (-2,1,0);
\draw[thick,dotted, ->] (0,0,0) -- (-4,2,0) node[anchor=west]{$\vec{1}$};
\draw[thick,->] (0,0,0) -- (hY) node[anchor=west]{$\hy$};
\draw[thick,->] (0,0,0) -- (Y) node[anchor=south]{$y$};
\draw[thick,->] (0,0,0) -- (1,2,0) node[anchor=north]{$x$};
\draw[dotted] (hY) -- (Y);
\draw[dotted] (hY) -- (bY);
\draw[dotted] (Y) -- (bY);
\draw[thick,->] (0,0,0) -- (bY) node[anchor=south]{$\bar{y}\cdot \vec{1}$};
\end{tikzpicture}





\begin{sol}
$\sum y_i^2=\sum \hy_i^2+\sum \he_i^2$, $TSS=ESS+RSS$,
\end{sol}
\end{problem}



\begin{problem}
Покажите на картинке TSS, ESS, RSS, $R^2$, $\sCorr(\hy,y)$, $\sCov(\hy,y)$

\tdplotsetmaincoords{70}{110}
\begin{tikzpicture}[tdplot_main_coords]
\coordinate (hY) at (0,2.7,0);
\coordinate (Y) at (-2,2,2);
\coordinate (bY) at (-2,1,0);
\draw[thick,dotted, ->] (0,0,0) -- (-4,2,0) node[anchor=west]{$\vec{1}$};
\draw[thick,->] (0,0,0) -- (hY) node[anchor=west]{$\hy$};
\draw[thick,->] (0,0,0) -- (Y) node[anchor=south]{$y$};
\draw[thick,->] (0,0,0) -- (1,2,0) node[anchor=north]{$x$};
\draw[dotted] (hY) -- (Y);
\draw[dotted] (hY) -- (bY);
\draw[dotted] (Y) -- (bY);
\draw[thick,->] (0,0,0) -- (bY) node[anchor=south]{$\bar{y}\cdot \vec{1}$};
\end{tikzpicture}




\begin{sol}
$\sCorr(\hy, y)=\frac{\sCov(\hy, y)}{\sqrt{\sVar(\hy)\sVar{(y)}}}$

$\sCorr(\hy, y)^2=\frac{(\sCov(\hy, y))^2}{\sVar(\hy)\sVar{(y)}} $

$R^2\cdot TSS/(n-1)\cdot ESS/(n-1)=(\sCov(\hy, y))^2=(\sCov(\hy-\bar y, y-\bar y))^2$
Отсюда можно понять, что ковариация для двухмерного случая равна произведению длин векторов $\hy-\bar y$ и $y-\bar y$ — $\sqrt{ESS}$ и $\sqrt{TSS}$ на косинус угла между ними ($\sqrt{R^2}$). Геометрически скалярное произведение можно изобразить как произведение длин одного из векторов на проекцию второго вектора на первый. Если будет проецировать $y-\bar y\overrightarrow{1}$ на $\hy-\bar y\overrightarrow{1}$, то получим как раз $ESS$ — тот квадрат на рисунке, что уже построен.


$\sCov(\hy, y)=\sqrt{ESS^2/(n-1)^2}=ESS/(n-1)$


\end{sol}
\end{problem}


\begin{problem}
Предложите аналог $R^2$ для случая, когда константа среди регрессоров отсутствует. Аналог должен быть всегда в диапазоне $[0;1]$, совпадать с обычным $R^2$, когда среди регрессоров есть константа, равняться единице в случае нулевого $\he$.


\begin{sol}
Спроецируем единичный столбец на «плоскость», обозначим его $1'$. Делаем проекцию $y$ на «плоскость» и на $1'$. Далее аналогично.
\end{sol}
\end{problem}



\begin{problem}
Вася оценил регрессию $y$ на константу, $x$ и $z$. А затем, делать ему нечего, регрессию $y$ на константу и полученный $\hy$. Какие оценки коэффициентов у него получатся? Чему будет равна оценка дисперсии коэффицента при $\hy$? Почему оценка коэффициента неслучайна, а оценка её дисперсии положительна?


\begin{sol}
Проекция $y$ на $\hy$ это $\hy$, поэтому оценки коэффициентов будут 0 и 1. Оценка дисперсии $\frac{RSS}{(n-2)ESS}$. Нарушены предпосылки теоремы Гаусса-Маркова, например, ошибки новой модели в сумме дают 0, значит коррелированы.
\end{sol}
\end{problem}




\begin{problem}
При каких условиях $TSS=ESS+RSS$?

\begin{sol}
Либо в регрессию включена константа, либо единичный столбец (тут была опечатка, столбей) можно получить как линейную комбинацию регрессоров, например, включены дамми-переменные для каждого возможного значения качественной переменной.
\end{sol}
\end{problem}





\begin{problem}
Истинная модель имеет вид $y=X\beta +\e$. Вася оценивает модель $\hy=X \hb$ по первой части выборки, получает $\hb_a$, по второй части выборки — получает $\hb_b$ и по всей выборке — $\hb_{tot}$. Как связаны между собой $\hb_a$, $\hb_b$, $\hb_{tot}$? Как связаны между собой ковариационные матрицы $\Var(\hb_a)$,  $\Var(\hb_b)$ и  $\Var(\hb_{tot})$?


\begin{sol}
Сами оценки коэффициентов никак детерминистически не связаны, но при большом размере подвыборок примерно равны. А ковариационные матрицы связаны соотношением $\Var(\hb_a)^{-1}+\Var(\hb_b)^{-1}=\Var(\hb_{tot})^{-1}$
\end{sol}
\end{problem}





\begin{problem}
Модель линейной регрессии имеет вид $y_i=\b_1 x_{i,1}+\b_2 x_{i,2} + u_i$.
Сумма квадратов остатков имеет вид $Q\left(\hb_1,\hb_2\right)=\sum_{i=1}^n (y_1-\hb_1 x_{i,1}-\hb_2 x_{i,2})^2$.
\begin{enumerate}
\item Выпишите необходимые условия минимума суммы квадратов остатков
\item Найдите матрицу $X'X$ и вектор $X'y$ если матрица $X$ имеет вид
$X=
\left(
\begin{array}{cc}
x_{1,1} & x_{1,2} \\
\vdots & \vdots \\
x_{n,1} & x_{n,2}
\end{array}
\right)
$,
а вектор $y$ имеет вид
$y=
\left(
\begin{array}{c}
y_1 \\
\vdots \\
y_n
\end{array}
\right)
$
\item Докажите, что необходимые условия равносильны матричному уравнению $X'X\hb=X'y$, где
$\hb=
\left(
\begin{array}{c}
\hb_1 \\
\hb_2
\end{array}
\right)
$
\item Предполагая, что матрица $X'X$ обратима, найдите $\hb$
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item
\[\he'\he\rightarrow \min_{\hb}\]
\[(y-\hy)'(y-\hy)\rightarrow\min_{\hb}\]
\[(y-X\hb)'(y-X\hb)\rightarrow\min_{\hb}\]
\[y'y-\hb' X'y-y'X\hb+\hb'X'X\hb\rightarrow\min_{\hb}\]

Воспользуемся тем, что : $\frac{\partial x'A}{\partial x'}=A'$, $\frac{\partial Ax}{\partial x'}=A$,
$\frac{\partial x'Ax}{\partial x'}=x'(A'+A)$
Условие первого порядка:

\[-2(X'y)'+(X'X+(X'X)')\hb'=0\]
\[-2X'y+2\hb X'X=0\]

\item
$X'X=\begin{pmatrix}
x_{1,1}\ldots x_{n,1}\\
x_{1,2}\ldots x_{n,2}
\end{pmatrix}
\begin{pmatrix}
x_{1,1}& x_{1,2}\\
\vdots&\vdots\\
x_{n,1}& x_{n,2}
\end{pmatrix}=
\begin{pmatrix}
\sum_{i=1}^n x^2_{n,1}& x_{1,1}x_{1,2}\ldots x_{n,1}x_{n,2}\\
x_{1,1}x_{1,2}\ldots x_{n,1}x_{n,2}&\sum_{i=1}^nx^2_{n,2}
\end{pmatrix}$

$X'y=\begin{pmatrix}
x_{1,1}\ldots x_{n,1}\\
x_{1,2}\ldots x_{n,2}
\end{pmatrix}
\begin{pmatrix}
y_1\\
y_2\\
\vdots\\
y_n
\end{pmatrix}=
\begin{pmatrix}
x_{1,1}y_1\ldots x_{n,1}y_n\\
x_{1,2}y_1\ldots x_{n,2}y_n
\end{pmatrix}$

\item
Условие первого порядка:
\[-2(X'y)'+\hb'(X'X+(X'X)')=0\]
\[-2X'y+2X'X \hb =0\]
и
\[X'y=X'X\hb\]
\[\hb=(X'X)^{-1}X'y\]

\item
\begin{multline*}
\hb=(X'X)^{-1}X'y=
\begin{pmatrix}
\sum_{i=1}^n x^2_{n,1}& x_{1,1}x_{1,2}\ldots x_{n,1}x_{n,2}\\
x_{1,1}x_{1,2}\ldots x_{n,1}x_{n,2}&\sum_{i=1}^nx^2_{n,2}
\end{pmatrix}^{-1}
\begin{pmatrix}
x_{1,1}y_1\ldots x_{n,1}y_n\\
x_{1,2}y_1\ldots x_{n,2}y_n
\end{pmatrix}=\\
=\frac{1}{\sum_{i=1}^n x^2_{n,1}\sum_{i=1}^n x^2_{n,2}-(x_{1,1}x_{1,2}+\ldots+ x_{n,1}x_{n,2})^2}\cdot\\
\begin{pmatrix}
\sum_{i=1}^n x^2_{n,2}& -x_{1,1}x_{1,2}+\ldots+ x_{n,1}x_{n,2}\\
-x_{1,1}x_{1,2}+\ldots +x_{n,1}x_{n,2}&\sum_{i=1}^nx^2_{n,1}
\end{pmatrix}
\cdot
\begin{pmatrix}
x_{1,1}y_1+\ldots +x_{n,1}y_n\\
x_{1,2}y_1+\ldots +x_{n,2}y_n
\end{pmatrix}=\\
=\frac{1}{\sum_{i=1}^n x^2_{n,1}\sum_{i=1}^n x^2_{n,2}-(x_{1,1}x_{1,2}+\ldots+ x_{n,1}x_{n,2})^2}\cdot\\
\begin{pmatrix}
\sum_{i=1}^n x^2_{n,2}(x_{1,1}y_1+\ldots +x_{n,1}y_n) -(x_{1,1}x_{1,2}+\ldots+ x_{n,1}x_{n,2})(x_{1,2}y_1+\ldots +x_{n,2}y_n)\\
-(x_{1,1}y_1+\ldots +x_{n,1}y_n)(x_{1,1}x_{1,2}+\ldots +x_{n,1}x_{n,2})+\sum_{i=1}^nx^2_{n,1}(x_{1,2}y_1+\ldots +x_{n,2}y_n)
\end{pmatrix}
\end{multline*}
\end{enumerate}
\end{sol}
\end{problem}




\begin{problem}
Вася оценил исходную модель:
\[
y_i=\b_1+\b_2 x_i + u_i
\]

Для надежности Вася стандартизировал переменные, т.е. перешёл к $y_i^*=(y_i-\bar{y})/s_y$ и $x_i^*=(x_i-\bar{x})/s_x$. Затем Вася оценил ещё две модели:

\[
y_i^*=\b_1'+\b_2' x^*_i + u_i'
\]

и
\[
y_i^*=\b_2'' x^*_i + u_i''
\]

В решении можно считать $s_x$ и $s_y$ известными.

\begin{enumerate}
\item Найдите $\hb_1'$.
\item Как связаны между собой $\hb_2$, $\hb_2'$ и $\hb_2''$?
\item Как связаны между собой $\hat{u}_i$, $\hat{u}_i'$ и $\hat{u}_i''$?
\item Как связаны между собой $\hVar\left(\hb_2\right)$, $\hVar\left(\hb_2'\right)$ и $\hVar\left(\hb_2''\right)$?
\item Как выглядит матрица $\hVar\left(\hb'\right)$?
\item Как связаны между собой $t$-статистики $t_{\hb_2}$, $t_{\hb_2'}$ и $t_{\hb_2''}$?
\item Как связаны между собой $R^2$, $R^{2\prime}$ и $R^{2\prime\prime}$?
\item В нескольких предложениях прокомментируйте последствия перехода к стандартизированным переменным.
\end{enumerate}

\begin{sol}

\begin{enumerate}
\item $\hat\beta_1'=\bar y^* -\hb_2'\bar x^*=\overline{y-\bar y}-\hb_2'(\overline{x-\bar x})=0$

\item $\hb_2'=\frac{\Cov(y^*,x^*)}{\Var(x^*)}=\frac{\Cov(y-\bar y,x-\bar x)s_x/s_y}{\Var(x-\bar x)}=\frac{\Cov(y,x)s_x/s_y}{\Var(x)}=\frac{s_x}{s_y}\hb_2$

$\hb_2''=\frac{\overline{x^*y^*}}{\overline{\bar x^2}}=\frac{\Cov(x^*y^*)+\bar{x^*}\bar{y^*}}{\Var(x^*)+\bar{x^*}}=\frac{\Cov(y^*,x^*)}{\Var(x^*)}=\hb_2'=\frac{s_x}{s_y}\hb_2$

\item $\hat u_i'=y_i^*-\hy_i^*=y_i^*-\hb_1'-\hb_2'x_i^*=y_i^*-\frac{s_x}{s_y}\hb_2x_i^*=\frac{y_i-\bar y-\hb_2(x_i-\bar x)}{s_y}=\frac{y_i-\hb_2x_i-(\bar y-\hb_2\bar x)}{s_y}=\frac{y_i-\hb_2x_i-\hb_1}{s_y}=\frac{\hat u_i}{s_y}$

$\hat u_i''=y_i^*-\hy_i^*=y_i^*-\hb_2''x_i^*=y_i^*-\frac{s_x}{s_y}\hb_2'x_i^*=\frac{y_i-\bar y-\hb_2(x_i-\bar x)}{s_y}=\frac{y_i-\hb_2x_i-(\bar y-\hb_2\bar x)}{s_y}=\frac{y_i-\hb_2x_i-\hb_1}{s_y}=\frac{u_i}{s_y}=\hat u_i'$

\item $RSS=\sum_{i=1}^n \hat u_i^2=s_y^2\sum_{i=1}^n \hat u_i'^2=s_y^2RSS'$, $RSS'=\sum_{i=1}^n \hat u_i'^2=RSS''$.

$X'X=\begin{pmatrix}
n&\sum^n_{i=1}x_i\\
\sum^n_{i=1}x_i&\sum^n_{i=1}x_i^2
\end{pmatrix}$

$X'X_{new}=\begin{pmatrix}
n&\sum^n_{i=1}(x_i-\bar x)/s_x\\
\sum^n_{i=1}(x_i-\bar x)/s_x&\sum^n_{i=1}(x_i-\bar x)^2/s_x^2
\end{pmatrix}=
\begin{pmatrix}
n&\sum^n_{i=1}(x_i-\bar x)/s_x\\
(\sum^n_{i=1}x_i^2-\bar x^2)/s_x&\sum^n_{i=1}(x_i-\bar x)^2/s_x^2
\end{pmatrix}=
\begin{pmatrix}
n&0\\
0&(\sum^n_{i=1}x_i^2-n\bar x^2)/s_x^2
\end{pmatrix}$

$\hVar(\hb_2)=\frac{RSS_1}{n-2}(X'X)_{(2,2)}^{-1}=\frac{RSS_2s_y^2}{n-2}\frac{1}{n\sum^n_{i=1}x_i^2-(\sum^n_{i=1}x_i)^2}
\begin{pmatrix}
\sum^n_{i=1}x_i^2&-\sum^n_{i=1}x_i\\
-\sum^n_{i=1}x_i&n
\end{pmatrix}_{(2,2)}=\frac{RSS_2s_y^2}{n-2}\frac{n}{n\sum^n_{i=1}x_i^2-(\sum^n_{i=1}x_i)^2}=\frac{RSS_2s_y^2}{n-2}\frac{n/s_x^2}{(n\sum^n_{i=1}x_i^2-(\sum^n_{i=1}x_i)^2)/s_x^2}=\hVar(\hb_2')\frac{s_y^2}{s_x^2}$

$\hVar(\hb'_2)=\frac{RSS_2}{n-2}(X'X_{new})_{(2,2)}^{-1}=\frac{RSS_3}{n-1}(X'X_{new})_{(2,2)}^{-1}\frac{n-1}{n-2}=\hVar(\hb''_2)\frac{n-1}{n-2}$

\item $\hVar(\hb')=\frac{RSS_2}{n-2}(X'X_{new})^{-1}=\frac{\sum_{i=1}^n(y_i^*-\hb_1'-\hb_2'x_i^*)^2}{n-2}\begin{pmatrix}
(\sum^n_{i=1}x_i^2-n\bar x^2)/s_x^2&0\\
0&n
\end{pmatrix}$
где $\hb'=(X'X_{new})^{-1}\begin{pmatrix}
0\\
\sum_{i=1}^n(x_i-\bar x)y_i
\end{pmatrix}$, в частности, $\hb_1'=0$.

\item $t_{\hb_2}=\frac{\hat\beta}{\sqrt{\Var(\hb_2)}}=\frac{\hat\beta's_y/s_x}{\sqrt{\Var(\hb_2')}s_y/s_x}=t_{\hb_2'}=\sqrt{\frac{n-2}{n-1}}t_{\hb_2''}$

\item $TSS'=TSS''=\frac{\sum_{i=1}^n(y_i-\bar y)^2}{s_y^2}=\frac{TSS}{s_y^2}$.

$R'^2=R''^2$, так как соответствующие $TSS$ и $RSS$ равны.

$R^2=\frac{RSS}{TSS}=\frac{s_y^2RSS'}{TSS's_y^2}=\frac{RSS}{TSS}=R'^2$

\item При переходе к стандартизированным переменным изменяются оценки коэффициентов и остатки регрессии пропорционально стандартным отклонениям переменным. Также перестаёт играет роль регрессор-свободный член, так как матожидание эндогенной переменной становится равной 0. Также при переходе к стандартизированным переменным можно снизить оценку дисперсии переменных, так как можно убрать регрессор-свободный член. Качество регрессий по $R^2$ не изменяется.

\end{enumerate}
\end{sol}
\end{problem}




\begin{problem}
Регрессионная модель  задана в матричном виде при помощи уравнения $y=X\beta+\varepsilon$, где $\beta=(\beta_1,\beta_2,\beta_3)'$.
Известно, что $\E(\varepsilon)=0$  и  $\Var(\varepsilon)=\sigma^2\cdot I$.
Известно также, что

$y=\left(
\begin{array}{c}
1\\
2\\
3\\
4\\
5
\end{array}\right)$,
$X=\left(\begin{array}{ccc}
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 1 & 1
\end{array}\right)$.


Для удобства расчетов приведены матрицы


$X'X=\left(
\begin{array}{ccc}
5 & 2 & 1\\
2 & 2 & 1\\
1 & 1 & 1
\end{array}\right)$ и $(X'X)^{-1}=\frac{1}{3}\left(
\begin{array}{ccc}
1 & -1 & 0 \\
-1 & 4 & -3 \\
0 & -3 & 6
\end{array}\right)$.

\begin{enumerate}
\item Укажите число наблюдений.
\item Укажите число регрессоров с учетом свободного члена.
\item Запишите модель в скалярном виде.
\item Рассчитайте $TSS=\sum (y_i-\bar{y})^2$, $RSS=\sum (y_i-\hy_i)^2$ и $ESS=\sum (\hy_i-\bar{y})^2$.
\item Рассчитайте при помощи метода наименьших квадратов $\hb$, оценку для вектора неизвестных коэффициентов.
\item Чему равен $\he_5$, МНК-остаток регрессии, соответствующий 5-ому наблюдению?
\item Чему равен $R^2$  в модели? Прокомментируйте полученное значение с точки зрения качества оцененного уравнения регрессии.
\item Используя приведенные выше данные, рассчитайте несмещённую оценку для неизвестного параметра $\sigma^2$ регрессионной модели.
\item Рассчитайте $\hVar(\hb)$, оценку для ковариационной матрицы вектора МНК-коэффициентов $\hb$.
\item Найдите $\hVar(\hb_1)$, несмещённую оценку дисперсии МНК-коэффициента $\hb_1$.
\item Найдите $\hVar(\hb_2)$, несмещённую оценку дисперсии МНК-коэффициента $\hb_2$.
\item Найдите $\hCov(\hb_1,\hb_2)$, несмещённую оценку ковариации МНК-коэффициентов $\hb_1$ и $\hb_2$.
\item Найдите $\hVar(\hb_1+\hb_2)$, $\hVar(\hb_1-\hb_2)$, $\hVar(\hb_1+\hb_2+\hb_3)$, $\hVar(\hb_1+\hb_2-2\hb_3)$.
\item Найдите $\hCorr(\hb_1,\hb_2)$, оценку коэффициента корреляции МНК-коэффициентов $\hb_1$ и $\hb_2$.
\item Найдите $s_{\hb_1}$, стандартную ошибку МНК-коэффициента $\hb_1$.
\item Рассчитайте выборочную ковариацию $y$ и $\hy$.
\item Найдите выборочную дисперсию $y$, выборочную дисперсию $\hy$.
\end{enumerate}

\begin{sol}

\begin{enumerate}
\item $n=5$
\item $k=3$
\item $y_i=\beta_0+\beta_1x_1+\beta_2x_2+u_i$
\item $RSS=y'(I-X(X'X)^{-1}X')y=2$, $TSS=(1-3)^2+(2-3)^2+(3-3)^2+(4-3)^2+(5-3)^2=4+1+0+1+4=10$, $ESS=TSS-RSS=8$.
\item $\hb=X(X'X)^{-1}X'y=(2\>2\>1)'$
\item $\hat\e_5=y_5-\hy_5=5-2-2x^{(5)}_1-x^{(5)}_2=5-2-2-1=0$.
\item $R^2=\frac{ESS}{TSS}\frac{8}{10}=0.8$. $R^2$ высок, модель регрессии хорошо описывает данные.
\item Несмещённая оценка — $\hat\sigma^2$. $\hat\sigma^2=\frac{RSS}{n-k}=\frac{2}{5-3}=1$.
\item $\hVar(\hb)=\hat\sigma^2(X'X)^{-1}=\frac13\begin{pmatrix}
1&-1&0\\
-1&4&-3\\
0&-3&6
\end{pmatrix}
$
\item $\hVar(\hat\beta_1)=\frac13$
\item $\hVar(\hat\beta_2)=\frac43$
\item $\hCov(\hat\beta_1,\hat\beta_2)=-\frac13$
\item $\hVar(\hat\beta_1+\hat\beta_2)=\hVar(\hat\beta_1)+\hVar(\hat\beta_2)+2\hCov(\hat\beta_1,\hat\beta_2)=\frac13+\frac43-2\cdot\frac13=1$

$\hVar(\hat\beta_1-\hat\beta_2)=\hVar(\hat\beta_1)+\hVar(\hat\beta_2)-2\hCov(\hat\beta_1,\hat\beta_2)=\frac13+\frac43+2\cdot\frac13=\frac73$

$\hVar(\hat\beta_1+\hat\beta_2+\hat\beta_3)=\hVar(\hat\beta_1)+\hVar(\hat\beta_2)+\hVar(\hat\beta_3)+2\hCov(\hat\beta_1,\hat\beta_2)+2\hCov(\hat\beta_1,\hat\beta_3)+2\hCov(\hat\beta_2,\hat\beta_3)=\frac13(1+4+6-2-6)=1$

$\hVar(\hat\beta_1+\hat\beta_2-2\hat\beta_3)=\hVar(\hat\beta_1)+\hVar(\hat\beta_2)+4\hVar(\hat\beta_3)+2\hCov(\hat\beta_1,\hat\beta_2)-4\hCov(\hat\beta_1,\hat\beta_3)-4\hCov(\hat\beta_2,\hat\beta_3)=\frac13(1+4+4\cdot 6-1+12)=\frac{40}{3}$
\item $\hat\corr(\hat\beta_1,\hat\beta_2)=\frac{\hCov(\hat\beta_1,\hat\beta_2)}{\sqrt{\hVar(\hat\beta_1)\cdot\hVar(\hat\beta_1)}}=\frac{-\frac13}{\sqrt{\frac13\cdot\frac43}}=-\frac12$
\item $s_{\hat\beta_1}=\sqrt{\hVar(\hat\beta_1)}=\sqrt{\frac13}=\frac{\sqrt3}{3}$
\item $\hy=\begin{pmatrix}
2\\
2\\
2\\
4\\
5
\end{pmatrix}$, $\bar \hy=3$

 $\sCov(y,\hy)=\frac{\sum_{i=1}^n(y_i-\bar y)(\hy_i-\hat \bar y_i)}{n-1}=\frac{(1-3)(2-3)+(2-3)(2-3)+(3-3)(2-3)+(4-3)(4-3)+(5-3)(5-3)}{4}=\frac{2+1+1+4}{4}=2$
\item $\sVar(y)=\frac{\sum_{i=1}^n(y-\bar y)^2}{n-1}=\frac{(1-3)^2+(2-3)^2+(3-3)^2+(4-3)^2+(5-3)^2}{4}=\frac{4+1+0+1+4}{4}=2.5$, $\sVar(\hy)=\frac{\sum_{i=1}^n(\hy- \bar\hy)^2}{n-1}=\frac{(2-3)^2+(2-3)^2+(2-3)^2+(4-3)^2+(5-3)^2}{4}=2$.

\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
Теорема Фриша-Вау. Регрессоры разбиты на две группы: матрицу $X_1$ размера $n\times k_1$ и матрицу $X_2$ размера $n\times k_2$. Рассмотрим две процедуры:
\begin{enumerate}
\item[M1.] Строим регрессию вектора $y$ на все регрессоры, т.е. оцениваем модель:
\[
y=X\beta + \e = X_1 \beta_1 + X_2 \beta_2 +\e
\]
\item[M2.] Процедура из двух шагов:
\begin{enumerate}
\item Строим регрессию вектора $y$ на все регрессоры первой группы и получаем вектор остатков $M_1 y$, где $M_1=I-X_1(X_1'X_1)^{-1}X_1'$. Строим регрессию каждого регрессора из второй группы на все регрессоры первой группы и получаем в каждом случае вектор остатков. Эти остатки можно записать матрицей $M_1 X_2$.
\item Строим регрессию вектора $M_1 y$ на остатки $M_1 X_2$.
\end{enumerate}
Другими словами мы оцениваем модель:
\[
M_1 y = M_1 X_2 \gamma_2 + u
\]

\end{enumerate}
\begin{enumerate}
\item Верно ли, что МНК оценки коэффициентов $\hb_2$ и $\hat{\gamma}_2$ совпадают?
\item Верно ли, что остатки в обеих регрессиях совпадают?
\end{enumerate}

\begin{sol}
Подсказка: запишите матрицу $X$ как блочную и, пользуясь матричным выражением для $\hb$ и формулой Фробениуса, найдите $\hb_2$.

1. Да, верно.
$X=(X_1 X_2)$ — блочная матрица. Аналогично, $\hat\beta=\left(\begin{array}{c}
\hat\beta_1\\
\hat\beta_2
\end{array}\right)$ — блочная матрица (хотя на самом деле вектор).

\begin{multline*}
\hat\beta=(X'X)^{-1}X'y=((X_1X_2)'(X_1X_2))^{-1}(X_1X_2)'y=(\left(\begin{array}{c}
X_1'\\
X_2'
\end{array}\right)(X_1X_2))^{-1}(\left(\begin{array}{c}
X_1'\\
X_2'
\end{array}\right)y=\\=
\left(\begin{array}{cc}
X_1'X_1 & X_1'X_2\\
X_2'X_1 & X_2'X_2
\end{array}\right)^{-1}\left(\begin{array}{c}
X_1'\\
X_2'
\end{array}\right)y
\end{multline*}

Запишем и докажем формулу Фробениуса для обращения блочных матриц.

Формула Фробениуса:
\[
\begin{pmatrix} A & B \\
C & D \\
\end{pmatrix}^{-1}=
\begin{pmatrix}
A^{-1} +A^{-1} BH^{-1} CA^{-1}  & -A^{-1} BH^{-1}\\
 -H^{-1} CA^{-1}  & H^{-1}
\end{pmatrix}
\]
где $H=D-CA^{-1}B$.

Докажем формулу, обращая матрицу методом Гаусса. Умножим слева на $\left(\begin{array}{cc}
A^{-1} & 0\\
0 & I
\end{array}\right)$
\begin{multline*}
\left(\begin{array}{cc|cc}
A & B & I & 0\\
C & D & 0 & I
\end{array}\right)=
\left(\begin{array}{cc|cc}
I & A^{-1}B & A^{-1} & 0\\
C & D & 0 & I
\end{array}\right)=\\
\end{multline*}
вычтем из второй строки первую, умноженную на $C$
\begin{multline*}
=\left(\begin{array}{cc|cc}
I & A^{-1}B & A^{-1} & 0\\
0 & D-CA^{-1}B & -CA^{-1} & I
\end{array}\right)=
\end{multline*}
умножим слева на $\left(\begin{array}{cc}
I & 0\\
0 & (D-CA^{-1}B)^{-1}
\end{array}\right)$
\begin{multline*}
=\left(\begin{array}{cc|cc}
I & A^{-1}B & A^{-1} & 0\\
0 & I & -(D-CA^{-1}B)^{-1}CA^{-1} & (D-CA^{-1}B)^{-1}
\end{array}\right)=
\end{multline*}
вычтем из первой строки вторую, умноженную на $A^{-1}B$.
\begin{multline*}
=\left(\begin{array}{cc|cc}
I & 0 & A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} & -A^{-1}B(D-CA^{-1}B)^{-1}\\
0 & I & -(D-CA^{-1}B)^{-1}CA^{-1} & (D-CA^{-1}B)^{-1}
\end{array}\right)
\end{multline*}

Значит,
\begin{multline*}
\begin{pmatrix} A & B \\ C & D \\ \end{pmatrix}^{-1}
=\left(\begin{array}{cc}
A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1} & -A^{-1}B(D-CA^{-1}B)^{-1}\\
-(D-CA^{-1}B)^{-1}CA^{-1} & (D-CA^{-1}B)^{-1}
\end{array}\right)=\\=
\begin{pmatrix} A^{-1} +A^{-1} BH^{-1} CA^{-1}  & -A^{-1} BH^{-1}\\ -H^{-1} CA^{-1}  & H^{-1}\end{pmatrix}
\end{multline*}

По формуле Фробениуса получим, что
\[\left(\begin{array}{cc}
X_1'X_1 & X_1'X_2\\
X_2'X_1 & X_2'X_2
\end{array}\right)^{-1}=
\left(\begin{array}{cc}
(X_1'X_1)^{-1}+(X_1'X_1)^{-1}X_1'X_2H^{-1}X_2'X_1(X_1'X_1)^{-1} & -(X_1'X_1)^{-1}X_1'X_2H^{-1}\\
-H^{-1}X_2'X_1(X_1'X_1)^{-1} & H^{-1}
\end{array}\right),\]
где $H=X_2'X_2-X_2'X_1(X_1'X_1)^{-1}X_1'X_2$. Верхняя строка в данном пункте не важна, и сейчас её опустим. Заметим, что
\[H=X_2'X_2-X_2'X_1(X_1'X_1)^{-1}X_1'X_2=X_2'(I-X_1(X_1'X_1)^{-1}X_1')X_2=X_2'M_1X_2\]
Итак,
\begin{multline*}
\hat\beta=\left(\begin{array}{c}
\hat\beta_1\\
\hat\beta_2
\end{array}\right)=
\left(\begin{array}{cc}
X_1'X_1 & X_1'X_2\\
X_2'X_1 & X_2'X_2
\end{array}\right)^{-1}\left(\begin{array}{c}
X_1'\\
X_2'
\end{array}\right)y=\\=\left(\begin{array}{cc}
? & ?\\
-H^{-1}X_2'X_1(X_1'X_1)^{-1} & H^{-1}
\end{array}\right)\left(\begin{array}{c}
X_1'\\
X_2'
\end{array}\right)y=\\=\left(\begin{array}{c}
?\\
-H^{-1}X_2'X_1(X_1'X_1)^{-1}X_1'+H^{-1}X_2'
\end{array}\right)y=\\=
\left(\begin{array}{c}
?\\
H^{-1}X_2'(I-X_1(X_1'X_1)^{-1}X_1')
\end{array}\right)y=\\=
\left(\begin{array}{c}
?\\
H^{-1}X_2'M_1y
\end{array}\right)=
\left(\begin{array}{c}
?\\
(X_2'M_1X_2)^{-1}X_2'M_1y
\end{array}\right)
\end{multline*}

\[\hb_2=(X_2'M_1X_2)^{-1}X_2'M_1y\]
Заметим свойства матрицы-проектора $M_1$.
\[M_1'=(I-X_1(X_1'X_1)^{-1}X_1')'=I-X_1(X_1'X_1)^{-1}X_1'=M_1\]
\begin{multline*}
(M_1)^2=(I-X_1(X_1'X_1)^{-1}X_1')^2=I-2X_1(X_1'X_1)^{-1}X_1'+X_1(X_1'X_1)^{-1}X_1'\cdot X_1(X_1'X_1)^{-1}X_1'=\\
=I-X_1(X_1'X_1)^{-1}X_1'=M_1
\end{multline*}

Значит,
\begin{multline*}
\hb_2=(X_2'M_1X_2)^{-1}X_2'M_1y=(X_2'M_1M_1X_2)^{-1}X_2'M_1M_1y=(X_2'M_1'M_1X_2)^{-1}X_2'M_1'M_1y=\\=
((M_1X_2)'M_1X_2)^{-1}(M_1X_2)'M_1y
\end{multline*}
но ведь и
\[
\hat\gamma_2=
((M_1X_2)'M_1X_2)^{-1}(M_1X_2)'M_1y
\]
Значит, $\hb_2=\hat\gamma_2=
((M_1X_2)'M_1X_2)^{-1}(M_1X_2)'M_1y$
, что и требовалось доказать.

2. Да, верно.
\[\hy=X_1\hb_1+X_2\hb_2\]

\[M_1 \he=M_1y-M_1\hy=M_1y-M_1(X_1\hat\beta_1+X_2\hb_2)=M_1y-M_1X_2\hb_2-M_1X_1\hat\beta_1\]
\[M_1X_1=(I-X_1(X_1'X_1)^{-1}X_1')X_1=X_1-X_1(X_1'X_1)^{-1}X_1'X_1=0\]
\[M_1 \he=M_1y-M_1X_2\hb_2=M_1y-M_1X_2\hat\gamma_2=\hat u\]

\begin{multline*}
M_1\he=M_1(y-\hy)=M_1(I-X(X'X)^{-1}X')y=(I-X(X'X)^{-1}X')y
\end{multline*}
так как $M_1$ ортогональное дополнение к $X_1$, а $(I-X(X'X)^{-1}X')y$ уже лежит в ортогональном дополнении к $X_1$, так как $I-X(X'X)^{-1}X'$ ортогональное дополнение к к прямой сумме пространств $X_1$ и $X_2$ — $X_1\oplus X_2$.\end{sol}
\end{problem}




\begin{problem}
Всего имеется $100$ наблюдений. Для первых 50-ти наблюдений $X'X=\begin{pmatrix}
50 & 300 \\
300 & 2100
\end{pmatrix}$, $X'y=\begin{pmatrix}
300 & 2000
\end{pmatrix}'$, $y'y=2100$. По последним 50-ти наблюдениям: $X'X=\begin{pmatrix}
50 & 300 \\
300 & 2100
\end{pmatrix}$, $X'y=\begin{pmatrix}
300 & 2200
\end{pmatrix}'$, $y'y=2500$. По первым 50-ти наблюдениям оценивается модель $y_i = \beta_1 + \beta_2 x_i + \e_i$, по последним 50-ти наблюдениям оценивается модель $y_i = \gamma_1 + \gamma_2 x_i + \e_i$. Предположеним, что во всех 100 наблюдениях $\e_i$ независимы и нормальны $N(0;\sigma^2)$. На уровне значимости 5\% проверьте гипотезу $H_0: \, \beta=\gamma$.

\begin{sol}

Подсказка: в задаче следует применить тест Чоу.

Посчитаем $RSS$ в каждой из моделей
\[
RSS=y'(I-X(X'X)^{-1}X')y=y'y-y'X(X'X)^{-1}X'y=y'y-(X'y)'(X'X)^{-1}X'y
\]

$RSS_1=2000-1933.33=\frac{500}{3}$

$RSS_2=2500-2333.33=\frac{500}{3}$

$X'X=
\begin{pmatrix}
n&\sum^n_{i=1}x_i\\
\sum^n_{i=1}x_i&\sum^n_{i=1}x_i^2
\end{pmatrix}$, $X'y=
\begin{pmatrix}
\sum^{100}_{i=1}y_i\\
\sum^{100}_{i=1}x_iy_i
\end{pmatrix}$
значит, новый
$X'X=\begin{pmatrix}
100&600\\
600&4200
\end{pmatrix}$
а новый $X'y=
\begin{pmatrix}
600\\
4200
\end{pmatrix}$.

$y'y=\sum^{100}_{i=1}y_i^2=(y'y)_{1st}+(y'y)_{2nd}=4600$

$RSS_{pooled}=4600-4200=400$

Тест Чоу
$\frac{(RSS_{pooled}-RSS_1-RSS_2)/k}{(RSS_1+RSS_2)/(n-2k)}=\frac{(400-500/3-500/3)/2}{1000/3/96}=\frac{96}{10}=9.6>3.09=F_{2,96}$
гипотеза о том, что $\beta=\gamma$ отвергается.
\end{sol}
\end{problem}



\begin{problem}
Докажите, что МНК-оценки $\hb = (X' X)^{-1} X' y$ являются несмещёнными и линейными по переменной $y$.


\begin{sol}
Докажем несмещённость МНК-оценок.
\[\E\hb = \E\left( (X' X)^{-1} X' y \right) = (X' X)^{-1} X' \E(y) = \]
\[= (X' X)^{-1} X' \E(X\beta + \e) = (X' X)^{-1} X' X\beta = \beta\]
Обозначим $\varphi(X, y) = (X' X)^{-1} X' y$. Тогда $\hb = \varphi(X, y)$. Покажем, что функция $\varphi$ линейна по переменной $y$.
\begin{enumerate}
\item $\varphi(X, \lambda \cdot y) = (X' X)^{-1} X' (\lambda \cdot y) = \lambda (X' X)^{-1} X' y = \lambda \cdot \varphi(X, y)$
\item $\varphi(X, y + z) = (X' X)^{-1} X' (y + z) = (X' X)^{-1} X' y + (X' X)^{-1} X' z = \varphi(X, y) + \varphi(X, z)$
\end{enumerate}
Что и требовалось доказать.
\end{sol}
\end{problem}


\begin{problem}
Являются ли МНК-оценки линейными по переменной $X$?


\begin{sol}
Нет, так как для функции $\varphi(X, y) = (X' X)^{-1} X' y$ не выполнено, например, свойство однородности по переменной $X$. Действительно,
\[\varphi(X, \lambda \cdot y) = ((\lambda \cdot X)' (\lambda \cdot X))^{-1} (\lambda \cdot X)' y = \frac{1}{\lambda} \cdot (X' X)^{-1} X' y = \frac{1}{\lambda} \varphi(X, y)\].
\end{sol}
\end{problem}


\begin{problem}
Приведите пример несмещённой и линейной по переменной $y$ оценки, отличной от МНК.


\begin{sol}
$\tilde{\beta} = (X' CX)^{-1} X' Cy$, где
\[C = \begin{pmatrix}
1 & 0 & 0 & \cdots & 0 \\
0 & 2 & 0 & \cdots & 0 \\
0 & 0 & 3 & \ddots & \vdots \\
\vdots & \vdots & \ddots & \ddots & 0 \\
0 & 0 & \cdots & 0 & n \\
\end{pmatrix} \]
\end{sol}
\end{problem}



\begin{problem}
Известно, что в регрессии $y = X\beta + \e$ не выполняется условие $H\v1 = \v1$, где $\v1$ — единичный столбец, а $H = X(X'X)^{-1}X'$, $\pi = \frac{\v1\v1'}{\v1'\v1}$. Какие равенства будут верны?
\begin{enumerate}
\item $H\pi = \pi$;
\item $H^2 = H$;
\item $\sum_{i=1}^n \he_i = 0$;
\item $\bar{y} = \overline{\hy}$.
\end{enumerate}


\begin{sol}
$H\v1 = \v1 \Leftrightarrow H\pi = \pi$ поскольку, если матрицу $\pi$ записать по столбцам $\pi = \frac{1}{n}\begin{pmatrix}
\v1 & \v1 & \ldots & \v1
\end{pmatrix}$, то можно записать следующую цепочку равенств $H\pi = H\frac{1}{n}\begin{pmatrix}
\v1 & \v1 & \ldots & \v1
\end{pmatrix} = \frac{1}{n}\begin{pmatrix}
H\v1 & H\v1 & \ldots & H\v1
\end{pmatrix} = \frac{1}{n}\begin{pmatrix}
\v1 & \v1 & \ldots & \v1
\end{pmatrix} \Leftrightarrow H\v1 = \v1$.

Свойство $H^2 = H$ имеет место независимо от выполнимости условия $H\v1 = \v1$. Действительно, $H^2 = X (X' X)^{-1}X'X(X'X)^{-1}X' = X(X'X)^{-1}X' = H$.

Рассмотрите пример $y = \begin{pmatrix}
1 & -1 & 0
\end{pmatrix}'$, $x = \begin{pmatrix}
1 & 0 & -1
\end{pmatrix}'$. Постройте регрессию $y = \beta x + \e$ без свободного члена. Убедитесь, что $\sum_{i=1}^n \he_i = 0$ и $\bar{y} = \overline{\hy} = 0$, но $H\v1 \neq \v1$.


Ответ: $H\pi = \pi$
\end{sol}
\end{problem}




\begin{problem}
Какие из данных условий являются необходимыми условиями теоремы Гаусса-Маркова?
\begin{enumerate}
\item Правильная специфицикация модели: $y = X\beta + \e$;
\item Полный ранг матрицы $X$;
\item Невырожденность матрицы $X'X$;
\item Нормальность распределения случайной составляющей;
\item Пропорциональность ковариационной матрицы случайной составляющей единичной матрице;
\item Наличие в матрице $X$ единичного столбца.
\end{enumerate}


\begin{sol}
(1), (2), (3), (5)
\end{sol}
\end{problem}



\begin{problem}
Для регрессии $y = X\beta + \e$ с $\E(\e) = 0$, $\Var(\e) = \begin{pmatrix}
\sigma_1^2 &  & 0 \\
  & \ddots &   \\
0 &  & \sigma^2_n
\end{pmatrix}$ найдите математическое ожидание квадратичной формы $\e' \pi \e$.



\begin{sol}
\begin{multline*}
\E(\e' \pi \e) = \E(\tr[\e' \pi \e]) = \E(\tr[\pi \e \e']) = \tr[\pi \E(\e\e')]  =\\
\tr[\pi \Var(\e)] = \tr\left[ \frac{1}{n} \begin{pmatrix}
1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 \\
\ldots & \ldots & \ldots & \ldots \\
1 & 1 & 1 & 1 \\
\end{pmatrix} \begin{pmatrix}
\sigma_1^2 & 0 & \ldots & 0 \\
0 & \sigma_2^2 & \ldots & 0 \\
\ldots & \ldots & \ldots & \ldots \\
0 & \ldots & 0 & \sigma_n^2 \\
\end{pmatrix} \right] = \\
\frac{1}{n} tr\begin{pmatrix}
\sigma_1^2 & \sigma_2^2 & \ldots & \sigma_n^2 \\
\sigma_1^2 & \sigma_2^2 & \ldots & \sigma_n^2 \\
\ldots & \ldots & \ldots & \ldots \\
\sigma_1^2 & \sigma_2^2 & \ldots & \sigma_n^2 \\
\end{pmatrix} = \frac{1}{n} \sum_{i=1}^n \sigma_i^2
\end{multline*}
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим регрессию, для которой выполнены условия теоремы Гаусса-Маркова. Уравнение регрессии имеет вид $\hy = \hb_1 i + \hb_2 x_2 + \hb_3 x_3 + \hb_4 x_4$. Известны следующие данные:

\[
X' X = \begin{pmatrix}
100 & 123 & 96 & 109 \\
 & 252 & 125 & 189 \\
 & & 167 & 146 \\
 & & & 168 \\
\end{pmatrix}
\]
\[
(X'X)^{-1} = \begin{pmatrix}
0.03767 & & & \\
-0.06263 & 1.129 & & \\
-0.06247 & 1.107 & 1.110 & \\
0.1003 & -2.192 & -2.170 & 4.292 \\
\end{pmatrix}
\]

$X'y = \begin{pmatrix}
460\\
810\\
615\\
712\\
\end{pmatrix}$; $y'y = 3924$

\begin{enumerate}
\item Найдите $\hCorr(\hb_1, \hb_2)$.
\item Найдите $\hCorr(x_2, x_3)$.
\item Проверьте гипотезу $H_0: \beta_2 = 0$.
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item
\begin{multline*}
RSS = \he'\he y' (I - H) y = y' y - y'Hy = y'y - y' X(X'X)^{-1}X'y ;
\end{multline*}
При этом $y'y=3924$,  а
\begin{multline*}
y' X(X'X)^{-1}X'y= \\
 \begin{pmatrix}
460 & 810 & 615 & 712
\end{pmatrix} \begin{pmatrix}
0.038 & -0.063 & -0.063 & 0.100 \\
-0.063 & 1.129 & 1.107 & -2.192 \\
-0.063 & 1.107 & 1.110 & -2.170 \\
0.100 & -2.192 & -2.170 & 4.292 \\
\end{pmatrix} \cdot \\
\cdot
\begin{pmatrix}
460\\
810\\
615\\
712\\
\end{pmatrix} = 3051.2
\end{multline*}
Итого, $RSS= 3924 - 3051.2 = 872.8$

$\hs_{\e}^2 = \frac{RSS}{n-k} = \frac{872.8}{100-4} = 9.0917$

$\hVar(\hb) = \hs_{\e}^2 (X' X)^{-1} \Rightarrow \hCov(\hb_1, \hb_2) = -0.56939$, $\hVar(\hb_1) = 0.34251$, $\hVar(\hb_2) = 10.269$

$\hCorr(\hb_1, \hb_2) = \frac{\hCov(\hb_1, \hb_2)}{\sqrt{\hVar(\hb_1)}\sqrt{\hVar(\hb_2)}} = -0.30361$

\item(указание) $\hCorr(x_2, x_3) = \frac{\sum (x_{i2} - \overline{x}_2) (x_{i3} - \overline{x}_3)}{\sqrt{\sum (x_{i2} - \overline{x}_2)}\sqrt{\sum (x_{i3} - \overline{x}_3)}}$. Все необходимые величины можно извлечь из матрицы $X' X$ — это величины $\sum x_{i2}$ и $\sum x_{i3}$, а остальное -- из матрицы $X' (I - \pi) X = X' X - X' \pi X = X' X - (\pi X)' \pi X$. При этом имейте в виду, что
$\pi X = \begin{pmatrix}
1 & \overline{x}_1 & \overline{x}_2 & \overline{x}_3 \\
\ldots & \ldots & \ldots & \ldots \\
1 & \overline{x}_1 & \overline{x}_2 & \overline{x}_3 \\
\end{pmatrix}$ и $\overline{x}_1 = 1.23$, $\overline{x}_2 = 0.96$, $\overline{x}_3 = 1.09$

\item $\begin{pmatrix}
\hb_1 \\
\hb_2 \\
\hb_3 \\
\hb_4 \\
\hb_5 \\
\end{pmatrix} = \begin{pmatrix}
0.03767 & -0.06263 & -0.06247 & 0.1003 \\
-0.06263 & 1.129 & 1.107 & -2.192 \\
-0.06247 & 1.107 & 1.110 & -2.170 \\
0.1003 & -2.192 & -2.170 & 4.292 \\
\end{pmatrix} \begin{pmatrix}
460\\
810\\
615\\
712\\
\end{pmatrix} = \begin{pmatrix}
-0.40221 \\
6.1234 \\
5.9097 \\
-7.5256 \\
\end{pmatrix}$

$t = \frac{\hb_2}{\sqrt{\Var(\hb_2)}} \sim t_{100-4}$

$t = \frac{\hb_2}{\sqrt{\Var(\hb_2)}} = \frac{6.1234}{\sqrt{10.269}} = 1.9109 \Rightarrow \hb_2$ — не значим.
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
По данным для 15 фирм ($n = 15$) была оценена производственная функция Кобба-Дугласа: $\ln Q_i = \beta_1 + \beta_2 \ln L_i + \beta_3 \ln K_i + \e_i$. Полученные оценки:
\[\underset{s.e.}{\widehat{\ln Q}} = \underset{(4.48)}{0.5} + \underset{(0.7)}{0.76} \ln L + \underset{(0.138)}{0.19} \ln K\]
где $Q$ — выпуск, $L$ — трудозатраты, $K$ — капиталовложения. Матрица обратная к матрице регрессоров имеет вид:

$(X' X)^{-1} = \begin{pmatrix}
121573 & -19186 & 3718 \\
-19186 & 3030 & -589 \\
3718 & -589 & 116 \\
\end{pmatrix}$

\begin{enumerate}
\item Напишите формулу для несмещённой оценки ковариации $\hCov(\hb_2, \hb_3)$ и вычислите её по имеющимся данным, если это возможно.
\item Проверьте $H_0: \beta_2 + \beta_3 = 1$ при помощи $t$-статистики. Укажите формулу для статистики, а также число степеней свободы.
\item Постройте 95\% доверительный интервал для величины $\beta_2 + \beta_3$.
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item $\hCov\begin{pmatrix}
\hb_1 \\
\hb_2 \\
\hb_3 \\
\end{pmatrix} = \hs_{\e}^2 (X'X)^{-1}$ — несмещённая оценка для ковариационной матрицы \\ МНК-коэффициентов. Действительно, $\E\hCov\begin{pmatrix}
\hb_1 \\
\hb_2 \\
\hb_3 \\
\end{pmatrix} = \E\hs_{\e}^2 (X'X)^{-1} = \sigma_{\e}^2 (X'X)^{-1} = \Cov\begin{pmatrix}
\hb_1 \\
\hb_2 \\
\hb_3 \\
\end{pmatrix}$. Поэтому искомая оценка $\hCov(\hb_2, \hb_3) = \hs_{\e}^2 \left[ (X' X)^{-1} \right]_{23}$, где $\left[ (X' X)^{-1} \right]_{23}$ — элемент матрицы $(X' X)^{-1}$, расположенный во второй строке, 3-м столбце.

Заметим, что $\hs_{\hb_2}^2 = \hs_{\e}^2 \left[ (X' X)^{-1} \right]_{22} \Rightarrow 0.7^2 = \hs_{\e}^2 \cdot (3030) \Rightarrow \hs_{\e}^2 = 0.00016172$

Значит, $\hCov(\hb_2, \hb_3) = 0.00016172 \cdot (-589) = -0.095253$.

\item $t = \frac{\hb_2 + \hb_3 - \beta_2 - \beta_3}{\sqrt{\hVar(\hb_2 + \hb_3)}} \sim t_{n-k}$

Требуется проверить $H_0: \beta_2 + \beta_3 = 1$.

$\hVar(\hb_2 + \hb_3) = \hVar(\hb_2) + \hVar(\hb_3) + 2\hCov(\hb_2, \hb_3) = 0.7^2 + 0.138^2 + 2 \cdot 0.095253 = 0.319044$

$t = \frac{\hb_2 + \hb_3 - \beta_2 - \beta_3}{\sqrt{\hVar(\hb_2 + \hb_3)}} = \frac{0.76 + 0.19 - 1}{\sqrt{0.319044}} = -0.088520674$

Значит, гипотеза не отвергается на любом разумном уровне значимости.

\item Мы знаем, что $\frac{\hb_2 + \hb_3 - \beta_2 - \beta_3}{\sqrt{\hVar(\hb_2 + \hb_3)}} \sim t_{n-k} = t_{15-3}$, поэтому построить доверительный интервал для $\beta_2 + \beta_3$ не составляет труда. $\P \left( \left| \frac{\hb_2 + \hb_3 - \beta_2 - \beta_3}{\sqrt{\hVar(\hb_2 + \hb_3)}} \right| < t^* \right) = 0.95$

Обозначим $se=\sqrt{\hVar(\hb_2 + \hb_3)}$, тогда:

\begin{multline*}
\P \left( \left| \frac{\hb_2 + \hb_3 - \beta_2 - \beta_3}{\sqrt{\hVar(\hb_2 + \hb_3)}} \right| < t^* \right) = \\
\P \left( -t^* se < \hb_2 + \hb_3 - \beta_2 - \beta_3 < t^* se \right) = \\
\P \left( -t^*se  - (\hb_2 + \hb_3) < - \beta_2 - \beta_3  < -(\hb_2 + \hb_3) + t^* se \right) =\\
\P \left( (\hb_2 + \hb_3) + t^* se
> \beta_2 + \beta_3
> (\hb_2 + \hb_3) - t^* se \right)
\end{multline*}
Отсюда получаем доверительный интервал
\begin{multline*}
\beta_2 + \beta_3 \in \\
[(0.76 + 0.19) - 2.16 \cdot 0.319;  (0.76 + 0.19) + 2.16 \cdot 0.319 ]
\end{multline*}
Или $0.26< \beta_2 + \beta_3  < 1.639  $
\end{enumerate}
\end{sol}
\end{problem}



\begin{problem}
Напишите формулу для оценок коэффициентов в множественной регрессии с матрицами. Напишите формулу для ковариационной матрицы оценок.


\begin{sol}

Метод наименьших квадратов:
\[\he'\he\rightarrow \min_{\hb}\]
\[(y-\hy)'(y-\hy)\rightarrow\min_{\hb}\]
\[(y-X\hb)'(y-X\hb)\rightarrow\min_{\hb}\]
\[y'y-\hb' X'y-y'X\hb+\hb'X'X\hb\rightarrow\min_{\hb}\]

Воспользуемся тем, что : $\frac{\partial x'A}{\partial x'}=A'$, $\frac{\partial Ax}{\partial x'}=A$,
$\frac{\partial x'Ax}{\partial x'}=x'(A'+A)$
Условие первого порядка:
\[-2(X'y)'+(X'X+(X'X)')\hb'=0\]
\[-2X'y+2\hb X'X=0\]
и
\[\hb=(X'X)^{-1}X'y\]

\begin{multline*}
\Var(\hat\beta)=\Var((X'X)^{-1}X'y)=(X'X)^{-1}X'\Var(y)((X'X)^{-1}X')'=\\
=(X'X)^{-1}X'\Var(X\beta+\e)X(X'X)^{-1}
\end{multline*}
Так как $\beta$ является константой, то $\Var(X\beta)=0$. Тогда
\begin{multline*}
\Var(\hat\beta)=(X'X)^{-1}X'\Var(X\beta+\e)X(X'X)^{-1}=\\=(X'X)^{-1}X'\Var(\e)X(X'X)^{-1}=
(X'X)^{-1}X'\sigma^2X(X'X)^{-1}=\\=\sigma^2(X'X)^{-1}X'X(X'X)^{-1}=\sigma^2(X'X)^{-1}
\end{multline*}
\end{sol}
\end{problem}


\begin{problem}
<<"040_07">>=
model <- lm(dist ~ speed, data = cars)
model_sum <- summary(model)
hat_sigma <- model_sum$sigma
@

Исследователь оценил зависимость длины тормозного пути в футах от скорости автомобиля в милях в час по данным 1920-х годов.
При построении парной регрессии у него получилась $\hs=
<<"040_07b", echo=FALSE, results="asis">>=
cat(hat_sigma)
@
$ и оценка ковариационной матрицы


<<"040_08", results="asis">>=
xtable(vcov(model))
@


\begin{enumerate}
\item Определите количество наблюдений.
\item Найдите среднюю скорость автомобиля в милях в час.
\end{enumerate}


\begin{sol}
Находим $X'X$, её элементы и есть то, что нужно.
\end{sol}
\end{problem}




\begin{problem}
Дана оценка ковариацонной матрицы,
\[
\Var(\hb)=\begin{pmatrix}
1/3 & -1/3 & 0 \\
-1/3 & 4/3 & -1 \\
0 & -1 & 2
\end{pmatrix}
\]

Найдите $\Var(\hb_1+\hb_2-\hb_3)$


\begin{sol}
\[\Var(\hb_1+\hb_2-\hb_3)=\Var(\hb1)+\Var(\hb2)+\Var(\hb3)+
2\Cov(\hb1,\hb2)-2\Cov(\hb1,\hb3)-2\Cov(\hb2,\hb3)\]

\[\Var(\hb_1+\hb_2-\hb_3)=1/3+4/3+2-2/3+2=5\]
\end{sol}
\end{problem}


\begin{problem}
Методом наименьших квадратов по 5 наблюдениям оценивается модель $y_i=\beta_1+\beta_2 x_{i2}+\beta_3 x_{i3}+\varepsilon_i$. Известно, что
\[
\hb=\begin{pmatrix}
1 \\
2 \\
3
\end{pmatrix}, \;
(X'X)^{-1}= \begin{pmatrix}
1/3 & -1/3 & 0 \\
-1/3 & 4/3 & -1 \\
0 & -1 & 2.0000
\end{pmatrix}
\]
\[
\Var(\hb)=\begin{pmatrix}
1 & -1 & 0 \\
-1 & 4 & -3 \\
0 & -3 & 6
\end{pmatrix}, \; TSS=60
\]

Найдите $\hs^2$ и проверьте гипотезу о значимости регрессии в целом на уровне значимости 5\%.


\begin{sol}
Из того, что $\Var(\hb)=\sigma^2\cdot (X'X)^{-1}$ видно, что $\sigma^2=\frac13$.

$RSS=\sigma^2\cdot(n-k)=1/3\cdot 2=2/3$
\[R^2=49\frac13/50=148/150\]


$F=\frac{R^2/(k-1)}{(1-R^2)/(n-k)}=\frac{148/2}{2/2}=74$
$F^{crit}_{0.05, 2,2}=19<74$
гипотеза отвергается, регрессия значима.

\end{sol}
\end{problem}




\begin{problem}
По 30 наблюдениям оценивается модель парной регрессии $y_t=\beta_1 + \beta_2 x_t + \e_t$. Известны матрицы:
\[
X'X=\begin{pmatrix}
30 & 10 \\
10 & 40
\end{pmatrix}, \;
X'y=\begin{pmatrix}
40 \\
70
\end{pmatrix}, \;
y'y=80
\]

\begin{enumerate}
\item Оцените модель парной регрессии по 30 наблюдениям.
\item К имеющимся 30 наблюдениям добавили ещё одно, $x_{31}=1$, $y_{31}=2$. Оцените модель по 31 наблюдению.
\item Проведите тест Чоу на прогнозную силу. То есть проверьте, что зависимость на выборке из 31 наблюдения совпадает с зависимостью по 30 наблюдениям.
\end{enumerate}


\begin{sol}

\begin{enumerate}
\item
$\hb=(X'X)^{-1}X'y=
\frac{1}{110}\begin{pmatrix}
40&-10\\
-10&30
\end{pmatrix}
\begin{pmatrix}
40\\
70
\end{pmatrix}=
\frac{1}{11}
\begin{pmatrix}
9\\
17
\end{pmatrix}
$

\item
$X'X=
\begin{pmatrix}
n&\sum^n_{i=1}x_i\\
\sum^n_{i=1}x_i&\sum^n_{i=1}x_i^2
\end{pmatrix} $

Теперь
$X'X=\begin{pmatrix}
31&11\\
11&41
\end{pmatrix}
$,

$X'y=\begin{pmatrix}
1\ldots 1    &1\\
X_{old, no\>ones}&1
\end{pmatrix}
\begin{pmatrix}
y_{old}\\
2
\end{pmatrix}=
\begin{pmatrix}
42\\
72
\end{pmatrix}
$

\[
\hb^*=\frac{1}{1150}\begin{pmatrix}
41&-11\\
-11&31
\end{pmatrix}
\begin{pmatrix}
42\\
72
\end{pmatrix}=
\frac{1}{115}
\begin{pmatrix}
93\\
177
\end{pmatrix}
\]

\item
$RSS=y'(I-X(X'X)^{-1}X')y=y'y-y'X(X'X)^{-1}X'y=y'y-(X'y)'(X'X)^{-1}X'y$

$RSS_{old}=80-\begin{pmatrix}
40&70
\end{pmatrix}\frac{1}{11}
\begin{pmatrix}
9\\
17
\end{pmatrix}<0
$

\todo[inline]{что-то здесь не так}

\end{enumerate}
\end{sol}
\end{problem}


\begin{problem}
Как известно, $\hy=H y$, где матрица-шляпница $H$ задаётся формулой $H=X(X'X)^{-1}X'$.
\begin{enumerate}
\item Является ли вектор остатков $\he$ собственным вектором матрицы $H$? Если да, то какое собственное число ему соответствует?
\item Является ли вектор прогнозов $\hy$ собственным вектором матрицы $H$? Если да, то какое собственное число ему соответствует?
\item Приведите ещё примеры собственных векторов матрицы $H$ и найдите соответствующие им собственные числа.
\end{enumerate}


\begin{sol}
У $\he$ собственное число — $\lambda=0$, у $\hy$ собственное число — $\lambda=1$. Например, столбцы матрицы $X$ или их линейные комбинации являются собственными векторами с числом $\lambda=1$.
\end{sol}
\end{problem}




\begin{problem}
Эконометресса Эвелина оценила классическую линейную модель $y=X\b + \e$ методом наименьших квадратов, а затем, используя полученную оценку $\hb$, строит прогноз за пределы выборки. То есть истинные вневыборочные значения вектора $w$ получаются по формуле $w=Z\b + u$, а прогнозы Эвелина строит по формуле $\hat w = Z\hb$. Ошибки $\e_i$ и $u_j$ имеют одинаковое распределение и некоррелированы между собой. Найдите математическое ожидание суммы квадратов ошибок прогнозов,
\[
\E\left(  \sum_i (w_i - \hat w_i)^2  \right)
\]


\begin{sol}
$\sigma^2( n_2  + \tr((X'X)^{-1}Z'Z))$
\todo[inline]{проверить формулу!}
\end{sol}
\end{problem}


\begin{problem}
Вениамин оценил модель множественной регрессии. При этом оказалось, что $y_2=3$, $\hy_2=5$ и $H_{22}=0.7$, где $H$ — матрица-шляпница, т.е. $H=X(X'X)^{-1}X'$. Вдруг Вениамин отпрянул в ужасе! Он обнаружил, что второе наблюдение было внесено неправильно! Вместо $y_2=3$ должно быть $y_2=0.3$!

Как изменится $\hy_2$, если исправить неправильно внесённое наблюдение?
\begin{sol}
\[
\hy_2^{new} = \hy_2^{old} + H_{22} \Delta y_2 = 5 + 0.7(-2.7) = 3.11
\]
\end{sol}
\end{problem}



\begin{problem}
Выпишите в явном виде матрицу-шляпницу $H=X(X'X)^{-1}X'$ для моделей
\begin{enumerate}
\item $y_i = \beta + \e_i$;
\item $y_i = \beta x_i + \e_i$;
\item $y_i = \beta_1 + \beta_2 x_i + \e_i$.
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item $H = \frac{1}{n} S$, где $S$ — матрица строевого леса, то есть матрица из единиц размера $n\times n$.
\item \ldots
\item \ldots
\end{enumerate}
\end{sol}

\end{problem}

\begin{problem}
Рассмотрим модель $y=X\beta + \e$, где $\E(\e|X)=0$ и $\Var(\e|X)=\sigma^2 I$.
\begin{enumerate}
\item Вспомните или найдите $\Var(\hy|X)$ и $\Var(y|X)$.
\item В каких пределах может лежать произвольный элемент матрицы-шляпницы $H=X(X'X)^{-1}X'$?
\item Вениамин оценил множественную регрессию $y_i = \beta_1 + \beta_2 x_i + \beta_3 z_i + \e_i$. А после этого его попросили построить прогноз $y$ для $x=1$ и $z=2$. Вениамин неожиданно обнаружил, что в его данных в 42-м наблюдении как раз $x_{42}=1$, а $z_{42}=2$. Что ему лучше взять в качестве прогноза, $y_{42}$ или $\hy_{42}$?
\end{enumerate}

\begin{sol}
$0 \leq H_{ii} \leq 1$,  лучше взять $\hy_{42}$

Сделаем вектор-«пробник» $w$, где на первой позиции 1, а остальные — нули. Заметим, что $w'Hw=H_{11}$. С другой стороны, $w'Hw=w'H'Hw=(Hw)'Hw \geq 0$. Аналогично, $w'(I-H)w=1-H_{11}\geq 0$.
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим случайный вектор $w$, в котором все компоненты независимы и равновероятно равны нулю или единице. Определим вектор $v$ по формуле $v=1-w$.

\begin{enumerate}
\item Ортогональны ли вектора $v$ и $w$?
\item Найдите ковариационную матрицу векторов $v$ и $w$.
\end{enumerate}
\begin{sol}
Да, ортогональны, т.к. $\sum v_i w_i = 0$. $\Cov(v,w)=-0.25 I$.
\end{sol}
\end{problem}


\begin{problem}
Известна корреляционная матрица случайного вектора $x$, $C$. Известен вектор корреляций случайной величины $y$ и вектора $x$, $d$.

Найдите максимальную корреляцию между $y$ и линейной комбинацией случайных величин из вектора $x$

\begin{sol}
$d'C^{-1}d$
\end{sol}
\end{problem}

\begin{problem}
  Имеется всего два наблюдения, $y_1 = 1$, $y_2 = -3$. С помощью критериев $AIC$ и $BIC$ сравните две модели. Модель А: $y_i \sim \cN(0; 1)$ и независимы, модель Б: $y_i \sim \cN(\mu; 1)$ и независимы.
\begin{sol}
  $RSS_A = 1^2 + (-3)^2 = 10$, $RSS_B= 2^2 + 2^2 = 8$. $k_A = 0$, $k_B=1$.
\end{sol}
\end{problem}


\begin{problem}
Бизнесмен Сидоров предоставляет платные услуги по построению регрессий. Оценка одной регрессии независимо от числа регрессоров стоит 100 рублей. У умного, но бедного студента Петрова нет своего компьютера. У студента Петрова всего 100 рублей, а для курсовой работы ему нужно оценить три регрессии.

Как следует поступить студенту Петрову?
\begin{sol}
Трюк идейно такой, на примере одной объясняющей переменной:

\begin{enumerate}
\item Мы хотим оценить две регрессии: $y_a$ на $x_a$ и $y_b$ на $x_b$.
\item Записываем $y_a$, а под ним $y_b$, получаем $y$.
\item Напротив $y_a$ регрессорами будут  $x_a$ и столбец нулей.
\item Напротив $y_b$ регрессорами будут столбец нулей и $x_b$.
\item Строим одну регрессию
\[
y_i = \beta_a d_{ai}x_{ai} + \beta_b d_{bi}x_{bi} + u_i.
\]
\end{enumerate}

\end{sol}
\end{problem}



\begin{problem}
При описании априорного распределения в TVP-BVAR модели Primiceri возникает примерно следующая задача. Вектор $u_t$ имеет размерность $3\times 1$ и представим в виде $u_t = (u_{1t}, u_{2t}, u_{3t})'$. При $t \neq s$ вектор $u_t$ не зависит от вектора $u_s$. Распределение вектора $u_t$ не зависит от $t$. Компоненты вектора $u_t$ зависимы, однако компоненты вектора $v_t = Au_t$ независимы для нижнетреугольной матрицы $A$ вида
\[
A = \begin{pmatrix}
1      & 0      & 0 \\
a_{21} & 1      & 0 \\
a_{31} & a_{32} & 1 \\
\end{pmatrix}.
\]

Как зная $u_1$, $u_2$, \ldots, $u_T$ оценить неизвестные элементы матрицы $A$?
\begin{sol}
Чтобы оценить $a_{21}$ — построить регрессию $u_{2t}$ на $u_{1t}$ и взять оценку коэффициента с противоположным знаком.

Чтобы оценить $a_{31}$, $a_{32}$ — построить регрессию $u_{3t}$ на $u_{1t}$ и $u_{2t}$ и взять оценки коэффициентов с противоположным знаком.
\end{sol}
\end{problem}

\begin{problem}
Рассмотрим две модели: $y = X\beta + u$ и $z = X\gamma + v$. В этих двух моделях одинаковые матрицы регрессоров, но разные зависимые переменные. Для каждой модели в отдельности выполнены стандартные предпосылки и $\Cov(u, v) = a \cdot I$. Введём матрицу $Y$, в которую столбцами поместим $y$, а затем $z$, $Y = (y, z)$, и матрицу коэффициентов $B$, $B=(\beta, \gamma)$. Запись $\vec B$ означает векторизацию матрицы $B$, то есть это столбцы матрицы $B$, записанные друг под другом в один столбец.
\begin{enumerate}
\item Выпишите формулу для $\hat B$.
\item Предложите несмещённую оценку для параметра $a$.
\item Найдите $\Var(\vec\hat B)$.
\end{enumerate}
\begin{sol}
\[
\hat B = (X'X)^{-1}X'Y
\]

\[
\hat a = \hat u' \hat v / (n - k)
\]
\end{sol}
\end{problem}

\begin{problem}
\todo[inline]{доделать!}
У аккуратной эконометрессы Авдотьи все объясняющие переменные ортогональны друг другу и центрированы. Сначала Авдотья оценивает парные регрессии игрека на каждую объясняющую переменную $x_j$ по отдельности, $\hy_i = \hb_j x_ij$. Затем Авдотья оценивает множественную регрессию на  все объясняющие переменные сразу, $\hy_i = \hat\gamma_1 x_{i1} + \ldots + \hat\gamma_{k} x_{ik}$.

\begin{enumerate}
\item Как связаны коэффициенты при объясняющих переменных в этих регрессиях?
\item Как связаны $R^2$ в этих регрессиях, если Авдотья дополнительно нормирует регрессоры, чтобы они имели единичную длину?
\end{enumerate}

\begin{sol}
При ортогональных регрессорах $\hb_j = \hat\gamma_j$. Если регрессоры дополнительно стандартизированы, то $R^2 = R^2_1 + \ldots + R^2_j$.
\end{sol}
\end{problem}


\begin{problem}
Машенька построила регрессию с тремя регрессорами помимо константы и 11-ю наблюдениями с $R^2=0.8$. Вовочка назло Машеньке случайно переставляет значения зависимой переменной и просит Машеньку оценить регрессию заново.

Какой ожидаемый $R^2$ получит Машенька?
\begin{sol}
Заметим, что $R^2$ не изменится, если перейти к ортонормальному базису в пространстве регрессоров. В этом базисе, кстати, окажется, что $R^2 = \frac{\hb'\hb}{TSS}$. В силу ортогональности регрессоров $R^2$ распадётся в сумму $R^2_j$, где $R^2_j$ — коэффициент в регрессии зависимой переменной на константу и $j$-ый регрессор.

В силу соответствующей одномерной задачи $\E(R^2_j) = \frac{1}{n-1}$. Следовательно, $\E(R^2) = \frac{k-1}{n-1}=\frac{3}{10}$.
\end{sol}
\end{problem}

\begin{problem}
Эконометресса Рапунцель строит регрессию $y$ на $X$. А эконометрист Флин использует $QR$-разложение для матрицы $X$, то есть представляет $X$ в виде $X=QR$. Другими словами, Флин переходит к ортонормированному базису в пространстве столбцов матрицы $X$: $Q'Q=I$, $R$ — верхнетреугольная обратимая. А затем Флин строит регрессию $y$ на $Q$.

Верно ли, что Рапунцель и Флин получат одинаковые
\begin{enumerate}
  \item оценки коэффициентов?
  \item прогнозы $\hy$?
  \item ковариационные матрицы прогнозов $\Var(\hy|X)$?
\end{enumerate}

\begin{sol}
Коэффициенты выйдут разные, а прогнозы и ковариационные матрицы прогнозов — одинаковые.
Заметим, что:
\[
X'X = R'Q'QR=R'R.
\]
Матрица-шляпница для Рапунцель:
\[
H_X = X(X'X)^{-1}X'=QR(R'R)^{-1}R'Q' = QRR^{-1}R'^{-1}R'Q'=QQ'.
\]
Матрица-шляпница для Флина:
\[
H_Q = Q(Q'Q)^{-1}Q' = QIQ'=QQ'.
\]
Осталось вспомнить, что всё определяется матрицей-шляпницей, $\hy=Hy$, $\Var(\hy|X)=\sigma^2 H$.
\end{sol}
\end{problem}




\Closesolutionfile{solution_file}
