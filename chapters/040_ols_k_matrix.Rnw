\chapter{МНК с матрицами и вероятностями}


\begin{problem} 
Пусть $y = X\beta + \e$ --- регрессионная модель.
\begin{enumerate}
\item Сформулируйте теорему Гаусса-Маркова
\item Верно ли, что оценка $\hb = (X'X)^{-1}X'y$ несмещённая?
\item В условиях теоремы Гаусса-Маркова найдите ковариационную матрицу $\hb$
\end{enumerate}
\end{problem}

\begin{solution}
\end{solution}

\begin{problem} 
Пусть $y = X\beta + \e$ --- регрессионная модель и $\tilde{\beta} = ((X'X)^{-1}X'+ A)y$ --- несмещённая оценка вектора неизвестных параметров $\beta$. Верно ли, что $AX=0$?
\end{problem}

\begin{solution}
\end{solution}




\begin{problem}
 Пусть $y = X\beta + \e$ --- регрессионная модель, $X = \begin{pmatrix} 1 & 0 \\ 1 & 1 \\ 1 & 1 \end{pmatrix}$, $y = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix}$, $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \end{pmatrix}$, $\e = \begin{pmatrix} \e_1 \\ \e_2 \\ \e_3 \end{pmatrix}$, $\E(\e)$ = 0, $\Var(\e) = \sigma^2 I$. Найдите коэффициент корреляции $\Corr(\hb_1,\hb_2)$.
<<echo=FALSE>>=
X <- matrix(as.integer(c(1,1,1,0,1,1)), nrow=3, byrow = FALSE, dimnames = NULL)
y <- c(1,2,3)
@
\end{problem}

\begin{solution}
\end{solution}


\begin{problem} 
Пусть $y = X\beta + \e$ --- регрессионная модель, где $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$. Пусть $Z = XD$, где $D = \begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}$. Рассмотрите <<новую>> регрессионную модель $y = Z\alpha + u$, где $\alpha = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}$. Определите, как выражаются <<новые>> МНК-коэффициенты через <<старые>>.
<<echo=FALSE,results='asis'>>=
alpha <- matrix(as.integer(c(1,0,0,1,1,0,0,1,1)), nrow=3, byrow = FALSE, dimnames = NULL)
@
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Пусть $y = X\beta + \e$ --- регрессионная модель, где $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$. Пусть $Z = XD$, где $D = \begin{pmatrix} 1 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}$. Рассмотрите <<новую>> регрессионную модель $y = Z\alpha + u$, где $\alpha = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}$. Определите, как выражаются <<новые>> МНК-коэффициенты через <<старые>>.
<<echo=FALSE,results='asis'>>=
alpha2 <- matrix(as.integer(c(1,0,0,1,1,0,1,1,1)), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Пусть $y = X\beta + \e$ --- регрессионная модель, где $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$. Пусть $Z = XD$, где $D = \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 1 \\ 0 & 0 & 1 \end{pmatrix}$. Рассмотрите <<новую>> регрессионную модель $y = Z\alpha + u$, где $\alpha = \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_3 \end{pmatrix}$. Определите, как выражаются <<новые>> МНК-коэффициенты через <<старые>>.
<<echo=FALSE,results='asis'>>=
alpha3 <- matrix(as.integer(c(1,0,0,0,1,0,0,1,1)), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
@
\end{problem}

\begin{solution}
\end{solution}


\begin{problem}
Пусть $y = X\beta + \e$ --- регрессионная модель. Верно ли, что $\hat{\e}'\hat{y}=0$ и $\hat{y}'\hat{\e}=0$?
\end{problem}
\begin{solution}
да, да
\end{solution}

\begin{problem}
Пусть $y = X\beta + \e$ --- регрессионная модель, где $\E(\e)=0$, $\Var(\e)=\sigma_{\e}^2 I$. Пусть $A$ --- неслучайная матрица размера $k \times k$, $\det(A) \not= 0.$ Совершается преобразование регрессоров по правилу $Z=XA$. В преобразованных регрессорах уравнение выглядит так: $y = Z\gamma + u$, где $\E(u)=0$, $\Var(u)=\sigma_{u}^2 I.$

\begin{enumerate}
\item Как связаны между собой МНК-оценки $\hb$ и $\hat{\gamma}$?
\item Как связаны между собой векторы остатков регрессий?
\item Как связаны между собой прогнозные значения, полученные по двум регрессиям?
\end{enumerate}
\end{problem}

\begin{solution}
\begin{enumerate}
\item 
\begin{multline}
\hat{\gamma} = (Z'Z)^{-1}Z'y = A^{-1}(X'X)^{-1}(A')^{-1}A'X'y =\\
 A^{-1}(X'X)^{-1} X'y = A^{-1}\hb
\end{multline}
\item $\hat{u} = y - Z\hat{\gamma} = y - XAA^{-1}\hb = y - X\hb = \hat{\e}$
\item Пусть $z^0 = \begin{pmatrix} 1 & z_1^0 & \dots & z_{k-1}^0 \end{pmatrix}$ --- вектор размера $1 \times k$ и $x^0 = \begin{pmatrix} 1 & x_1^0 & \dots & x_{k-1}^0 \end{pmatrix}$ --- вектор размера $1 \times k$. Оба эти вектора представляют собой значения факторов. Тогда $z^0 = x^0 A$ и прогнозное значение для регрессии с преобразованными факторами равно $z^0 \hat{\gamma} = x^0 AA^{-1} \hb = x^0 \hb$ прогнозному значению для регрессии с исходными факторами. 
\end{enumerate}
\end{solution}


\begin{problem}
Рассмотрим оценку вида $\tilde{\beta} = ((X'X)^{-1} + \gamma I)X'y$ для вектора коэффициентов регрессионного уравнения $y = X\beta + \e$, удовлетворяющего условиям классической регрессионной модели. Найдите $\E(\tilde{\beta})$ и $\Var(\tilde{\beta}).$ 
\end{problem}

\begin{solution}
\begin{enumerate}
\item 
\begin{multline}
\E(\tilde{\beta}) = ((X'X)^{-1} + \gamma I)X'\E(y) = \\
 ((X'X)^{-1} + \gamma I)X'X\beta = \beta + \gamma X'X\beta
\end{multline}
\item 
\begin{multline}
\Var(\tilde{\beta}) = \Var(((X'X)^{-1} + \gamma I)X'y) = \\
 \Var(((X'X)^{-1} + \gamma I)X'\e) = \\
 (((X'X)^{-1} + \gamma I)X')\Var(\e)(((X'X)^{-1} + \gamma I)X')'=  \\
  (((X'X)^{-1} + \gamma I)X')\sigma_{\e}^2 I(((X'X)^{-1} + \gamma I)X')'= \\
  \sigma_{\e}^2((X'X)^{-1} + \gamma I)X'X((X'X)^{-1} + \gamma I) = \\
  \sigma_{\e}^2((X'X)^{-1} + \gamma I)(I + \gamma X'X) =\\
   \sigma_{\e}^2((X'X)^{-1} + 2\gamma I + \gamma ^2 X'X)
\end{multline}
\end{enumerate}
\end{solution}



\begin{problem}
Верно ли, что при невырожденном преобразовании факторов $R^2$ не меняется? А именно, пусть заданы две регрессионные модели: $y = X\beta + \e$ и $y = Z\alpha + u$, где $y$ --- вектор размера $n \times 1$, $X$ и $Z$ --- матрицы размера $n \times k$, $\beta$ и $\alpha$ --- вектора рамзера $k \times 1$, $\e$ и $u$ --- вектора размера $n \times 1$, а также $Z=XD$, $\det(D) \not= 0.$ Верно ли, что коэффициенты детерминации представленных выше моделей равны между собой?
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
Верно ли, что при невырожденном преобразовании факторов $RSS$ не меняется. А именно, пусть заданы две регрессиионные модели: $y = X\beta + \e$ и $y = Z\alpha + u$, где $y$ --- вектор размера $n \times 1$, $X$ и $Z$ --- матрицы размера $n \times k$, $\beta$ и $\alpha$ --- вектора рамзера $k \times 1$, $\e$ и $u$ --- вектора размера $n \times 1$, а также $Z=XD$, $\det(D) \not= 0.$ Верно ли, что сумма квадратов остатков в представленных выше моделях равны между собой?
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
Пусть регрессионная модель $y_i = \beta_1 + \beta_2 x_{i2} + \beta_3 x_{i3} + \e_i$, $i = 1, \ldots, n$, задана в матричном виде при помощи уравнения $y = X \beta + \e$, где $\beta =  \begin{pmatrix}
\beta_1 & \beta_2 & \beta_3\\
\end{pmatrix} ^T$. Известно, что $\E \e = 0$ и $\Var (\e) = 4 \cdot I$. Известно также, что:

$y =  \begin{pmatrix}
1 \\
2 \\
3 \\
4 \\
5 \\
\end{pmatrix} $, $X =  \begin{pmatrix}
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 1 & 0 \\
1 & 1 & 1 \\
\end{pmatrix} $

Для удобства расчётов ниже приведены матрицы:

$X^T X =  \begin{pmatrix}
5 & 3 & 1 \\
3 & 3 & 1 \\
1 & 1 & 1 \\
\end{pmatrix} $ и $(X^T X)^{-1} =  \begin{pmatrix}
0.5 & -0.5 & 0 \\
-0.5 & 1 & -0.5 \\
0 & -0.5 & 1.5 \\
\end{pmatrix} $.

Найдите:
\begin{enumerate}
\item $\Var (\e_1)$
\item $\Var (\beta_1)$
\item $\Var (\hb_1)$
\item $\widehat{\Var }(\hb_1)$
\item $\E (\hb_1^2) - \beta_1^2$
\item $\Cov (\hb_2, \hb_3)$
\item $\widehat{\Cov }(\hb_2, \hb_3)$
\item $\Var (\hb_2 - \hb_3)$
\item $\widehat{\Var }(\hb_2 - \hb_3)$
\item $\Var (\beta_2 - \beta_3)$
\item $\Corr (\hb_2, \hb_3)$
\item $\widehat{\Corr}(\hb_2, \hb_3)$
\item $\E (\hat{\sigma}^2)$
\item $\hat{\sigma}^2$
\end{enumerate}
\end{problem}
\begin{solution}
\end{solution}



\begin{problem}
Пусть $y_i = \beta_1 + \beta_2 x_{i2} + \beta_3 x_{i3} + \e_i$ --- регрессионная модель, где $X = \begin{pmatrix} 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 1 \end{pmatrix}$, $y = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \\ 5 \end{pmatrix}$, $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$, $\e = \begin{pmatrix} \e_1 \\ \e_2 \\ \e_3 \\ \e_4 \\ \e_5  \end{pmatrix}$, ошибки $\e_i$ независимы и нормально распределены с $\E(\e)$ = 0, $Var(\e) = \sigma^2 I$. Для удобства расчётов даны матрицы: $X'X = \begin{pmatrix} 5 & 2 & 1 \\ 2 & 2 & 1\\ 1 & 1 & 1 \end{pmatrix}$ и $(X'X)^{-1}= \begin{pmatrix} 0.3333 & -0.3333 & 0.0000 \\ -0.3333 & 1.3333 & -1.0000 \\ 0.0000 & -1.0000 & 2.0000 \end{pmatrix}$


<<echo=FALSE,results='asis'>>=
x <- matrix(as.integer(c(1,1,1,1,1,0,0,0,1,1,0,0,0,0,1)), nrow=5, ncol=3, byrow = FALSE, dimnames = NULL)
y <- matrix(as.integer(c(1,2,3,4,5)), nrow=5, ncol=1, byrow = FALSE, dimnames = NULL)
xx <- matrix(as.integer(c(5,2,1,2,2,1,1,1,1)), nrow=3, ncol=3, byrow = FALSE, dimnames = NULL)
x_x <- solve(xx)
@


\begin{enumerate}
\item Укажите число наблюдений
\item Укажите число регрессоров в модели, учитывая свободный член
\item Найдите $TSS = \sum_{i=1}^n (y_i - \bar y)^2$
\item Найдите $RSS = \sum_{i=1}^n (y_i - \hat{y_i})^2$
\item Методом МНК найдите оценку для вектора неизвестных коэффициентов
\item Чему равен $R^2$ в модели? Прокомментируйте полученное значение с точки зрения качества оценённого уравнения регрессии
\item Сформулируйте основную и альтернативную гипотезы, которые соответствуют тесту на значимость переменной $x_2$ в уравнении регрессии
\item Протестируйте на значимость переменную $x_2$ в уравнении регрессии на уровне значимости $10\%$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод о значимости переменной $x_2$
\end{enumerate}
\item Найдите $P-$значение, соответствующее наблюдаемому значению тестовой статистики $(T_{obs})$ из предыдущего пункта. На основе полученного $P-$значения сделайте вывод о значимости переменной $x_2$
\item На уровне значимости $10\%$ проверьте гипотезу $H_0: \beta_2 = 1$ против альтернативной $H_a: \beta_2 \not= 1$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item На уровне значимости $10\%$ проверьте гипотезу $H_0: \beta_2 = 1$ против альтернативной $H_a: \beta_2 > 1$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item На уровне значимости $10\%$ проверьте гипотезу $H_0: \beta_2 = 1$ против альтернативной $H_a: \beta_2 < 1$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item Сформулируйте основную гипотезу, которая соответствует тесту на значимость регрессии <<в целом>>
\item На уровне значимости $5\%$ проверьте гипотезу о значимости регрессии <<в целом>>:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item Найдите $P-$значение, соответствующее наблюдаемому значению тестовой статистики $(T_{obs})$ из предыдущего пункта. На основе полученного $P-$значения сделайте вывод о значимости регрессии <<в целом>>
\item На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_2 + \beta_3 = 2$ против альтернативной $H_a: \beta_2 + \beta_3 \not= 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_2 + \beta_3 = 2$ против альтернативной $H_a: \beta_2 + \beta_3 > 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\item На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_2 + \beta_3 = 2$ против альтернативной $H_a: \beta_2 + \beta_3 < 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\end{enumerate}
\end{problem}

\begin{solution}
\begin{enumerate}
\item $n = 5$
\item $k = 3$
\item $TSS = 10$
\item $RSS = 2$
\item $\hb = \begin{pmatrix} \hb_1 \\ \hb_2 \\ \hb_3 \end{pmatrix} = (X'X)^{-1}X'y = \begin{pmatrix} 2 \\ 2 \\ 1 \end{pmatrix}$
\item $R^2 = 1 - \frac {RSS}{TSS} = 0.8.$ $R^2$ высокий, построенная эконометрическая модель <<хорошо>> описывает данные
\item Основная гипотеза --- $H_0: \beta_2 = 0$, альтернативная гипотеза --- $H_a: \beta_2 \not= 0$
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 - \beta_2}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - \beta_2}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hb_2 - 0}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - 0}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-0}{\sqrt{{\frac{2}{5-3}}1.3333}} = 1.7321$
\item Нижняя граница $= -2.920$, верхняя граница $= 2.920$
\item Поскольку $T_{obs} = 1.7321$, что принадлежит промежутку от -2.920 до 2.920, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item $p-value(T_{obs}) = \mathbb{P}(|T|>|T_{obs}|) = 2F_{T}(|T_{obs}|)$, где $F_{T}(|T_{obs}|)$ --- функция распределения $t-$распределения с $n - k = 5 - 3 = 2$ степенями свободы в точке $|T_{obs}|$. $p-value(T_{obs}) = 2tcdf(-|T_{obs}|, n - k) = 2tcdf(-1.7321,2) = 0.2253$. Поскольку $P-$значение превосходит уровень значимости $10\%$, то основная гипотеза --- $H_0: \beta_2 = 0$ не может быть отвергнута
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 - \beta_2}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - \beta_2}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hb_2 - 1}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - 1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-1}{\sqrt{{\frac{2}{5-3}}1.3333}} = 0.8660$
\item Нижняя граница $= -2.920$, верхняя граница $= 2.920$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от -2.920 до 2.920, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 - \beta_2}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - \beta_2}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hb_2 - 1}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - 1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-1}{\sqrt{{\frac{2}{5-3}}1.3333}} = 0.8660$
\item Нижняя граница $= -\infty$, верхняя граница $= 1.8856$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $-\infty$ до $1.8856$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 - \beta_2}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - \beta_2}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}}; n = 5; k = 3$
\item $T \sim t(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{\hb_2 - 1}{\sqrt{\widehat{Var}(\hb_2)}} = \frac {\hb_2 - 1}{\sqrt{{\frac{RSS}{n-k}}[(X'X)^{-1}]_{22}}} = \frac{2-1}{\sqrt{{\frac{2}{5-3}}1.3333}} = 0.8660$
\item Нижняя граница $= -1.8856$, верхняя граница $= +\infty$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $-1.8856$ до $+\infty$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $10\%$
\end{enumerate}
\item Основная гипотеза --- $H_0: \beta_2 = \beta_3 = 0$, альтернативная гипотеза --- $H_a: |\beta_2| + |\beta_3| > 0$
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{R^2}{1 - R^2} \cdot \frac{n-k}{k}; n = 5; k = 3$
\item $T \sim F(n-k); n = 5; k = 3$
\item $T_{obs} = \frac{R^2}{1 - R^2} \cdot \frac{n-k}{k} = \frac{0.8}{1 - 0.8} \cdot \frac{5-3}{2} = 4$
\item Нижняя граница $= 0$, верхняя граница $= 19$
\item Поскольку $T_{obs} = 4$, что принадлежит промежутку от $0$ до $19$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$. Следовательно, регрессия в целом незначима. Напомним, что $R^2 = 0.8$, то есть он высокий. Но при этом регрессия <<в целом>> незначима. Такой эффект может возникать при малом объёме выборки, например, таком, как в данной задаче
\end{enumerate}
\item $p-value(T_{obs}) = \mathbb{P}(|T|>|T_{obs}|) = 2F_{T}(|T_{obs}|)$, где $F_{T}(|T_{obs}|)$ --- функция распределения $F-$распределения c $k = 3$ и $n - k = 5 - 3 = 2$ степенями свободы в точке $T_{obs}$. $p-value(T_{obs}) = 1 - fcdf(-|T_{obs}|, n - k) = 1 - fcdf(4,2) = 0.2$. Поскольку $P-$значение превосходит уровень значимости $10\%$, то основная гипотеза --- $H_0: \beta_2 = \beta_3 = 0$ не может быть отвергнута. Таким образом, регрессия <<в целом>> незначима
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 + \hb_3 - (\beta_2 + \beta_3)}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}}$, где $\widehat{\Var}(\hb_2 + \hb_3) = \widehat{\Var}(\hb_2) + \widehat{\Var}(\hb_3) + 2\widehat{\Cov}(\hb_2;\hb_3) = \hat{\sigma}^2 [(X'X)^{-1}]_{22} + 2\hat{\sigma}^2 [(X'X)^{-1}]_{23} + \hat{\sigma}^2 [(X'X)^{-1}]_{33}= \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33})$
\item $T \sim t(n-k); n = 5; k = 3$
\item $\widehat{\Var}(\hb_2 + \hb_3) = \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33}) = \frac{2}{5 - 3} (1.3333 + 2(-1.0000) + 2.0000) = 1.3333.$ Тогда $T_{obs} = \frac{\hb_2 + \hb_3 - 2}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} = \frac{2 + 1 - 2}{\sqrt{1.3333}} = 0.8660$
\item Нижняя граница $= - 4.3027$, верхняя граница $= 4.3027$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $- 4.3027$ до $4.3027$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 + \hb_3 - (\beta_2 + \beta_3)}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}}$, где $\widehat{\Var}(\hb_2 + \hb_3) = \widehat{\Var}(\hb_2) + \widehat{\Var}(\hb_3) + 2\widehat{\Cov}(\hb_2;\hb_3) = \hat{\sigma}^2 [(X'X)^{-1}]_{22} + 2\hat{\sigma}^2 [(X'X)^{-1}]_{23} + \hat{\sigma}^2 [(X'X)^{-1}]_{33}= \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33})$
\item $T \sim t(n-k); n = 5; k = 3$
\item $\widehat{\Var}(\hb_2 + \hb_3) = \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33}) = \frac{2}{5 - 3} (1.3333 + 2(-1.0000) + 2.0000) = 1.3333.$ Тогда $T_{obs} = \frac{\hb_2 + \hb_3 - 2}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} = \frac{2 + 1 - 2}{\sqrt{1.3333}} = 0.8660$
\item Нижняя граница $= - \infty$, верхняя граница $= 2.9200$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $- \infty$ до $2.9200$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$
\end{enumerate}
\item Проверка гипотезы
\begin{enumerate}
\item $T = \frac{\hb_2 + \hb_3 - (\beta_2 + \beta_3)}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}}$, где $\widehat{\Var}(\hb_2 + \hb_3) = \widehat{\Var}(\hb_2) + \widehat{\Var}(\hb_3) + 2\widehat{\Cov}(\hb_2;\hb_3) = \hat{\sigma}^2 [(X'X)^{-1}]_{22} + 2\hat{\sigma}^2 [(X'X)^{-1}]_{23} + \hat{\sigma}^2 [(X'X)^{-1}]_{33}= \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33})$
\item $T \sim t(n-k); n = 5; k = 3$
\item $\widehat{\Var}(\hb_2 + \hb_3) = \frac{RSS}{n - k}([(X'X)^{-1}]_{22} + 2[(X'X)^{-1}]_{23} + [(X'X)^{-1}]_{33}) = \frac{2}{5 - 3} (1.3333 + 2(-1.0000) + 2.0000) = 1.3333.$ Тогда $T_{obs} = \frac{\hb_2 + \hb_3 - 2}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} = \frac{2 + 1 - 2}{\sqrt{1.3333}} = 0.8660$
\item Нижняя граница $= - 2.9200$, верхняя граница $= + \infty$
\item Поскольку $T_{obs} = 0.8660$, что принадлежит промежутку от $-  2.9200$ до $+ \infty$, то на основе имеющихся данных нельзя отвергнуть основную гипотезу на уровне значимости $5\%$
\end{enumerate}
\end{enumerate}
\end{solution}




\begin{problem}
Пусть $y = X\beta + \e$ --- регрессионная модель, где $X = \begin{pmatrix} 1 & 0 & 0 \\ 1 & 0 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 0 \\ 1 & 1 & 1 \end{pmatrix}$, $y = \begin{pmatrix} 1 \\ 2 \\ 3 \\ 4 \\ 5 \end{pmatrix}$, $\beta = \begin{pmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{pmatrix}$, $\e = \begin{pmatrix} \e_1 \\ \e_2 \\ \e_3 \\ \e_4 \\ \e_5  \end{pmatrix}$, $\E(\e)$ = 0, $Var(\e) = \sigma^2 I$.

<<echo=FALSE,results='asis'>>=
x <- matrix(as.integer(c(1,1,1,1,1,0,0,1,1,1,0,0,0,0,1)), nrow=5, ncol=3, byrow = FALSE, dimnames = NULL)
y <- matrix(as.integer(c(1,2,3,4,5)), nrow=5, ncol=1, byrow = FALSE, dimnames = NULL)
@

На уровне значимости $5\%$ проверьте гипотезу  $H_0: \beta_1 + \beta_2 = 2$ против альтернативной $H_a: \beta_1 + \beta_2 \not= 2$:
\begin{enumerate}
\item Приведите формулу для тестовой статистики 
\item Укажите распределение тестовой статистики  при верной $H_0$
\item Вычислите наблюдаемое значение тестовой статистики
\item Укажите границы области, где основная гипотеза не отвергается
\item Сделайте статистический вывод
\end{enumerate}
\end{problem}
\begin{solution}
\end{solution}



\begin{problem}
По 13 наблюдениям Вася оценил модель со свободным членом, пятью количественными регрессорами и двумя качественными. Качественные регрессоры Вася правильно закодировал с помощью дамми-переменных. Одна качественная переменная принимала четыре значения, другая --- пять.

\begin{enumerate}
\item Найдите $SSR$, $R^2$
\item Как выглядит матрица $X(X'X)^{-1}X'$?
\item Почему 13 --- несчастливое число?
\end{enumerate} 
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
В рамках классической линейной модели найдите все математические ожидания и все ковариационные матрицы всех пар случайных векторов: $\ve$, $y$, $\hy$, $\he$, $\hb$. Т.е. найдите $\E(\e)$, $\E(y)$, \ldots и $\Cov(\e,y)$, $\Cov(\e,\hy)$, \ldots
\end{problem}

\begin{solution}
$\Var(\hb)=\sigma^2 (X'X)^{-1}$
\end{solution}


\begin{problem}
Найдите $\E(\sum (\e_i-\bar{\e})^2 )$, $\E(RSS)$
\end{problem}

\begin{solution}
$(n-1)\sigma^2$, $(n-k)\sigma^2$
\end{solution}


\begin{problem}
Используя матрицы $P=X(X'X)^{-1}X'$ и $\pi=\v1(\v1'\v1)^{-1}\v1'$ запишите $RSS$, $TSS$ и $ESS$ в матричной форме
\end{problem}
\begin{solution}
$TSS=y'(I-\pi)y$, $RSS=y'(I-P)y$, $ESS=y'(P-\pi)y$ 
\end{solution}



\begin{problem}
Найдите $\E(TSS)$, $\E(ESS)$. Надо быть морально готовым к тому, что они выйдут громоздкие 
\end{problem}
\begin{solution}
$\E(TSS)=(n-1)\sigma^2+\beta'X'(I-\pi)X\beta$
\end{solution}


\begin{problem}
Вася строит регрессию $y$ на некий набор объясняющих переменных и константу. А на самом деле $y_i=\beta_1+\e_i$. Чему равно $\E(TSS)$, $\E(RSS)$, $\E(ESS)$ в этом случае?
\end{problem}

\begin{solution}
$(n-1)\sigma^2$, $(n-k)\sigma^2$, $(k-1)\sigma^2$
\end{solution}



\begin{problem}
Рассмотрим классическую линейную модель. Являются ли векторы $\he$ и $\hy$ перпендикулярными? Найдите $\Cov(\he,\hy)$
\end{problem}
\begin{solution}
\end{solution}

\begin{problem}
Чему в классической модели регрессии равны $\E(\e)$, $\E(\he)$? Верно ли что $\sum \e_i$ равна 0? Верно ли что $\sum \he_i$ равна 0?
\end{problem}
\begin{solution}
$\E(\e)=0$, $\E(\he)=0$, $\sum \e_i$ может оказаться равной нулю только случайно, в нормальной модели это происходит с вероятностью 0, $\sum \he_i=0$ в модели со свободным членом
\end{solution}


\begin{problem}
Найдите на Картинке все перпендикулярные векторы. Найдите на Картинке все прямоугольные треугольники. Сформулируйте для них теоремы Пифагора.
\end{problem}

\begin{solution}
$\sum y_i^2=\sum \hy_i^2+\sum \he_i^2$, $TSS=ESS+RSS$, 
\end{solution}


\begin{problem}
Покажите на Картинке TSS, ESS, RSS, $R^2$, $\sCov(\hy,y)$
\end{problem}
\begin{solution}
\end{solution}

\begin{problem}
Предложите аналог $R^2$ для случая, когда константа среди регрессоров отсутствует. Аналог должен быть всегда в диапазоне $[0;1]$, совпадать с обычным $R^2$, когда среди регрессоров есть константа, равняться единице в случае нулевого $\he$.
\end{problem}

\begin{solution}
Спроецируем единичный столбец на <<плоскость>>, обозначим его $1'$. Делаем проекцию $y$ на <<плоскость>> и на $1'$. Далее аналогично. 
\end{solution}


\begin{problem}
Вася оценил регрессию $y$ на константу, $x$ и $z$. А затем, делать ему нечего, регрессию $y$ на константу и полученный $\hy$. Какие оценки коэффициентов у него получатся? Чему будет равна оценка дисперсии коэффицента при $\hy$? Почему оценка коэффициента неслучайна, а оценка её дисперсии положительна?
\end{problem}

\begin{solution}
Проекция $y$ на $\hy$ это $\hy$, поэтому оценки коэффициентов будут 0 и 1. Оценка дисперсии $\frac{RSS}{(n-2)ESS}$. Нарушены предпосылки теоремы Гаусса-Маркова, например, ошибки новой модели в сумме дают 0, значит коррелированы. 
\end{solution}



\begin{problem}
При каких условиях $TSS=ESS+RSS$?
\end{problem}
\begin{solution}
Либо в регрессию включена константа, либо единичный столбец (тут была опечатка, столбей) можно получить как линейную комбинацию регрессоров, например, включены дамми-переменные для каждого возможного значения качественной переменной.
\end{solution}




\begin{problem}
Истинная модель имеет вид $y=X\beta +\e$. Вася оценивает модель $\hy=X \hb$ по первой части выборки, получает $\hb_a$, по второй части выборки --- получает $\hb_b$ и по всей выборке --- $\hb_{tot}$. Как связаны между собой $\hb_a$, $\hb_b$, $\hb_{tot}$? Как связаны между собой ковариационные матрицы $\Var(\hb_a)$,  $\Var(\hb_b)$ и  $\Var(\hb_{tot})$?
\end{problem}

\begin{solution}
Сами оценки коэффициентов никак детерминистически не связаны, но при большом размере подвыборок примерно равны. А ковариационные матрицы связаны соотношением $\Var(\hb_a)^{-1}+\Var(\hb_b)^{-1}=\Var(\hb_{tot})^{-1}$ 
\end{solution}




\begin{problem}
Модель линейной регрессии имеет вид $y_i=\b_1 x_{i,1}+\b_2 x_{i,2} + u_i$.
Сумма квадратов остатков имеет вид $Q\left(\hb_1,\hb_2\right)=\sum_{i=1}^n (y_1-\hb_1 x_{i,1}-\hb_2 x_{i,2})^2$.
\begin{enumerate}
\item Выпишите необходимые условия минимума суммы квадратов остатков
\item Найдите матрицу $X'X$ и вектор $X'y$ если матрица $X$ имеет вид
$X=
\left(
\begin{array}{cc}
x_{1,1} & x_{1,2} \\
\vdots & \vdots \\
x_{n,1} & x_{n,2}
\end{array}
\right)
$,
а вектор $y$ имеет вид
$y=
\left(
\begin{array}{c}
y_1 \\
\vdots \\
y_n
\end{array}
\right)
$
\item Докажите, что необходимые условия равносильны матричному уравнению $X'X\hb=X'y$, где
$\hb=
\left(
\begin{array}{c}
\hb_1 \\
\hb_2
\end{array}
\right)
$
\item Предполагая, что матрица $X'X$ обратима, найдите $\hb$
\end{enumerate}
\end{problem}
\begin{solution}
\end{solution}



\begin{problem}
Вася оценил исходную модель:
\[
y_i=\b_1+\b_2 x_i + u_i
\]

Для надежности Вася стандартизировал переменные, т.е. перешёл к $y_i^*=(y_i-\bar{y})/s_y$ и $x_i^*=(x_i-\bar{x})/s_x$. Затем Вася оценил ещё две модели:

\[
y_i^*=\b_1'+\b_2' x^*_i + u_i'
\]

и 
\[
y_i^*=\b_2'' x^*_i + u_i''
\]

В решении можно считать $s_x$ и $s_y$ известными.
 
\begin{enumerate}
\item Найдите $\hb_1'$
\item Как связаны между собой $\hb_2$, $\hb_2'$ и $\hb_2''$? 
\item Как связаны между собой $\hat{u}_i$, $\hat{u}_i'$ и $\hat{u}_i''$?
\item Как связаны между собой $\hVar\left(\hb_2\right)$, $\hVar\left(\hb_2'\right)$ и $\hVar\left(\hb_2''\right)$?
\item Как выглядит матрица $\hVar\left(\hb'\right)$?
\item Как связаны между собой $t$-статистики $t_{\hb_2}$, $t_{\hb_2'}$ и $t_{\hb_2''}$?
\item Как связаны между собой $R^2$, $R^{2\prime}$ и $R^{2\prime\prime}$?
\item В нескольких предложениях прокомментируйте последствия перехода к стандартизированным переменным
\end{enumerate}
\end{problem}
\begin{solution}
\end{solution}



\begin{problem}
Регрессионная модель  задана в матричном виде при помощи уравнения $y=X\beta+\varepsilon$, где $\beta=(\beta_1,\beta_2,\beta_3)'$.
Известно, что $\E(\varepsilon)=0$  и  $\Var(\varepsilon)=\sigma^2\cdot I$.
Известно также, что 

$y=\left(
\begin{array}{c} 
1\\ 
2\\ 
3\\ 
4\\ 
5
\end{array}\right)$, 
$X=\left(\begin{array}{ccc}
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 0 & 0 \\
1 & 1 & 0 \\
1 & 1 & 1 
\end{array}\right)$.


Для удобства расчетов приведены матрицы 


$X'X=\left(
\begin{array}{ccc} 
5 & 2 & 1\\ 
2 & 2 & 1\\ 
1 & 1 & 1 
\end{array}\right)$ и $(X'X)^{-1}=\frac{1}{3}\left(
\begin{array}{ccc} 
1 & -1 & 0 \\
-1 & 4 & -3 \\
0 & -3 & 6
\end{array}\right)$.

\begin{enumerate}
\item Укажите число наблюдений.
\item Укажите число регрессоров с учетом свободного члена.
\item Запишите модель в скалярном виде
\item Рассчитайте $TSS=\sum (y_i-\bar{y})^2$, $RSS=\sum (y_i-\hat{y}_i)^2$ и $ESS=\sum (\hat{y}_i-\bar{y})^2$.
\item Рассчитайте при помощи метода наименьших квадратов $\hb$, оценку для вектора неизвестных коэффициентов.
\item Чему равен $\he_5$, МНК-остаток регрессии, соответствующий 5-ому наблюдению?
\item Чему равен $R^2$  в модели? Прокомментируйте полученное значение с точки зрения качества оцененного уравнения регрессии.
\item Используя приведенные выше данные, рассчитайте несмещенную оценку для неизвестного параметра $\sigma^2$ регрессионной модели.
\item Рассчитайте $\widehat{\Var}(\hb)$, оценку для ковариационной матрицы вектора МНК-коэффициентов $\hb$.  
\item Найдите $\widehat{\Var}(\hb_1)$, несмещенную оценку дисперсии МНК-коэффициента $\hb_1$.
\item Найдите $\widehat{\Var}(\hb_2)$, несмещенную оценку дисперсии МНК-коэффициента $\hb_2$.
\item Найдите $\widehat{\Cov}(\hb_1,\hb_2)$, несмещенную оценку ковариации МНК-коэффициентов $\hb_1$ и $\hb_2$.
\item Найдите $\widehat{\Var}(\hb_1+\hb_2)$, $\widehat{\Var}(\hb_1-\hb_2)$, $\widehat{\Var}(\hb_1+\hb_2+\hb_3)$, $\widehat{\Var}(\hb_1+\hb_2-2\hb_3)$
\item Найдите $\hCorr(\hb_1,\hb_2)$, оценку коэффициента корреляции МНК-коэффициентов $\hb_1$ и $\hb_2$.
\item Найдите $s_{\hb_1}$, стандартную ошибку МНК-коэффициента $\hb_1$.
\item Рассчитайте выборочную ковариацию $y$ и $\hy$.
\item Найдите выборочную дисперсию $y$, выборочную дисперсию $\hy$.
\end{enumerate}
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
Теорема Фриша-Вау. Регрессоры разбиты на две группы: матрицу $X_1$ размера $n\times k_1$ и матрицу $X_2$ размера $n\times k_2$. Рассмотрим две процедуры:
\begin{enumerate}
\item[M1.] Строим регрессия вектора $y$ на все регрессоры, т.е. оцениваем модель:
\[ 
y=X\beta + \e = X_1 \beta_1 + X_2 \beta_2 +\e 
\]
\item[M2.] Процедура из двух шагов:
\begin{enumerate}
\item Строим регрессию вектора $y$ на все регрессоры первой группы и получаем вектор остатков $M_1 y$, где $M_1=I-X_1(X_1'X_1)^{-1}X_1'$. Строим регрессию каждого регрессора из второй группы на все регрессоры первой группы и получаем в каждом случае вектор остатков. Эти остатки можно записать матрицей $M_1 X_2$. 
\item Строим регрессию вектора $M_1 y$ на остатки $M_1 X_2$.
\end{enumerate}
Другими словами мы оцениваем модель:
\[
M_1 y = M_1 X_2 \gamma_2 + u
\]

\end{enumerate}
\begin{enumerate}
\item Верно ли, что МНК оценки коэффициентов $\hb_2$ и $\hat{\gamma}_2$ совпадают?
\item Верно ли, что остатки в обеих регрессиях совпадают?
\end{enumerate}
\end{problem}
\begin{solution}
да, да
\end{solution}



\begin{problem}
Всего имеется $100$ наблюдений. Для первых 50-ти наблюдений $X'X=\begin{pmatrix}
50 & 300 \\
300 & 2100
\end{pmatrix}$, $X'y=\begin{pmatrix}
300 & 2000
\end{pmatrix}'$, $y'y=2100$. По последним 50-ти наблюдениям: $X'X=\begin{pmatrix}
50 & 300 \\
300 & 2100
\end{pmatrix}$, $X'y=\begin{pmatrix}
300 & 2200
\end{pmatrix}'$, $y'y=2500$. По первым 50-ти наблюдениям оценивается модель $y_i = \beta_1 + \beta_2 x_i + \e_i$, по последним 50-ти наблюдениям оценивается модель $y_i = \gamma_1 + \gamma_2 x_i + \e_i$. Предположеним, что во всех 100 наблюдениях $\e_i$ независимы и нормальны $N(0;\sigma^2)$. На уровне значимости 5\% проверьте гипотезу $H_0: \, \beta=\gamma$.
\end{problem}
\begin{solution}
\end{solution}


\begin{problem}
Докажите, что МНК-оценки $\hb = (X^T X)^{-1} X^T y$ являются несмещенными и линейными по переменной $y$.
\end{problem}

\begin{solution}
Докажем несмещенность МНК-оценок.
$$\E\hb = \E\left( (X^T X)^{-1} X^T y \right) = (X^T X)^{-1} X^T \E(y) = $$
$$= (X^T X)^{-1} X^T \E(X\beta + \e) = (X^T X)^{-1} X^T X\beta = \beta$$
Обозначим $\varphi(X, y) = (X^T X)^{-1} X^T y$. Тогда $\hb = \varphi(X, y)$. Покажем, что функция $\varphi$ линейна по переменной $y$.
\begin{enumerate}
\item $\varphi(X, \lambda \cdot y) = (X^T X)^{-1} X^T (\lambda \cdot y) = \lambda (X^T X)^{-1} X^T y = \lambda \cdot \varphi(X, y)$
\item $\varphi(X, y + z) = (X^T X)^{-1} X^T (y + z) = (X^T X)^{-1} X^T y + (X^T X)^{-1} X^T z = \varphi(X, y) + \varphi(X, z)$
\end{enumerate}
Что и требовалось доказать.
\end{solution}

\begin{problem}
Являются ли МНК-оценки линейными по переменной $X$?
\end{problem}

\begin{solution}
Нет, так как для функции $\varphi(X, y) = (X^T X)^{-1} X^T y$ не выполнено, например, свойство однородности по переменной $X$. Действительно,
$$\varphi(X, \lambda \cdot y) = ((\lambda \cdot X)^T (\lambda \cdot X))^{-1} (\lambda \cdot X)^T y = \frac{1}{\lambda} \cdot (X^T X)^{-1} X^T y = \frac{1}{\lambda} \varphi(X, y)$$.
\end{solution}

\begin{problem}
Приведите пример несмещенной и линейной по переменной $y$ оценки, отличной от МНК.
\end{problem}

\begin{solution} 
$\tilde{\beta} = (X^T CX)^{-1} X^T Cy$, где
$$C = \begin{bmatrix}
1 & 0 & 0 & \cdots & 0 \\
0 & 2 & 0 & \cdots & 0 \\
0 & 0 & 3 & \ddots & \vdots \\
\vdots & \vdots & \ddots & \ddots & 0 \\
0 & 0 & \cdots & 0 & n \\
\end{bmatrix} $$
\end{solution}


\begin{problem}
Если для регрессии $y = X\beta + \e$ не выполняется условие $Pi = i$, где $i$ --- единичный столбец, а $P \equiv X(X^TX)^{-1}X^T$, $\pi \equiv \frac{ii^T}{i^Ti}$, то будут неверны равенства:
\begin{enumerate}
\begin{minipage}[h]{0.49\linewidth}
\item[(1)] $P\pi = \pi$
\item[(2)] $P^2 = P$
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\item[(3)] $\sum_{i=1}^n \hat{\e}_i = 0$
\item[(4)] $\overline{Y} = \overline{\hat{Y}}$
\end{minipage}
\end{enumerate}
\end{problem}

\begin{solution} 
$Pi = i \Leftrightarrow P\pi = \pi$ поскольку, если матрицу $\pi$ записать по столбцам $\pi = \frac{1}{n}\begin{bmatrix}
i & i & \ldots & i
\end{bmatrix}$, то можно записать следующую цепочку равенств $P\pi = P\frac{1}{n}\begin{bmatrix}
i & i & \ldots & i
\end{bmatrix} = \frac{1}{n}\begin{bmatrix}
Pi & Pi & \ldots & Pi
\end{bmatrix} = \frac{1}{n}\begin{bmatrix}
i & i & \ldots & i
\end{bmatrix} \Leftrightarrow Pi = i$.

Свойство $P^2 = P$ имеет место независимо от выполнимости условия $Pi = i$. Действительно, $P^2 = X (X^T X)^{-1}X^TX(X^TX)^{-1}X^T = X(X^TX)^{-1}X^T = P$.

Рассмотрите пример $y = \begin{bmatrix}
1 & -1 & 0
\end{bmatrix}^T$, $x = \begin{bmatrix}
1 & 0 & -1
\end{bmatrix}^T$. Постройте регрессию $y = \beta x + \e$ без свободного члена. Убедитесь, что $\sum_{i=1}^n \hat{\e}_i = 0$ и $\overline{Y} = \overline{\hat{Y}} = 0$, но $Pi \neq i$.


Ответ: $P\pi = \pi$
\end{solution}



\begin{problem}
Необходимыми условиями теоремы Гаусса-Маркова являются
\begin{enumerate}
\item Правильная специфицикация модели: $y = X\beta + \e$,
\item Полный ранг матрицы $X$,
\item Невырожденность матрицы $X^T X$
\item Нормальность распределения случайной составляющей
\item Скалярность (пропорциональность единичной матрице) ковариационной матрицы случайной составляющей,
\item Наличие в матрице $X$ единичного столбца
\end{enumerate}
\end{problem}

\begin{solution}
(1), (2) $\Leftrightarrow$ (3), (5)
\end{solution}


\begin{problem}
Для регрессии $y = X\beta + \e$ с $\E(\e) = 0$, $\Var(\e) = \begin{bmatrix}
\sigma_1^2 &  & 0 \\
  & \ddots &   \\
0 &  & \sigma^2_n
\end{bmatrix}$ найдите математическое ожидание квадратичной формы $\e^T \pi \e$.
\end{problem}


\begin{solution}
\begin{multline}
\E(\e^T \pi \e) = \E(\tr[\e^T \pi \e]) = \E(\tr[\pi \e \e^T]) = \tr[\pi \E(\e\e^T)]  =\\
\tr[\pi \Var(\e)] = \tr\left[ \frac{1}{n} \begin{bmatrix}
1 & 1 & 1 & 1 \\
1 & 1 & 1 & 1 \\
\ldots & \ldots & \ldots & \ldots \\
1 & 1 & 1 & 1 \\
\end{bmatrix} \begin{bmatrix}
\sigma_1^2 & 0 & \ldots & 0 \\
0 & \sigma_2^2 & \ldots & 0 \\
\ldots & \ldots & \ldots & \ldots \\
0 & \ldots & 0 & \sigma_n^2 \\
\end{bmatrix} \right] = \\
\frac{1}{n} tr\begin{bmatrix}
\sigma_1^2 & \sigma_2^2 & \ldots & \sigma_n^2 \\
\sigma_1^2 & \sigma_2^2 & \ldots & \sigma_n^2 \\
\ldots & \ldots & \ldots & \ldots \\
\sigma_1^2 & \sigma_2^2 & \ldots & \sigma_n^2 \\
\end{bmatrix} = \frac{1}{n} \sum_{i=1}^n \sigma_i^2
\end{multline}
\end{solution}

\begin{problem}
Рассмотрим регрессию, для которой выполнены условия теоремы Гаусса-Маркова. Уравнение регрессии имеет вид $\hat{y} = \hb_1 i + \hb_2 x_2 + \hb_3 x_3 + \hb_4 x_4$. Известны следующие данные:

\[
X^T X = \begin{bmatrix}
100 & 123 & 96 & 109 \\
 & 252 & 125 & 189 \\
 & & 167 & 146 \\
 & & & 168 \\
\end{bmatrix}
\]
\[
(X^TX)^{-1} = \begin{bmatrix}
0.03767 & & & \\
-0.06263 & 1.129 & & \\
-0.06247 & 1.107 & 1.110 & \\
0.1003 & -2.192 & -2.170 & 4.292 \\
\end{bmatrix}
\] 

$X^Ty = \begin{bmatrix}
460\\
810\\
615\\
712\\
\end{bmatrix}$; $y^Ty = 3924$

\begin{enumerate}
\item Найти $\widehat{\Corr}(\hb_1, \hb_2)$ (1 балл)
\item Найти $\widehat{\Corr}(x_2, x_3)$ (1 балл)
\item Проверить гипотезу $H_0: \beta_2 = 0$ (1 балл)
\end{enumerate}
\end{problem}

\begin{solution}
\begin{enumerate}
\item 
\begin{multline}
RSS = \hat{\e}^T\hat{\e} y^T (I - P) y = y^T y - y^TPy = y^Ty - y^T X(X^TX)^{-1}X^Ty ;
\end{multline}
При этом $y^Ty=3924$,  а
\begin{multline}
y^T X(X^TX)^{-1}X^Ty= \\
 \begin{bmatrix}
460 & 810 & 615 & 712
\end{bmatrix} \begin{bmatrix}
0.038 & -0.063 & -0.063 & 0.100 \\
-0.063 & 1.129 & 1.107 & -2.192 \\
-0.063 & 1.107 & 1.110 & -2.170 \\
0.100 & -2.192 & -2.170 & 4.292 \\
\end{bmatrix} \cdot \\
\cdot 
\begin{bmatrix}
460\\
810\\
615\\
712\\
\end{bmatrix} = 3051.2
\end{multline}
Итого, $RSS= 3924 - 3051.2 = 872.8$
 
$\hat{\sigma}_{\e}^2 = \frac{RSS}{n-k} = \frac{872.8}{100-4} = 9.0917$

$\widehat{\Var}(\hb) = \hat{\sigma}_{\e}^2 (X^T X)^{-1} \Rightarrow \widehat{\Cov}(\hb_1, \hb_2) = -0.56939$, $\widehat{\Var}(\hb_1) = 0.34251$, $\widehat{\Var}(\hb_2) = 10.269$

$\widehat{\Corr}(\hb_1, \hb_2) = \frac{\widehat{\Cov}(\hb_1, \hb_2)}{\sqrt{\widehat{\Var}(\hb_1)}\sqrt{\widehat{\Var}(\hb_2)}} = -0.30361$

\item(указание) $\widehat{\Corr}(x_2, x_3) = \frac{\sum (x_{i2} - \overline{x}_2) (x_{i3} - \overline{x}_3)}{\sqrt{\sum (x_{i2} - \overline{x}_2)}\sqrt{\sum (x_{i3} - \overline{x}_3)}}$. Все необходимые величины можно извлечь из матрицы $X^T X$ --- это величины $\sum x_{i2}$ и $\sum x_{i3}$, а остальное -- из матрицы $X^T (I - \pi) X = X^T X - X^T \pi X = X^T X - (\pi X)^T \pi X$. При этом имейте в виду, что
$\pi X = \begin{bmatrix}
1 & \overline{x}_1 & \overline{x}_2 & \overline{x}_3 \\
\ldots & \ldots & \ldots & \ldots \\
1 & \overline{x}_1 & \overline{x}_2 & \overline{x}_3 \\
\end{bmatrix}$ и $\overline{x}_1 = 1.23$, $\overline{x}_2 = 0.96$, $\overline{x}_3 = 1.09$

\item $\begin{bmatrix}
\hb_1 \\
\hb_2 \\
\hb_3 \\
\hb_4 \\
\hb_5 \\
\end{bmatrix} = \begin{bmatrix}
0.03767 & -0.06263 & -0.06247 & 0.1003 \\
-0.06263 & 1.129 & 1.107 & -2.192 \\
-0.06247 & 1.107 & 1.110 & -2.170 \\
0.1003 & -2.192 & -2.170 & 4.292 \\
\end{bmatrix} \begin{bmatrix}
460\\
810\\
615\\
712\\
\end{bmatrix} = \begin{bmatrix}
-0.40221 \\
6.1234 \\
5.9097 \\
-7.5256 \\
\end{bmatrix}$

$t = \frac{\hb_2}{\sqrt{\Var(\hb_2)}} \sim t_{100-4}$

$t = \frac{\hb_2}{\sqrt{\Var(\hb_2)}} = \frac{6.1234}{\sqrt{10.269}} = 1.9109 \Rightarrow \hb_2$ --- не значим.
\end{enumerate}
\end{solution}


\begin{problem}
По данным для 15 фирм ($n = 15$) была оценена производственная функция Кобба-Дугласа: $\ln Q_i = \beta_1 + \beta_2 \ln L_i + \beta_3 \ln K_i + \e_i$. Полученные оценки: $$\underset{s.e.}{\widehat{\ln Q}} = \underset{(4.48)}{0.5} + \underset{(0.7)}{0.76} \ln L + \underset{(0.138)}{0.19} \ln K$$где $Q$ --- выпуск, $L$ --- трудозатраты, $K$ --- капиталовложения. Матрица обратная к матрице регрессоров имеет вид:

$(X^T X)^{-1} = \begin{bmatrix}
121573 & -19186 & 3718 \\
-19186 & 3030 & -589 \\
3718 & -589 & 116 \\
\end{bmatrix}$

Требуется:
\begin{enumerate}
\item Написать формулу для несмещенной оценки ковариации $\widehat{\Cov}(\hb_2, \hb_3)$ и вычислить её по имеющимся данным (если это возможно);
\item Проверить $H_0: \beta_2 + \beta_3 = 1$ при помощи t-статистики (обязательно требуется указать формулу для статистики, а также указать число степеней свободы);
\item Построить 95\% доверительный интервал для величины $\beta_2 + \beta_3$.
\end{enumerate}
\end{problem}

\begin{solution}
\begin{enumerate}
\item $\widehat{\Cov}\begin{bmatrix}
\hb_1 \\
\hb_2 \\
\hb_3 \\
\end{bmatrix} = \hat{\sigma}_{\e}^2 (X^TX)^{-1}$ --- несмещённая оценка для ковариационной матрицы \\ МНК-коэффициентов. Действительно, $\E\widehat{\Cov}\begin{bmatrix}
\hb_1 \\
\hb_2 \\
\hb_3 \\
\end{bmatrix} = \E\hat{\sigma}_{\e}^2 (X^TX)^{-1} = \sigma_{\e}^2 (X^TX)^{-1} = \Cov\begin{bmatrix}
\hb_1 \\
\hb_2 \\
\hb_3 \\
\end{bmatrix}$. Поэтому искомая оценка $\widehat{\Cov}(\hb_2, \hb_3) = \hat{\sigma}_{\e}^2 \left[ (X^T X)^{-1} \right]_{23}$, где $\left[ (X^T X)^{-1} \right]_{23}$ --- элемент матрицы $(X^T X)^{-1}$, расположенный во второй строке, 3-м столбце.

Заметим, что $\hat{\sigma}_{\hb_2}^2 = \hat{\sigma}_{\e}^2 \left[ (X^T X)^{-1} \right]_{22} \Rightarrow 0.7^2 = \hat{\sigma}_{\e}^2 \cdot (3030) \Rightarrow \hat{\sigma}_{\e}^2 = 0.00016172$

Значит, $\widehat{\Cov}(\hb_2, \hb_3) = 0.00016172 \cdot (-589) = -0.095253$.

\item $t = \frac{\hb_2 + \hb_3 - \beta_2 - \beta_3}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} \sim t_{n-k}$

Требуется проверить $H_0: \beta_2 + \beta_3 = 1$.

$\widehat{\Var}(\hb_2 + \hb_3) = \widehat{\Var}(\hb_2) + \widehat{\Var}(\hb_3) + 2\widehat{\Cov}(\hb_2, \hb_3) = 0.7^2 + 0.138^2 + 2 \cdot 0.095253 = 0.319044$

$t = \frac{\hb_2 + \hb_3 - \beta_2 - \beta_3}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} = \frac{0.76 + 0.19 - 1}{\sqrt{0.319044}} = -0.088520674$

Значит, гипотеза не отвергается на любом <<разумном>> уровне значимости.

\item Мы знаем, что $\frac{\hb_2 + \hb_3 - \beta_2 - \beta_3}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} \sim t_{n-k} = t_{15-3}$, поэтому построить доверительный интервал для $\beta_2 + \beta_3$ не составляет труда. $\mathbb{P}\left( \left| \frac{\hb_2 + \hb_3 - \beta_2 - \beta_3}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} \right| < t^* \right) = 0.95$

Обозначим $se=\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}$, тогда:

\begin{multline}
\mathbb{P}\left( \left| \frac{\hb_2 + \hb_3 - \beta_2 - \beta_3}{\sqrt{\widehat{\Var}(\hb_2 + \hb_3)}} \right| < t^* \right) = \\
\mathbb{P}\left( -t^* se < \hb_2 + \hb_3 - \beta_2 - \beta_3 < t^* se \right) = \\ 
\mathbb{P}\left( -t^*se  - (\hb_2 + \hb_3) < - \beta_2 - \beta_3  < -(\hb_2 + \hb_3) + t^* se \right) =\\ 
\mathbb{P}\left( (\hb_2 + \hb_3) + t^* se 
> \beta_2 + \beta_3 
> (\hb_2 + \hb_3) - t^* se \right) 
\end{multline}
Отсюда получаем доверительный интервал
\begin{multline}
\beta_2 + \beta_3 \in \\
[(0.76 + 0.19) - 2.16 \cdot 0.319;  (0.76 + 0.19) + 2.16 \cdot 0.319 ]
\end{multline}
Или $0.26< \beta_2 + \beta_3  < 1.639  $
\end{enumerate}
\end{solution}


\begin{problem}
Напишите формулу для оценок коэффициентов в множественной регрессии с матрицами. Напишите формулу для ковариационной матрицы оценок.
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
<<>>=
model <- lm(dist~speed,data=cars)
model.sum <- summary(model)
hat.sigma <- model.sum$sigma
@

Исследователь оценил зависимость длины тормозного пути в футах от скорости автомобиля в милях в час по данным 1920-х годов.
При построении парной регрессии у него получилась $\hat{\sigma}=\Sexpr{hat.sigma}$ и оценка ковариационной матрицы 


<<results="asis">>=
xtable(vcov(model))
@


\begin{enumerate}
\item Определите количество наблюдений
\item Найдите среднюю скорость автомобиля в милях в час
\end{enumerate}
\end{problem}

\begin{solution}
Находим $X'X$, её элементы и есть то, что нужно.
\end{solution}



\begin{problem}
Дана оценка ковариацонной матрицы, 
\[
\widehat{Var}(\hat{\beta})=\begin{pmatrix}
1/3 & -1/3 & 0 \\
-1/3 & 4/3 & -1 \\
0 & -1 & 2
\end{pmatrix}
\]

Найдите $\widehat{Var}(\hb_1+\hb_2-\hb_3)$
\end{problem}

\begin{solution}
\end{solution}

\begin{problem}
Методом наименьших квадратов по 5 наблюдениям оценивается модель $y_i=\beta_1+\beta_2 x_{i2}+\beta_3 x_{i3}+\varepsilon_i$. Известно, что
\[
\hb=\begin{pmatrix}
1 \\
2 \\
3
\end{pmatrix}, \;
(X'X)^{-1}= \begin{pmatrix} 
1/3 & -1/3 & 0 \\ 
-1/3 & 4/3 & -1 \\ 
0 & -1 & 2.0000 
\end{pmatrix}
\]
\[
\widehat{Var}(\hb)=\begin{pmatrix}
1 & -1 & 0 \\
-1 & 4 & -3 \\ 
0 & -3 & 6
\end{pmatrix}, \; TSS=60
\]

Найдите $\hat{\sigma}^2$ и проверьте гипотезу о значимости регрессии в целом на уровне значимости 5\%.
\end{problem}

\begin{solution}
\end{solution}