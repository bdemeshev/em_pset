\Opensolutionfile{solution_file}[solutions/sols_010]
% в квадратных скобках фактическое имя файла

\chapter{МНК без матриц и вероятностей}

\begin{problem} %1.1
Пусть $a = (a_1,\dots,a_n)$ и $b = (b_1,\dots,b_n)$ — два произвольных вектора. Определите, какие равенства справедливы:
\begin{enumerate}
\item $\sum_{i=1}^n {(a_i-\bar a)} = 0$;
\item $\sum_{i=1}^n {(a_i-\bar a)^2} = \sum_{i=1}^n {(a_i-\bar a)a_i}$;
\item $\sum_{i=1}^n {(a_i-\bar a)(b_i-\bar b)} = \sum_{i=1}^n {(a_i-\bar a)b_i}$;
\item $\sum_{i=1}^n {(a_i-\bar a)(b_i-\bar b)} = \sum_{i=1}^n {a_i b_i}$;
\item $\sum_{i=1}^n a_i = n\bar a$;
\item $\sum_{i=1}^n (a_i - \bar a)^2 = \sum_{i=1}^n a_i^2 - n \bar a^2$;
\item $\sum_{i=1}^n a_i^2 = \left( \sum_{i=1}^n a_i \right)^2$;
\item $\sum_{i=1}^n a_i^2 = (n\bar a)^2$;
\item $\sum_{i=1}^n \bar a = n \bar a$;
\item $\sum_{i=1}^n a_i \bar a = n \bar a^2$;
\item $\sum_{i=1}^n {(a_i-\bar a)b_i} = 0$.
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item Верно: \[\sum_{i=1}^{n}(a_i-\bar{a}) = \sum_{i=1}^{n}a_i - n\cdot\bar{a} = \sum_{i=1}^{n}a_i - \sum_{i=1}^{n}a_i = 0\]
\item Верно: \[\sum_{i=1}^{n}(a_i-\bar{a})^2 = \sum_{i=1}^{n}(a_i-\bar{a})(a_i+\bar{a}) = \sum_{i=1}^{n}(a_i-\bar{a})a_i + \bar{a}\underbrace{\sum_{i=1}^{n}(a_i-\bar{a})}_{=0} = \sum_{i=1}^{n}(a_i-\bar{a})a_i \]

\item Верно: \[\sum_{i=1}^{n}(a_i-\bar{a})(b_i-\bar{b}) = \sum_{i=1}^{n}(a_i-\bar{a})b_i - \bar{b}\underbrace{\sum_{i=1}^{n}(a_i-\bar{a})}_{=0} = \sum_{i=1}^{n}(a_i-\bar{a})b_i \]
\item А вот это неверно! (следует из предыдущего пункта)
\item Верно
\item Верно
\item Неверно
\item Неверно
\item Верно
\item Верно: \[\sum_{i=1}^{n}a_i\bar{a} = \frac{n}{n}\bar{a}\sum_{i=1}^{n}a_i = n \bar{a} \frac{\sum_{i=1}^{n}a_i}{n} = n\bar{a}^2 \]
\item Неверно (см. пунк 3)
\end{enumerate}
\end{sol}
\end{problem}





\begin{problem} %1.2
При помощи метода наименьших квадратов найдите оценку неизвестного параметра $\theta$ в следующих моделях:

\begin{enumerate}
\item $y_i = \theta + \theta x_i + \varepsilon_i$;
\item $y_i = \theta - \theta x_i + \e_i$;
\item $\ln y_i = \theta + \ln x_i + \e_i$;
\item $y_i = \theta + x_i + \e_i$;
\item $y_i = 1 + \theta x_i + \e_i$;
\item $y_i = \theta / x_i + \e_i$;
\item $y_i = \theta x_i + (1-\theta)z_i+\e_i$.
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item \(\htheta = \sum y_i (1 + x_i) / \sum (1 + x_i)^2\)

Стандартная процедура МНК:
\[RSS = \sum \e_i^2 = \sum \left(y_i - \theta - \theta x_i\right)^2 \rightarrow \min \limits_\theta\]
\[\frac{\partial RSS}{\partial \theta} = 2 \sum \left(y_i - \theta - \theta x_i\right)(-1 - x_i) \]
\[\sum \left(y_i - \htheta - \htheta x_i\right)(-1 - x_i) = 0\]
\[\sum y_i (-1 - x_i) + \htheta \sum (-1 - x_i)^2 = 0 \]
\[\htheta = \frac{\sum y_i (1 + x_i)}{\sum (1 + x_i)^2} \]

\item \(\htheta = \sum \left(y_i (1 - x_i)\right) / \sum (1 - x_i)^2\)

\item \(\htheta = \left( \sum \text{ln} (y_i / x_i) \right) / n \)

\item \(\htheta = \left( \sum (y_i - x_i) \right) / n \)

\item \(\htheta = \sum \left((y_i - 1) x_i\right) / \sum x_i^2\)

\item \(\htheta = \sum (y_i / x_i^2) / \sum (1 /x^3)\)

\item \(\htheta = \sum \left((y_i - z_i)(x_i - z_i) \right) / \sum \left(x_i - z_i\right)^2 \)

\end{enumerate}
\end{sol}
\end{problem}


\begin{problem} %1.3
Покажите, что для моделей $y_i= \alpha + \beta x_i + \e_i$, $z_i = \gamma + \delta x_i + \upsilon_i$ и $y_i + z_i = \mu + \lambda x_i + \xi_i$ МНК-оценки связаны соотношениями $\hat{\mu}=\hat{\alpha}+\hat{\gamma}$ и $\hat{\lambda}=\hb + \hat{\delta}$.


\begin{sol}
Заметим, что $y_i + z_i = \underbrace{(\alpha + \gamma)}_{\mu} + \underbrace{(\beta+\delta)}_{\lambda}x_i + u_i$.

Если оценить данную модель при помощи МНК, получим как раз то, что нужно доказать.
\end{sol}
\end{problem}


\begin{problem} %1.4
Найдите МНК-оценки параметров $\alpha$ и $\beta$ в модели $y_i = \alpha + \beta y_i + \e_i$.


\begin{sol}
\(\hat{\alpha} = 0, \ \hb = 1 \)
\end{sol}
\end{problem}


\begin{problem} %1.5
Рассмотрите модели $y_i = \alpha + \beta (y_i + z_i) + \e_i$, $z_i = \gamma + \delta(y_i+z_i) + \e_i$.
\begin{enumerate}
\item Как связаны между собой $\hat{\alpha}$ и $\hat{\gamma}$?
\item Как связаны между собой $\hb$ и $\hat{\delta}$?
\end{enumerate}


\begin{sol} % 1.5.
Рассмотрим регрессию суммы $(y_i + z_i)$ на саму себя. Естественно, в ней
\[
\widehat{y_i + z_i} = 0 + 1 \cdot (y_i + z_i).
\]

Отсюда получаем, что $\hat{\alpha} + \hat{\gamma} = 0$ и $\hb + \hat{\delta} = 1$.
\end{sol}
\end{problem}




\begin{problem} %1.6
Как связаны МНК-оценки параметров $\alpha, \beta$ и $\gamma, \delta$ в моделях $y_i = \alpha + \beta x_i + \e_i$ и $z_i = \gamma + \delta x_i + \upsilon_i$, если $z_i = 2 y_i$?


\begin{sol}

Исходя из условия, нужно оценить методом МНК коэффициенты двух следующих моделей:
\[y_i = \alpha + \beta x_i + \e_i \]
\[y_i = \frac{\gamma}{2} + \frac{\delta}{2} x_i + \frac{1}{2} v_i \]

Заметим, что на минимизацию суммы квадратов остатков коэффициент \(1/2\) не влияет, следовательно:
\[\hat{\gamma} = 2\hat{\alpha}, \ \hat{\delta} = 2 \hb  \]

\end{sol}
\end{problem}


\begin{problem} %1.6
Для модели $y_i = \beta_1 x_i + \beta_2 z_i + \e_i$ решите условную задачу о наименьших квадратах:
\[
Q(\beta_1, \beta_2) := \sum_{i=1}^n (y_i - \beta_1 x_i - \beta_2 z_i)^2 \rightarrow \underset{\beta_1 + \beta_2 = 1}{\min}.
\]


\begin{sol}
Выпишем задачу:
\[
\begin{cases}
RSS = \sum\limits_{i=1}^{n}(y_i - \beta_1x_i - \beta_2z_i)^2 \rightarrow \min\limits_{\beta_1, \beta_2}\\
\beta_1 + \beta_2 = 1
\end{cases}
\]

Можем превратить ее в задачу минимизации функции одного аргумента:
\[
RSS =  \sum\limits_{i=1}^{n}(y_i - x_i - \beta_2(z_i-x_i))^2 \rightarrow \min_{\beta_2}
\]

Выпишем условия первого порядка:
\[
\frac{\partial RSS}{\partial \beta_2} = \sum\limits_{i=1}^{n}2(y_i-x_i-\hb_2(z_i-x_i))(x_i-z_i)=0
\]

Отсюда:
\[
\sum\limits_{i=1}^{n}(y_i-x_i)(x_i-z_i) + \hb_2\sum\limits_{i=1}^{n}(z_i-x_i)^2 = 0 \Rightarrow \hb_2 = \frac{\sum\limits_{i=1}^n (y_i-x_i)(z_i-x_i)}{\sum\limits_{i=1}^n (z_i-x_i)^2}
\]

А оценка $\beta_1$ найдется из соотношения $\hb_1+\hb_2 = 1$.

\end{sol}
\end{problem}




\begin{problem} %1.8
Даны $n$ пар чисел: $(x_1, y_1)$, \ldots, $(x_n,y_n)$. Мы прогнозируем $y_i$ по формуле $\hy_i=\hb x_i$. Найдите $\hb$ методом наименьших квадратов.


\begin{sol}
$\hb=\sum x_i y_i/\sum x_i^2$
\end{sol}
\end{problem}


\begin{problem} %1.9
Даны $n$ чисел: $y_1$, \ldots, $y_n$. Мы прогнозируем $y_i$ по формуле $\hy_i=\hb$. Найдите $\hb$ методом наименьших квадратов.

\begin{sol}
Нужно решить задачу:
\[
RSS = \sum\limits_{i=1}^n(y_i-\beta)^2 \rightarrow \min\limits_{\beta}
\]

Условия первого порядка:
\[
\frac{\partial RSS}{\partial \beta} = \sum\limits_{i=1}^{n}2(y_i-\hb) = 0 \Rightarrow \sum\limits_{i=1}^n y_i-n\hb = 0
\]

Поэтому
\[
\hb = \frac{\sum_{i=1}^ny_i}{n} = \bar{y}
\]
\end{sol}
\end{problem}


\begin{problem} % 1.10
Даны $n$ пар чисел: $(x_1, y_1)$, \ldots, $(x_n,y_n)$. Мы прогнозируем $y_i$ по формуле $\hy_i=\hb_1+\hb_2 x_i$. Найдите $\hb_1$ и $\hb_2$ методом наименьших квадратов.

\begin{sol}
$\hb_2=\sum (x_i-\bar{x})(y_i-\bar{y})/\sum(x_i-\bar{x})^2$, $\hb_1=\bar{y}-\hb_2\bar{x}$
\end{sol}
\end{problem}


\begin{problem}%1.11
Даны $n$ пар чисел: $(x_1, y_1)$, \ldots, $(x_n,y_n)$. Мы прогнозируем $y_i$ по формуле $\hy_i=1+\hb x_i$. Найдите $\hb$ методом наименьших квадратов.

\begin{sol}
Имеем следующую задачу:
\[
RSS = \sum\limits_i(y_i-1-\beta x_i)^2 \rightarrow \min\limits_{\beta}
\]

Откуда сразу все находим:
\[
\frac{\partial RSS}{\partial \beta} = \sum\limits_i2(y_i-1-\hb x_i)(-x_i) = 0 \Rightarrow \sum\limits_i (y_i-1-\hb x_i)x_i=0 \Rightarrow
\]
\[
\sum\limits_i x_iy_i-\sum\limits_ix_i - \hb\sum\limits_ix_i^2 = 0 \Rightarrow \hb = \frac{\sum_ix_i(y_i-1)}{\sum_ix_i^2}
\]
\end{sol}
\end{problem}


\begin{problem} %1.12
Перед нами два золотых слитка и весы, производящие взвешивания с ошибками. Взвесив первый слиток, мы получили результат $300$ грамм, взвесив второй слиток — $200$ грамм, взвесив оба слитка — $400$ грамм. Оцените вес каждого слитка методом наименьших квадратов.

\begin{sol}
Обозначив вес первого слитка за \(\beta_1\), вес второго слитка за \(\beta_2\), а показания весов за \(y_i\), получим, что
\[y_1 = \beta_1 + \e_1, \ y_2 = \beta_2 + \e_2, \ y_3 = \beta_1 + \beta_2 + \e_3\]

Тогда
\[(300 - \beta_1)^2 + (200 - \beta_2)^2 + (400 - \beta_1 - \beta_2)^2 \rightarrow \min \limits_{\beta_1,\  \beta_2} \]
\[\hb_1 = \frac{800}{3}, \ \hb_2 = \frac{500}{3} \]
\end{sol}
\end{problem}



\begin{problem} %1.13
Аня и Настя утверждают, что лектор опоздал на 10 минут. Таня считает, что лектор опоздал на 3 минуты. С помощью МНК оцените, на сколько опоздал лектор.

\begin{sol}

Ане, Насте и Тане нужно оценить модель $y_i = \beta + \e$. Для этого они должны решить следующую задачу:

\[RSS = 2(10-\beta)^2 + (3-\beta)^2\rightarrow \min \limits_{\beta}\]

\[\frac{\partial RSS}{\partial \beta} = -4(10-\beta)-2(3-\beta) \]

Условия первого порядка:

\[-4(10-\hb)-2(3-\hb)=0\]

\[20 - 2\hb +3 -\hb=0\]

\[3\hb = 23\]

\[\hb = \frac{23}{3}\]

\end{sol}
\end{problem}



\begin{problem} %1.14
Функция $f(x)$ непрерывна на отрезке $[0;1]$. Найдите аналог МНК-оценок для регрессии без свободного члена в непрерывном случае. Более подробно: найдите минимум по $\hb$ для функции
\[
Q(\hb)= \int_0^1 (f(x)-\hb x)^2\,dx.
\]

\begin{sol}
Условие первого порядка $\int_0^1 -2x(f(x)-\hb x) \, dx =0$, получаем
\[
\hb = \frac{\int_0^1 x f(x)\, dx} {\int_0^1 x^2 \, dx}
\]

\(\hb = \left(\int \limits_0^1 f(x) x dx\right) / \left(\int \limits_0^1 x^2 dx\right)\)
\end{sol}
\end{problem}



\begin{problem} %1.15
Есть двести наблюдений. Вовочка оценил модель $\hy=\hb_1+\hb_2 x$ по первой сотне наблюдений. Петечка оценил модель $\hy=\hat{\gamma}_1+\hat{\gamma}_2 x$ по второй сотне наблюдений. Машенька оценила модель $\hy=\hat{m}_1+\hat{m}_2 x$ по всем наблюдениям.
\begin{enumerate}
\item Возможно ли, что $\hb_2>0$, $\hat{\gamma}_2>0$, но $\hat{m}_2<0$?
\item Возможно ли, что $\hb_1>0$, $\hat{\gamma}_1>0$, но $\hat{m}_1<0$?
\item Возможно ли одновременное выполнение всех упомянутых условий?
\end{enumerate}


\begin{sol}
\begin{enumerate}
\item Проявите воображение! Все зависит от данных. Например, может быть вот так:

\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               frame=lines,
               framesep=2mm]{r}
tikz("../R_plots/aggregate_regression.tikz", standAlone = FALSE, bareBones = TRUE)
n <- 100;
s <- rep(c(0, 4), c(n/2, n/2));
x <- c(1 + runif(n/2), runif(n/2));
y <- 2 * x + s + rnorm(n, sd = 0.15)


plot(x, y, type = "n", frame = "FALSE")
points(x[1 : (n/2)], y[1 : (n/2)], pch = 21,
       col = "black", bg = "ForestGreen", cex = 2)
points(x[(n/2 + 1) : n], y[(n/2 + 1) : n],
       pch = 21, col = "black", bg = "SkyBlue", cex = 2)

modelV1 <- lm(y ~ x + s)
# модели по 1:100 и 101:200 в отдельности
abline(coef(modelV1)[1], coef(modelV1)[2], lwd = 3)
abline(coef(modelV1)[1] + 4 * coef(modelV1)[3],
       coef(modelV1)[2], lwd = 3)
modelV2 <- lm(y ~ x)
# общая модель
abline(modelV2, lwd = 2, col = "red")
invisible(dev.off())
\end{minted}

\begin{minipage}{0.6\textwidth}
\begin{center}
\begin{tikzpicture}[scale = 0.025]
\input{R_plots/aggregate_regression.tikz}
\end{tikzpicture}
\end{center}
\end{minipage}


\item Так тоже бывает:

\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               frame=lines,
               framesep=2mm]{r}
tikz("../R_plots/aggregate_regression_b.tikz", standAlone = FALSE, bareBones = TRUE)
n <- 100;
s <- rep(c(0, 4), c(n/2, n/2));
x <- c(runif(n/2), 1 + runif(n/2));
y <- -2 * x + s + rnorm(n, sd = 0.15)

plot(x, y, type = "n", frame = FALSE)
points(x[1 : (n/2)], y[1 : (n/2)], pch = 21,
       col = "black", bg = "ForestGreen", cex = 2)
points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21,
       col = "black", bg = "SkyBlue", cex = 2)

modelV1 <- lm(y ~ x + s)
# модели по 1:100 и 101:200 в отдельности
abline(coef(modelV1)[1], coef(modelV1)[2], lwd = 3)
abline(coef(modelV1)[1] + 4 * coef(modelV1)[3], coef(modelV1)[2], lwd = 3)
modelV2 <- lm(y ~ x)
# общая модель
abline(modelV2, lwd = 2, col = "red")
invisible(dev.off())
\end{minted}

\begin{minipage}{0.6\textwidth}
\begin{center}
\begin{tikzpicture}[scale = 0.025]
\input{R_plots/aggregate_regression_b.tikz}
\end{tikzpicture}
\end{center}
\end{minipage}

\item А вот так не бывает!
\end{enumerate}
\end{sol}
\end{problem}




\begin{problem} %1.16
Вася оценил модель $y=\beta_1+\beta_2 d+\beta_3 x+\varepsilon$. Дамми-переменная $d$ обозначает пол, 1 для мужчин и 0 для женщин. Оказалось, что $\hb_2>0$. Означает ли это, что для мужчин $\bar{y}$ больше, чем $\bar{y}$ для женщин?


\begin{sol}
Нет. Коэффициенты можно интерпретировать только «при прочих равных», т.е. при равных $x$. Из-за разных $x$ может оказаться, что у мужчин $\bar{y}$ меньше, чем $\bar{y}$ для женщин.
\end{sol}
\end{problem}




\begin{problem} %1.17
Какие из указанные моделей можно представить в линейном виде?
\begin{enumerate}
\item $y_i=\beta_1+\frac{\beta_2}{x_i}+\e_i$;
\item $y_i=\exp(\beta_1+\beta_2 x_i+\e_i)$;
\item $y_i=1+\frac{1}{\exp(\beta_1+\beta_2 x_i+\e_i)}$;
\item $y_i=\frac{1}{1+\exp(\beta_1+\beta_2 x_i+\e_i)}$;
\item $y_i=x_i^{\beta_2}e^{\beta_1+\e_i}$;
\item $y_i=\beta_1\exp(\beta_2 x_i + \e_i)$.
\end{enumerate}


\begin{sol}
Модель можно представить в линейном виде, когда неизвестные параметры входят в нее линейно.
\begin{enumerate}
\item Обозначим $z_i = 1/x_i$, и готово.
\item Возьмем логарифм от обеих частей\ldots
\item Вычтем единицу из обеих частей и снова логарифм\ldots
\item Перевернем обе части уравнения, вычтем единицу и прологарифмируем\ldots
\item Вместо $x_i$ возьмем $e^{x_i}$ и прологарифмируем\ldots
\item Вместо $\beta_1$ возьмем $e^{\beta_1}$ и прологарифмируем\ldots
\end{enumerate}
\end{sol}
\end{problem}




\begin{problem} %1.18
У эконометриста Вовочки есть переменная $1_f$, которая равна 1, если $i$-ый человек в выборке — женщина, и 0, если мужчина. Есть переменная $1_m$, которая равна 1, если $i$-ый человек в выборке — мужчина, и 0, если женщина. Какие $\hy$ получатся, если Вовочка попытается построить регрессии:
\begin{enumerate}
\item $y$ на константу и $1_f$;
\item $y$ на константу и $1_m$;
\item $y$ на $1_f$ и $1_m$ без константы;
\item $y$ на константу, $1_f$ и $1_m$.
\end{enumerate}


\begin{sol}
Пусть \(\bar{y}_m\) — среднее значение \(y\) по выборке для мужчин, \(\bar{y}_f\) — среднее значение \(y\) по выборке для женщин. Тогда

\begin{enumerate}
\item \(\hy = \bar{y}_m + \left( \bar{y}_m -  \bar{y}_f \right) \cdot 1_f \)

Оцениваемая модель:
\[y_i = \beta_1 + \beta_2 1_{f, i} + \e_i \]

Стандартная процедура МНК:
\[RSS = \sum \e_i^2 = \sum \left(y_i - \beta_1 - \beta_2 \cdot 1_f \right)^2 \rightarrow \min \limits_{\beta_1, \beta_2}\]
\[\frac{\partial RSS}{\partial \beta_1} = 2 \sum \left(y_i - \beta_1 - \beta_2\right)(-1) \]
\[\frac{\partial RSS}{\partial \beta_2} = 2 \sum \left(y_i - \beta_1 - \beta_2 \right)(-1_f) \]

Условия первого порядка:
\[\sum \left(y_i - \hb_1 - \hb_2 \cdot 1_f\right) = 0\]
\[\sum \left(\left(y_i - \hb_1 - \hb_2 \cdot 1_f\right)\cdot 1_f\right)= 0\]

Осталось немного поработать с оператором суммирования. Обозначим за \(k\) — число женщин в нашей выборке объемом \(n\), мужчин тогда будет \(n - k\). Тогда
\[\sum \left(y_i - \hb_1 - \hb_2 \cdot 1_f\right) = k \bar{y}_f + (n - k) \bar{y}_m - n \hb_0 - k \hb_2 = 0\]
\[\sum \left(\left(y_i - \hb_1 - \hb_2 \cdot 1_f\right)\cdot 1_f\right) = k \bar{y}_f - k \hb_1 - k \hb_2 =0\]

Отсюда легко ищутся оценки коэффициентов:
\[\hb_1 = \bar{y}_m, \ \hb_2 = \bar{y}_f - \bar{y}_m  \]

Следовательно:
\[\hy = \bar{y}_m + \left( \bar{y}_f -  \bar{y}_m \right) \cdot 1_f \]

\item \(\hy = \bar{y}_f + \left( \bar{y}_m -  \bar{y}_f \right) \cdot 1_m \)

\item \(\hy = \bar{y}_m \cdot 1_m + \bar{y}_f \cdot 1_f \)

\item Условия первого порядка линейного зависимы — мультиколлинеарность. МНК здесь неприменим.
\end{enumerate}
\end{sol}
\end{problem}




\begin{problem} %1.19
У эконометриста Вовочки есть три переменных: $r_i$ — доход $i$-го человека в выборке, $m_i$ — пол (1 — мальчик, 0 — девочка) и $f_i$ — пол (1 — девочка, 0 — мальчик). Вовочка оценил две модели
\begin{enumerate}
\item[] Модель A: $m_i=\beta_1+\beta_2 r_i+\ve_i$;
\item[] Модель B: $f_i=\gamma_1+\gamma_2 r_i+u_i$.
\end{enumerate}
\begin{enumerate}
\item Как связаны между собой оценки $\hb_1$ и $\hat{\gamma}_1$?
\item Как связаны между собой оценки $\hb_2$ и $\hat{\gamma}_2$?
\end{enumerate}


\begin{sol}
Если сложить попарно $m_i$ и $f_i$, то в сумме всегда выйдет единица. А оценки, полученные при помощи метода наименьших квадратов линейны по объясняемой переменной, то есть оценки коэффициентов модели $m_i+f_i \sim \dots$ это суммы соответствующих оценок из двух разных моделей. Но они должны получиться равными 1 и 0 соответствнно (так как зависимая переменная — вектор из единиц). Поэтому $\hb_1 + \hat{\gamma}_1 = 1$, $\hb_2 + \hat{\gamma}_2 = 0$.
\end{sol}
\end{problem}




\begin{problem} %1.20
Эконометрист Вовочка оценил линейную регрессионную модель, где $y$ измерялся в тугриках. Затем он оценил ту же модель, но измерял $y$ в мунгу (1 тугрик = 100 мунгу). Как изменятся оценки коэффициентов?


\begin{sol}
Все оценки коэффициентов увеличатся в 100 раз.
\end{sol}
\end{problem}



\begin{problem} %1.21
Возможно ли, что при оценке парной регрессии $y=\beta_1+\beta_2 x+\e$ оказывается, что $\hb_2>0$, а при оценке регрессии без константы, $y=\gamma x+\e$, оказывается, что $\hat{\gamma}<0$?


\begin{sol}
Да, возможно:

\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               frame=lines,
               framesep=2mm]{r}
tikz("../R_plots/with_and_without_c.tikz", standAlone = FALSE, bareBones = TRUE)
x <- c(rnorm(n, mean = 4, sd = 2))
y <- x - 7 + runif(n, min = -1, max = 1)

plot(x,y, pch = 21, bg = "ForestGreen",
     col = "black", xlim = c(0,10), ylim = c(-5,3))
abline(coef(lm(y ~ x))[1], coef(lm(y ~ x))[2], lwd = 2)
abline(0, coef(lm(y ~ 0 + x))[1] , lwd = 2)
labels <- c("With intercept", "Without intercept")
text(c(7.5, 1.5), c(2, 0.4), labels)
coef(lm(y ~ x))
coef(lm(y ~ 0 + x))
invisible(dev.off())
\end{minted}

\begin{minipage}{0.6\textwidth}
\begin{center}
\begin{tikzpicture}[scale = 0.025]
\input{R_plots/with_and_without_c.tikz}
\end{tikzpicture}
\end{center}
\end{minipage}
\end{sol}
\end{problem}




\begin{problem} %1.22
Эконометрист Вовочка оценил регрессию $y$ только на константу. Какой коэффициент $R^2$ он получит?


\begin{sol}
Так как \(\hy = \hb = \bar{y} \), то \(R^2 = 0\).
\end{sol}
\end{problem}




\begin{problem} %1.23
Эконометрист Вовочка оценил методом наименьших квадратов модель 1, $y=\b_1+\b_2 x+\b_3 z+\e$, а затем модель 2, $y=\b_1+\b_2 x+\b_3 z+\b_4 w+\e$. Сравните полученные $ESS$, $RSS$, $TSS$ и $R^2$.


\begin{sol}
Вспомним формулу для  $TSS$:
\[
TSS = \sum\limits_{i=1}^n (y_i-\bar{y})^2
\]
Так как значения $y$ остались теми же, $TSS_1 = TSS_2$.

\[
RSS = \sum\limits_{i=1}^n (y_i-\hy_i)^2 \hspace{2cm} ESS = \sum\limits_{i=1}^n (\hy_i-\bar{y})^2
\]

Добавление еще одного регрессора не уменьшит точность оценки, то есть $RSS_2\leqslant RSS_1$, $ESS_2 \geqslant ESS_1$.

Соответственно, коэффициент $R^2 = ESS/TSS$ не уменьшится, то есть $R^2_2 \geqslant R^2_1$.

\end{sol}
\end{problem}





\begin{problem} %1.24
Создайте набор данных с тремя переменными $y$, $x$ и $z$ со следующими свойствами. При оценке модели $\hy=\hb_1+\hb_2 x$ получается $\hb_2>0$. При оценке модели $\hy=\hat{\gamma}_1+ \hat{\gamma}_2 x+\hat{\gamma}_3 z$ получается $\hat{\gamma}_2<0$. Объясните принцип, руководствуясь которым легко создать такой набор данных.


\begin{sol}
Интересный результат: смена знака коэффициента перед \(x\) при удалении переменной \(z\) не может произойти, если абсолютное значение \(t\)-статистики коэффициента перед \(z\) в регрессии с регрессорами \(x\) и \(z\) меньше абсолютного значения \(t\)-статистики коэффициента перед \(x\).\footnote{Leamer, E. E., 1975. A Result on the Sign of Restricted Least-Squares Estimates. Journal of Econometrics, 3, 387--390.}

Пример такого набора данных:
\begin{minted}[mathescape,
               linenos,
               numbersep=5pt,
               frame=lines,
               framesep=2mm]{r}
set.seed(777)
y <- 1:50
z <- y + rnorm(length(y), 0, 0.01)
x <- y + rnorm(length(y), 0, 1)
m_short <- lm(y ~ x)
m_long <- lm(y ~ x + z)
texreg(list(m_short, m_long))
\end{minted}

\begin{table}
\begin{center}
\begin{tabular}{l c c }
\hline
 & Model 1 & Model 2 \\
\hline
(Intercept) & $0.04$       & $-0.00$      \\
            & $(0.27)$     & $(0.00)$     \\
x           & $1.00^{***}$ & $-0.00$      \\
            & $(0.01)$     & $(0.00)$     \\
z           &              & $1.00^{***}$ \\
            &              & $(0.00)$     \\
\hline
R$^2$       & 1.00         & 1.00         \\
Adj. R$^2$  & 1.00         & 1.00         \\
Num. obs.   & 50           & 50           \\
RMSE        & 0.94         & 0.01         \\
\hline
\multicolumn{3}{l}{\scriptsize{$^{***}p<0.001$, $^{**}p<0.01$, $^*p<0.05$}}
\end{tabular}
\caption{Statistical models}
\label{table:coefficients}
\end{center}
\end{table}


\end{sol}
\end{problem}



\begin{problem} %1.25
У меня есть набор данных с выборочным средним $\bar{y}$ и выборочной дисперсией $s_y^2$. Как нужно преобразовать данные, чтобы выборочное среднее равнялось $7$, а выборочная дисперсия — $9$?


\begin{sol}
$y_i^*=7+3(y_i-\bar{y})/s_y$
% эта задача не использует понятия вероятностей, хотя близка. Пусть будет в невероятностной секции.
Нужно вспомнить свойства математического ожидания и дисперсии и провести следующие преобразования:
\[
\tilde{y}_i = \frac{y_i-\bar{y}}{s_y} \Rightarrow \E[\tilde{y}] = 0, \hspace{2mm} \Var(\tilde{y}) = 1
\]
\[
y^*_i = \tilde{y}_i\cdot3 + 7 \Rightarrow \E[y^*] = 7, \hspace{2mm} \Var(y^*) = 9
\]
\end{sol}
\end{problem}



\begin{problem} %1.26
Эконометресса Анжела оценила две парных регрессии $\hy_i = 2 - 3 x_i$  и $\hat x_i = 5 - \frac{1}{12}y_i$. Найдите $R^2$ в каждой регрессии и выборочную корреляцию между $x_i$ и $y_i$.
\begin{sol}

$R^2$ одинаков для этих моделей. Докажем это в общем виде.

$R^2$ для модели $\hy_i = \hb_1 +\hb_2 x_i$:

\begin{math}
R^2 = \frac{ESS}{TSS}= \frac{\sum\limits_{i=1}^n (\hat{y_i} - \bar{y})^2}{\sum\limits_{i=1}^n(y_i-\bar{y})^2}=\frac{\sum\limits_{i=1}^n(\hb_1 + \hb_2x_i - \bar{y})^2}{\sum\limits_{i=1}^n(y_i-\bar{y})^2}=\frac{\sum\limits_{i=1}^n(\bar{y} - \hb_2\bar{x} + \hb_2x_i - \bar{y})^2}{\sum\limits_{i=1}^n(y_i-\bar{y})^2}  = \frac{\hb_2^2 \sum\limits_{i=1}^n(x_i - \bar{x})^2}{\sum\limits_{i=1}^n(y_i-\bar{y})^2} = \\ = \frac{ \left( \sum\limits_{i=1}^n(x_i-\bar{x})(y_i - \bar{y}) \right)^2 \sum\limits_{i=1}^n(x_i - \bar{x})^2}{\left( \sum\limits_{i=1}^n(x_i - \bar{x})^2 \right)^2 \sum\limits_{i=1}^n(y_i-\bar{y})^2} = \frac{ \left( \sum\limits_{i=1}^n(x_i-\bar{x})(y_i - \bar{y}) \right)^2}{\sum\limits_{i=1}^n(x_i - \bar{x})^2 = \sum\limits_{i=1}^n(y_i-\bar{y})^2} = \frac{\sum\limits_{i=1}^n(x_i-\bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^n(x_i - \bar{x})^2} \frac{\sum\limits_{i=1}^n(x_i-\bar{x})(y_i - \bar{y})}{\sum\limits_{i=1}^n(y_i-\bar{y})^2} = \\ = \hb_2 \hat{\alpha}_2
\end{math}

Если проделать те же действия для модели $\hat x_i = \hat{\alpha}_1 +\hat{\alpha}_2 x_i$, мы получим тот же результат.

$R^2 = -3 \cdot \frac{-1}{12}=\frac{1}{4}$.

Также можно заметить, что $\hCorr^2 = R^2$. То есть, выборочная корреляция равна $-1/2$, так как коэффициенты отрицательные (между \(y\) и \(x\) отрицательная зависимость).
\end{sol}
\end{problem}

\begin{problem} %1.27
На работе Феофан построил парную регрессию по трём наблюдениям и посчитал прогнозы $\hy_i$. Придя домой он отчасти вспомнил результаты:

\begin{tabular}{rr}
\toprule
$y_i$ & $\hy_i$ \\
\midrule
$0$ & $1$ \\
$6$ & ? \\
$6$ & ? \\
\bottomrule
\end{tabular}

Поднапрягшись, Феофан вспомнил, что третий прогноз был больше второго. Помогите Феофану восстановить пропущенные значения.


\begin{sol}
На две неизвестных $a$ и $b$ нужно два уравнения. Эти два уравнения — ортогональность вектора остатков плоскости регрессоров. А именно:

\[
\begin{cases}
\sum_i (y_i - \hy_i) = 0 \\
\sum_i (y_i - \hy_i) \hy_i = 0 \\
\end{cases}
\]

В нашем случае

\[
\begin{cases}
-1 +(6-a) + (6-b) = 0 \\
-1 + (6 - a)a + (6-b)b = 0 \\
\end{cases}
\]

Решаем квадратное уравнение и получаем два решения: $a=4$ и $a=7$. Итого: $a=4$, $b=7$.
\end{sol}
\end{problem}


\begin{problem}
Вся выборка поделена на две части. Возможны ли такие ситуации:
\begin{enumerate}
\item Выборочная корреляция между $y$ и $x$ примерно равна нулю в каждой части, а по всей выборке примерно равна единице;
\item Выборочная корреляция между $y$ и $x$ примерно ранва единице в каждой части, а по всей выборке примерно равна нулю?
\end{enumerate}

\begin{sol}
Обе ситуации возможны.
\end{sol}
\end{problem}





\Closesolutionfile{solution_file}
