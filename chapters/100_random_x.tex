\Opensolutionfile{solution_file}[solutions/sols_100]
% в квадратных скобках фактическое имя файла

\chapter{Метод инструментальных переменных}



Экзогенность, $\E(\e\mid X)=0$.

Предопределённость, $\E(\e_t\mid x_t)=0$ для всех $t$.



\begin{problem}
Настоящая зависимость между $y_i$ и $x_i$ имеет вид $y_i=\beta x_i +\e_i$, где $\e_i$ независимы и равновероятно принимают значения $-1$ и $+1$. Эконометресса Аврора оценивает коэффициент $\beta$ по двум наблюдениям. Аврора не знает чему равны настоящие $x_i$, вместо них она наблюдает значения $x^*_i=x_i+u_i$, где $u_i$ — ошибки измерения, поэтому она строит регрессию $y_i$ на $x_i^*$. Ошибки измерения регрессора, $u_i$, независимы и равновероятно принимают значения $-1$ и $+1$. Величины $u_i$ и $\e_j$ независимы.
\begin{enumerate}
\item Выведите явную формулу для оценки $\hb$.
\item Чему равно $u_i^2$? Чему равно $\e_i^2$?
\item Найдите $\E(\hb)$, если $x_1=0$ и $x_2=1$.
\item Найдите $\E(\hb)$, если $x_1=0$ и $x_2=2$.
\item Интуитивно объясните, как меняется смещение с ростом расстояния между $x_1$ и $x_2$.
\end{enumerate}


\begin{sol}
Замечаем, что $u_i^2=\e_i^2=1$. Считаем:
\[
\E(\hb \mid x_1, x_2) = \E\left( \frac{x_1^*}{(x_1^*)^2  + (x_2^*)^2}\right) \E(y_1) + \E\left( \frac{x_2^*}{(x_1^*)^2  + (x_2^*)^2}\right) \E(y_2)
\]

В обоих случаях $\E(y_1)=0$. Получаем, $\E(\hb \mid x_1=0, \, x_2=1)=0.2\beta$, $\E(\hb \mid x_1=0, \, x_2=2)=0.8\beta$. Интуитивно объясняем: рисуем прямую по двум точкам. Мы знаем абсциссы точек с точностью $\pm 1$. Если точки близки, то это может сильно менять оценку наклона, если точки далеки, то случайность слабо влияет на наклон.
\end{sol}
\end{problem}


\begin{problem}
Известна совместная функция плотности пары величин $X_i$, $Y_i$
\[
f(x,y)=
\begin{cases}
x+y, \; x\in[0;1], \; y\in [0;1] \\
0, \; \text{иначе}
\end{cases}
\]
Найдитe
\begin{enumerate}
\item $\E(X_i)$, $\E(Y_i)$, $\Var(X_i)$, $\Var(Y_i)$, $\Cov(X_i,Y_i)$;
\item $\E(Y_i \mid X_i)$, $\E(X_i \mid Y_i)$;
\item Вася оценивает модель $y_i=\beta_1+\beta_2 x_i+\e_i$ по огромному количеству наблюдений. Чему примерно у него окажутся равны $\hb_1$, $\hb_2$, $\hs^2$, $\hVar(\hb_2)$? Чему равно $\E(\hb_2)$?
%\item Петя оцениваем модель $y_i=\beta_1+\beta_2 x_i+\beta_2 x_i^2+\e_i$. Найдите $\E(\hb_1)$, $\E(\hb_2)$, $\E(\hb_3)$, $\Var(\hb)$ (?)
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Задан совместный закон распределения случайных величин $x$ и $\e$:

\begin{tabular}{c|cccc}
\toprule
 & $\e=-2$ & $\e=-1$ & $\e=1$ & $\e=-2$ \\
\midrule
$x=1$ & 0.2 & 0.3 & 0 & 0 \\
$x=2$ & 0 & 0 & 0.3 & 0.2 \\
\bottomrule
\end{tabular}

Найдите $\E(\varepsilon)$, $\E(\varepsilon \mid x)$, $\Cov(\varepsilon,x)$, $\Cov(\varepsilon,x|x)$


\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Приведите примеры дискретных случайных величин $\e$ и $x$, таких, что
\begin{enumerate}
\item $\E(\e)=0$, $\E(\e\mid x)=0$, но величины зависимы. Чему в этом случае равно $\Cov(\e,x)$?
\item $\E(\e)=0$, $\Cov(\e,x)=0$, но $\E(\e\mid x)\neq 0$. Зависимы ли эти случайные величины?
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Эконометресса Агнесса хочет оценить модель $y_i=\beta_1+\beta_2 x_i +\e_i$, но, к сожалению, величина $x_i$ ненаблюдаема. Вместо неё доступна величина  $x_i^*$. Величина $x_i^*$  содержит ошибку измерения, $x_i^*=x_i+a_i$. Известно, что $\Var(x_i)=9$, $\Var(a_i)=4$,   $\Var(\e_i)=1$. Величины $x_i$, $a_i$ и $\e_i$ независимы.

Агнесса оценивает регрессию $\hy_i=\hb_1+\hb_2 x_i^*$ с помощью МНК.
\begin{enumerate}
\item Найдите $\plim \hb_2$.
\item Являются ли оценки, получаемые Агнессой, состоятельными?
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
ошибка измерения и коррелированная инструментальная ?
\todo[inline]{допридумывать задачу на инструментальные}
\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Эконометресса Анжелла хочет оценить модель $y_i=\beta_1+\beta_2 x_i +\beta_3 w_i +\e_i$, но, к сожалению, величина $w_i$ ненаблюдаема. Известно, что $\Var(x_i)=9$, $\Var(w_i)=4$,  $\Var(\e_i)=1$ и $\Cov(x_i,w_i)=-2$. Случайная составляющая не коррелированна с регрессорами.

За неимением $w_i$ Анжелла оценивает регрессию $\hy_i=\hb_1+\hb_2 x_i$ с помощью МНК.
\begin{enumerate}
\item Найдите $\plim \hb_2$.
\item Являются ли оценки, получаемые Агнессой, состоятельными?
\end{enumerate}

%Анжелла сумела найти переменную $z_i$, такую что $\Cov(z_i,\e_i)=0$ $\Cov(z_i,


\begin{sol}
\end{sol}
\end{problem}




\begin{problem}
Насяльника хочет оценить модель $y_i=\beta_1+\beta_2 x_i +\e_i$, к сожалению, величина $x_i$ ненаблюдаема. Вместо неё доступны  $x_i^a$, померянный Равшаном, и $x_i^b$, померянный Джумшутом. Естественно, $x_i^a$ и $x_i^b$ содержат ошибку измерения, $x_i^a=x_i+a_i$, $x_i^b=x_i+b_i$. Известно, что $\Var(x_i)=\sigma^2_x$, $\Var(a_i)=\sigma^2_a$,  $\Var(b_i)=\sigma^2_b$,  $\Var(\e_i)=\sigma^2_{\e}$. Величины $x_i$, $a_i$, $b_i$ и $\e_i$ независимы.

\begin{enumerate}
\item Найдите асимптотическое смещение оценки $\hb_2$ для следующих случаев:
\begin{enumerate}
\item С помощью МНК оценивается модель $\hy_i=\hb_1+\hb_2 x_i^a$.
\item С помощью МНК оценивается модель $\hy_i=\hb_1+\hb_2 x_i^b$.
\item С помощью МНК оценивается модель $\hy_i=\hb_1+\hb_2 \frac{x_i^a+x_i^b}{2}$
\item Методом инструментальных переменных оценивается модель $\hy_i=\hb_1+\hb_2 x_i^a$, в роли инструмента выступает $x_i^b$.
\item Методом инструментальных переменных оценивается модель $\hy_i=\hb_1+\hb_2 x_i^b$, в роли инструмента выступает $x_i^a$.
\end{enumerate}
\item Какой из методов даёт наибольшее смещение? Наименьшее смещение?
\item Какой из вариантов применения инструментальных переменных предпочтительнее с точки зрения дисперсии получаемой оценки $\hb_2$?
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Наблюдения представляют собой случайную выборку. Известна истинная ковариационная матрица и математическое ожидание:
\[
\Var
\begin{pmatrix}
x_i \\
y_i
\end{pmatrix}
=
\begin{pmatrix}
9 & 1 \\
1 & 16
\end{pmatrix}, \;
\E\begin{pmatrix}
x_i \\
y_i
\end{pmatrix}
=
\begin{pmatrix}
2 \\
-3
\end{pmatrix}
\]

\begin{enumerate}
\item Найдите $\plim \hb_1$ и $\plim \hb_2$ в регрессии $\hy_i = \hb_1 + \hb_2 x_i$.
\item Найдите $\plim \hb_1$ и $\plim \hb_2$ в регрессии $\hat x_i = \hb_1 + \hb_2 y_i$.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}





\begin{problem}
Наблюдения представляют собой случайную выборку. Известна истинная ковариационная матрица:
\[
\Var
\begin{pmatrix}
x_i \\
y_i \\
z_i
\end{pmatrix}
 =
\begin{pmatrix}
9 & 1 & 2 \\
1 & 16 & -1 \\
2 & -1 & 25
\end{pmatrix}
\]

\begin{enumerate}
\item Найдите $\plim \hb_2$ в модели $\hy_i = \hb_1 + \hb_2 z_i$.
\item Найдите $\plim \hat \mu_2$ в модели $\hat x_i = \hat \mu_1 + \hat \mu_2 z_i$.
\item Найдите $\plim \hat \gamma_2$ в модели $\hy_i = \hat \gamma_1 + \hat \gamma_2 x_i + \hat \gamma_3 z_i $.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим классическую линейную регрессионную модель:
\[
y_i = \b_1 + \b_2 x_i + \b_3 z_i + \e_i
\]

\begin{enumerate}
\item Огюст Люмьер утверждает, что при нестохастических регрессорах математические ожидания $\E(y_i)$ различны. Луи Люмьер утверждает, что при стохастических регрессорах и предпосылке о том, что наблюдения являются случайной выборкой, все $\E(y_i)$ равны между собой. Кто из них прав?
\item Помогите Луи Люмьеру найти $\plim \he_1$ и $\plim \hy_1$.
\end{enumerate}

\begin{sol}
Оба правы, $\plim \he_1 = \e_1$ и $\plim \hy_1= \b_1 + \b_2 x_i + \b_3 z_i$.
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим классическую линейную регрессионную модель:
\[
y_i = \b_1 + \b_2 x_i + \b_3 z_i + \e_i
\]

Наблюдения является случайной выборкой. Истинная ковариция $\Cov(x_i, z_i)$ равна нулю. Мы оцениваем с помощью МНК две регрессии.

Регрессия 1:
\[
\hy_i = \hb_1 + \hb_2 x_i + \hb_3 z_i
\]

Регрессия 2:
\[
\hy_i = \hat \gamma_1 + \hat \gamma_2 x_i
\]

\begin{enumerate}
\item Верно ли, что $\hb_2$ совпадает с $\hat \gamma_2$?
\item Верно ли, что $\plim \hb_2 = \plim \hat \gamma_2$?
\end{enumerate}

\begin{sol}
В общем случае $\hb_2 \neq \hat \gamma_2$, однако $\plim \hb_2 = \plim \hat \gamma_2$.
\end{sol}
\end{problem}


\begin{problem}
Наблюдения представляют собой случайную выборку. Зависимые переменные $y_{t1}$ и $y_{t2}$ находятся из системы:

\[
\begin{cases}
y_{t1} = \beta_{11} + \beta_{12} x_t + \e_{t1} \\
y_{t2} = \beta_{21} + \beta_{22} z_t + \beta_{23} y_{t1} + \e_{t2}
\end{cases},
\]
где вектор ошибок $\e_t$ имеет совместное нормальное распределение
\[
\e_t \sim \cN\left(
\begin{pmatrix}
  0 \\
  0
\end{pmatrix};
\begin{pmatrix}
  1 & \rho \\
  \rho & 1
\end{pmatrix}
\right)
\]

Эконометресса Анжела оценивает с помощью МНК первое уравнение, а эконометресса Эвридика — второе.
\begin{enumerate}
\item Найдите пределы по вероятности получаемых ими оценок.
\item Будут ли оценки состоятельными?
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
  Эконометресса Венера оценивает регрессию
  \[
    y_i = \beta_1 + \beta_2 x_i + u_i,
  \]

  А на самом деле $y_i = \beta_1 + \beta_2 x_i + \beta_3 x_i^2 + w_i$, причём $x_i \sim \cN (0; 1)$ и ошибки $w_i \sim \cN(0; \sigma^2)$. Все остальные предпосылки теоремы Гаусса-Маркова выполнены.

  \begin{enumerate}
    \item Будут ли оценки $\hb_1$ и $\hb_2$, получаемые Венерой, несмещёнными?
    \item Будут ли оценки $\hb_1$ и $\hb_2$, получаемые Венерой, состоятельными?
  \end{enumerate}

\begin{sol}
  Оценки будут несмещёнными и состоятельными. Вызвано это тем, что $u_i = \beta_3 x_i^2 + w_i$ некоррелировано с $x_i$.
\end{sol}
\end{problem}

\begin{problem}
Вектора $(x_1, u_1)$, $(x_2, u_2)$, \ldots независимы и одинаково распределены. Также известно, что $x_i \sim \cN(10; 9)$ и $\E(u_i|x_i)=0$.

Найдите $\plim \left(\frac{1}{n}x'x\right)^{-1}$, $\plim \frac{1}{n}x'u$ и $\plim (x'x)^{-1}x'u$
\begin{sol}
$\plim \left(\frac{1}{n}x'x\right)^{-1} = 109^{-1}$, $\plim \frac{1}{n}x'u = 0$ и $\plim (x'x)^{-1}x'u = 0$
\end{sol}
\end{problem}


\begin{problem}
  Возможно ли, что $\E(x_i | u_i)=0$ и $\E(u_i|x_i)=0$, но при этом $x_i$ и $u_i$ зависимы?
\begin{sol}
  Да, например, равномерное распределение $(u_i, x_i)$ на круге или на окружности. Или равновероятное на восьми точках, $(\pm 1, \pm 1)$, $(\pm 2, \pm 2)$.
\end{sol}
\end{problem}


\begin{problem}
В некотором интституте на некотором факультете задумали провести эксперимент: раздать студентам учебники разных цветов случайным образом и посмотреть на итоговую успеваемость по эконометрике. Учебники есть двух цветов: зелёненькие и красненькие. Поэтому модель имеет вид:
\[
y_i = \beta_1 + \beta_2 green_i + u_i
\]

Здесь $y_i$ — результат по эконометрике, $green_i$ — дамми-переменная на зелёный учебник и $u_i$ — прочие характеристики студента. Зелёные и красненькие учебники планировалось раздавать равновероятно. Однако библиотекарь всё прошляпил и разрешил студентам самим выбирать учебник, какой понравится. В результате вместо переменной $green_i$ получилась переменная $green_i^*$. Известно, что $\E(green_i^*)=\alpha$ и $\Cov(green_i^*, u_i)=\gamma$.

Де-факто оценивалась модель
\[
\hy_i = \hat \theta_1 + \hat \theta_2 green_i^*
\]

\begin{enumerate}
\item Найдите $\plim \hat \theta_1$, $\plim \hat \theta_2$.
\item Найдите $\E \hat \theta_1$, $\E \hat \theta_2$.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
  Придумайте такие случайные величины $x_1$, $x_2$, $u_1$, что $x_1$ и $x_2$ независимы и одинаково распределены, причём $\E(u_1|x_1)=0$, а $\E(u_1|x_1, x_2) \neq 0$.
\begin{sol}
  Например, можно взять $u_1=x_2$, и все величины $\cN(0;1)$.
\end{sol}
\end{problem}

\begin{problem}
 Рассмотрим классическую парную регрессию со стохастическим регрессором. Всего три наблюдения:

 \begin{tabular}{rr}
 \toprule
 $y_1$ & $x_1$ \\
 $y_2$ & $x_2$ \\
 $y_3$ & $x_3$ \\
 \bottomrule
 \end{tabular}

\begin{enumerate}
 \item Соедините линиями независимые случайные величины.
 \item Соедините линиями одинаково распределённые случайные величины.
\end{enumerate}

\begin{sol}
  Одинаково распределены: $y_1 \sim y_2 \sim y_3$, $x_1 \sim x_2 \sim x_3$. Независимы переменные с разными номерами.
\end{sol}
\end{problem}


\begin{problem}
Исследователь Кирилл хочет посчитать вероятность того, что температура за окном $z$ больше нуля.
Кирилл видит показания градусника, $z + \e$, где $\e$ — ошибка измерения.
Случайные величины $z$ и $\e$ независимы,
ошибка измерения $\e$ распределена симметрично около нуля.
Кирилл задумался о поведении функции $h(t)=\P(z > 0 | z + \e = t)$.

  \begin{enumerate}
      \item Верно ли, что $h(t)$ неубывающая функция?
      \item Найдите $h(t)$ если $z$ и $\e$ имеют совместное нормальное распределение с $z\sim \cN(\mu_z, \sigma^2_z)$, $\e\sim \cN(0;1)$.
  \end{enumerate}
  \begin{sol}
  \begin{enumerate}
    \item В общем случае функция $h(t)$ может быть немонотонной.
      Например, ошибка $\e$ равновероятно принимает значения $-100$, $-1$, $1$ и $100$,
      а погода за окном равновероятно принимает значения $-99$ и $1$.
      Тогда знание того, что $z+\e=0$, позволяет сделать вывод, что $z=1$,
      а знание того, что $z+\e = 1$, приводит к выводу $z=-99$.
    \item Сначала находим $\E(z|z+\e)$ и $\Var(z|z+\e)$, а затем ответ выражается через стандартную нормальную функцию распределения $F$.
  \end{enumerate}
\end{sol}
\end{problem}




\Closesolutionfile{solution_file}
