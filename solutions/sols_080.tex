\protect \hypertarget {soln:8.1}{}
\begin{solution}{{8.1}}
Рассмотрим вопрос на примере парной регрессии $y_i = \beta_1 + \beta_2 x_i + \e_i$. Можно выделить условную и безусловную гетероскедастичность. Безусловная — $\Var(\e_i)\neq const$. Условная — $\Var(\e_i | x_i) \neq const$.

Условная и безусловная дисперсии связаны соотношением:

\[
\Var(\e_i) = \E(\Var(\e_i|x_i)) + \Var(\E(\e_i|x_i))
\]

То есть при условии $\E(\e_i |x_i)=0$ из условной гомоскедастичности следует безусловная.

Удобно изучать гетероскедастичность при парадигме стохастических регрессоров, а именно, предполагать, что наблюдения представляют собой случайную выборку. В этой ситуации получаются одновременно условно гетероскедастичные и безусловно гомоскедастичные ошибки.
\end{solution}
\protect \hypertarget {soln:8.2}{}
\begin{solution}{{8.2}}
Поделить зависимую переменную и каждый регрессор, включая единичный столбец, на $|x_i|$.
\end{solution}
\protect \hypertarget {soln:8.3}{}
\begin{solution}{{8.3}}
Поделить зависимую переменную и каждый регрессор, включая единичный столбец, на $\sqrt{|x_i|}$.
\end{solution}
\protect \hypertarget {soln:8.4}{}
\begin{solution}{{8.4}}
$\Var(\e_i)=cx_i^4$
\end{solution}
\protect \hypertarget {soln:8.5}{}
\begin{solution}{{8.5}}
$\Var(\e_i)=c x_i$
\end{solution}
\protect \hypertarget {soln:8.6}{}
\begin{solution}{{8.6}}
По графику видно, что с увеличением общей площади увеличивается разброс цены. Поэтому разумно, например, рассмотреть следующие подходы:
\begin{enumerate}
\item Перейти к логарифмам, т.е. оценивать модель $\ln price_i=\beta_1+\beta_2 \ln totsp_i +\varepsilon_i$.
\item Оценивать квантильную регрессию. В ней угловые коэффициенты линейной зависимости будут отличаться для разных квантилей переменной $price$.
\item Обычную модель линейной регрессии с гетероскедастичностью вида $\Var(\varepsilon_i)=\sigma^2 totsp_i^2$.
\end{enumerate}
\end{solution}
\protect \hypertarget {soln:8.7}{}
\begin{solution}{{8.7}}
\end{solution}
\protect \hypertarget {soln:8.8}{}
\begin{solution}{{8.8}}
Протестируем гетероскедастичность ошибок при помощи теста Голдфельда-
Квандта. $H_0: \Var(\e_i)=\sigma^2$, $H_a: \Var(\e_i)=f(x_i)$

\begin{enumerate}
\item Тестовая статистика $GQ=\frac{RSS_3/(n_3-k)}{RSS_1/(n_1-k)}$, где $n_1=11$ — число наблюдений в первой подгруппе, $n_3=11$ — число наблюдений в
последней подгруппе, $k=3$ — число факторов в модели, считая единичный столбец.
\item Распределение тестовой статистики при верной $H_0$: $GQ\sim F_{n_3-k,n_1-k}$
\item Наблюдаемое значение $GQ_{obs}=1.41$
\item Область, в которой $H_0$ не отвергается: $GQ\in [0;3.44]$
\item Статистический вывод: поскольку $GQ_{obs} \in [0;3.44]$, то на основании имеющихся наблюдений на уровне значимости 5\% основная гипотеза $H_0$ не может быть отвергнута. Таким образом, тест Голдфельда-Квандта не выявил гетероскедастичность.
\end{enumerate}
\end{solution}
\protect \hypertarget {soln:8.9}{}
\begin{solution}{{8.9}}
Протестируем гетероскедастичность ошибок при помощи теста Голдфельда-
Квандта. $H_0: \Var(\e_i)=\sigma^2$, $H_a: \Var(\e_i)=f(x_i)$

\begin{enumerate}
\item Тестовая статистика $GQ=\frac{RSS_3/(n_3-k)}{RSS_1/(n_1-k)}$, где $n_1=21$ — число наблюдений в первой подгруппе, $n_3=21$ — число наблюдений в
последней подгруппе, $k=3$ — число факторов в модели, считая единичный столбец.
\item Распределение тестовой статистики при верной $H_0$: $GQ\sim F_{n_3-k,n_1-k}$
\item Наблюдаемое значение $GQ_{obs}=6.49$
\item Область, в которой $H_0$ не отвергается: $GQ\in [0;3.12]$
\item Статистический вывод: поскольку $GQ_{obs} \notin [0;3.12]$, то на основании имеющихся наблюдений на уровне значимости 1\% основная гипотеза $H_0$ отвергается. Таким образом, тест Голдфельда-Квандта выявил гетероскедастичность.
\end{enumerate}
\end{solution}
\protect \hypertarget {soln:8.10}{}
\begin{solution}{{8.10}}
Протестируем гетероскедастичность ошибок при помощи теста Голдфельда-
Квандта. $H_0: \Var(\e_i)=\sigma^2$, $H_a: \Var(\e_i)=f(x_i)$

\begin{enumerate}
\item Тестовая статистика $GQ=\frac{RSS_3/(n_3-k)}{RSS_1/(n_1-k)}$, где $n_1=11$ — число наблюдений в первой подгруппе, $n_3=11$ — число наблюдений в
последней подгруппе, $k=3$ — число факторов в модели, считая единичный столбец.
\item Распределение тестовой статистики при верной $H_0$: $GQ\sim F_{n_3-k,n_1-k}$
\item Наблюдаемое значение $GQ_{obs}=2.88$
\item Область, в которой $H_0$ не отвергается: $GQ\in [0;3.44]$
\item Статистический вывод: поскольку $GQ_{obs} \in [0;3.44]$, то на основании имеющихся наблюдений на уровне значимости 5\% основная гипотеза $H_0$ не может быть отвергнута. Таким образом, тест Голдфельда-Квандта не выявил гетероскедастичность.
\end{enumerate}
\end{solution}
\protect \hypertarget {soln:8.11}{}
\begin{solution}{{8.11}}
Протестируем гетероскедастичность ошибок при помощи теста Голдфельда-
Квандта. $H_0: \Var(\e_i)=\sigma^2$, $H_a: \Var(\e_i)=f(x_i)$

\begin{enumerate}
\item Тестовая статистика $GQ=\frac{RSS_3/(n_3-k)}{RSS_1/(n_1-k)}$, где $n_1=21$ — число наблюдений в первой подгруппе, $n_3=21$ — число наблюдений в
последней подгруппе, $k=3$ — число факторов в модели, считая единичный столбец.
\item Распределение тестовой статистики при верной $H_0$: $GQ\sim F_{n_3-k,n_1-k}$
\item Наблюдаемое значение $GQ_{obs}=5.91$
\item Область, в которой $H_0$ не отвергается: $GQ\in [0;2.21]$
\item Статистический вывод: поскольку $GQ_{obs} \notin [0;2.21]$, то на основании имеющихся наблюдений на уровне значимости 5\% основная гипотеза $H_0$ отвергается. Таким образом, тест Голдфельда-Квандта выявил гетероскедастичность.
\end{enumerate}
\end{solution}
\protect \hypertarget {soln:8.12}{}
\begin{solution}{{8.12}}
Протестируем гетероскедастичность ошибок при помощи теста Уайта. $H_0: \Var(\e_i)=\sigma^2$, $H_a: \Var(\e_i)=\delta_1+\delta_2 x_i +\delta_3 z_i+\delta_4 x_i^2+\delta_5 z_i^2+\delta_6 x_i z_i$.
\begin{enumerate}
\item Тестовая статистика $W=n\cdot R^2_{aux}$, где $n$ — число наблюдений, $R^2_{aux}$ — коэффициент детерминации для вспомогательной регрессии.
\item Распределение тестовой статистики при верной $H_0$: $W\sim \chi^2_{k_{aux}-1}$, где $k_{aux}=6$ — число регрессоров во вспомогательной регрессии, считая константу.
\item Наблюдаемое значение тестовой статистики: $W_{obs}=18$
\item Область, в которой $H_0$ не отвергается: $W\in [0;W_{crit}]=[0;11.07]$
\item Статистический вывод: поскольку $W_{obs} \notin [0;11.07]$, то на основании имеющихся наблюдений на уровне значимости 5\% основная гипотеза $H_0$ отвергается. Таким образом, тест Уайта выявил гетероскедастичность.
\end{enumerate}
\end{solution}
\protect \hypertarget {soln:8.13}{}
\begin{solution}{{8.13}}
\end{solution}
\protect \hypertarget {soln:8.14}{}
\begin{solution}{{8.14}}
\end{solution}
\protect \hypertarget {soln:8.15}{}
\begin{solution}{{8.15}}
\end{solution}
\protect \hypertarget {soln:8.16}{}
\begin{solution}{{8.16}}
\end{solution}
\protect \hypertarget {soln:8.17}{}
\begin{solution}{{8.17}}
\end{solution}
\protect \hypertarget {soln:8.18}{}
\begin{solution}{{8.18}}
\end{solution}
\protect \hypertarget {soln:8.19}{}
\begin{solution}{{8.19}}
\[
\E(\hb|X) = \E((X'X)^{-1}X'y|X)=(X'X)^{-1}X'\E(X\beta + u|X)=\beta + \E(u|X)=\beta
\]
\end{solution}
\protect \hypertarget {soln:8.20}{}
\begin{solution}{{8.20}}
\end{solution}
\protect \hypertarget {soln:8.21}{}
\begin{solution}{{8.21}}
\end{solution}
\protect \hypertarget {soln:8.22}{}
\begin{solution}{{8.22}}
\end{solution}
\protect \hypertarget {soln:8.23}{}
\begin{solution}{{8.23}}
$0.0752$, $5$, $10$
\end{solution}
\protect \hypertarget {soln:8.24}{}
\begin{solution}{{8.24}}
$k(k+1)/2$
\end{solution}
\protect \hypertarget {soln:8.25}{}
\begin{solution}{{8.25}}
\end{solution}
\protect \hypertarget {soln:8.26}{}
\begin{solution}{{8.26}}
Известно, что оценки параметров, получаемые по обобщённому методу наименьших квадратов, являются наилучшими, поэтому:
$\delta^2
\begin{pmatrix}
x_1     & 0      & \cdots & 0 \\
0       & x_2    & \cdots & 0 \\
\vdots  & \vdots & \ddots & \vdots \\
0       & 0      & \cdots & x_n \\
\end{pmatrix}$
\end{solution}
\protect \hypertarget {soln:8.27}{}
\begin{solution}{{8.27}}
\begin{multline*}
\Cov(\hb_{GLS}, \e) = \Cov \left( (X' \Sigma^{-1} X)^{-1} X' \Sigma^{-1} y, \e \right) = \\
= \Cov \left( (X' \Sigma^{-1} X)^{-1} X' \Sigma^{-1} \e, \e \right) = \\
= (X' \Sigma^{-1} X)^{-1} X' \Sigma^{-1} \Cov(\e, \e) =\\
= (X' \Sigma^{-1} X)^{-1} X' \Sigma^{-1} \Sigma = (X' \Sigma^{-1} X)^{-1} X'
\end{multline*}
\end{solution}
\protect \hypertarget {soln:8.28}{}
\begin{solution}{{8.28}}
Для нахождения эффективной оценки воспользуемся взвешенным методом наименьших квадратов. Разделим каждое из уравнений $y_i = \beta_1 + \e$ на корень из дисперсии $\e_i$ с тем, чтобы ошибки в полученных уравнениях имели равные дисперсии (в этом случае можно будет сослаться на т. Гаусса-Маркова). Итак, после деления i-го уравнения на величину $\sqrt{x_i}/\sigma_{\e}$, мы получаем:
\[
\begin{pmatrix}
y_1 \sqrt{x_1}/\sigma_{\e} \\
y_2 \sqrt{x_2}/\sigma_{\e} \\
\ldots \\
y_n \sqrt{x_n}/\sigma_{\e} \\
\end{pmatrix} = \beta_1 \begin{pmatrix}
\sqrt{x_1}/\sigma_{\e} \\
\sqrt{x_2}/\sigma_{\e} \\
\ldots \\
\sqrt{x_n}/\sigma_{\e} \\
\end{pmatrix} + \begin{pmatrix}
\e_1 \sqrt{x_1}/\sigma_{\e} \\
\e_2 \sqrt{x_2}/\sigma_{\e} \\
\ldots \\
\e_n \sqrt{x_n}/\sigma_{\e} \\
\end{pmatrix}
\]
Поскольку условия т. Гаусса-Маркова для последней модели выполнены, то МНК-оценка для последней модели будет наиболее эффективной. Поэтому
\[
\hb_1 = \frac{\sum_{i=1}^n (y_i \sqrt{x_i}/\sigma_{\e})(\sqrt{x_i}/\sigma_{\e})}{\sum_{i=1}^n (\sqrt{x_1}/\sigma_{\e})} = \frac{\sum_{i=1}^n y_i x_i}{\sum_{i=1}^n x_i^2}
\]
\end{solution}
\protect \hypertarget {soln:8.29}{}
\begin{solution}{{8.29}}
\end{solution}
\protect \hypertarget {soln:8.30}{}
\begin{solution}{{8.30}}
В предположении о гомоскедастичности, $\gamma_2=0$, оценка правдоподобия совпадает с МНК-оценкой, значит $\hb=\sum y_i x_i/ \sum x_i^2$. И $\hs^2_i=RSS/n$, значит $\hat{\gamma_1}=\ln(RSS/n)$.
\end{solution}
\protect \hypertarget {soln:8.31}{}
\begin{solution}{{8.31}}
Решение средствами пакета \verb|sandwich|
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{df} \hlkwb{<-} \hlkwd{tibble}\hlstd{(}\hlkwc{y} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{3}\hlstd{,} \hlnum{3}\hlstd{),} \hlkwc{x} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{))}

\hlstd{model} \hlkwb{<-} \hlkwd{lm}\hlstd{(}\hlkwc{data} \hlstd{= df, y} \hlopt{~} \hlstd{x)}
\hlkwd{coef}\hlstd{(model)}
\end{alltt}
\begin{verbatim}
## (Intercept)           x
##           2           1
\end{verbatim}
\begin{alltt}
\hlcom{# residuals}
\hlkwd{resid}\hlstd{(model)}
\end{alltt}
\begin{verbatim}
##             1             2             3
## -1.000000e+00  1.000000e+00 -1.665335e-16
\end{verbatim}
\begin{alltt}
\hlkwd{vcov}\hlstd{(model)}
\end{alltt}
\begin{verbatim}
##             (Intercept)  x
## (Intercept)           1 -1
## x                    -1  3
\end{verbatim}
\begin{alltt}
\hlkwd{vcovHC}\hlstd{(model)} \hlcom{# should fail}
\end{alltt}
\begin{verbatim}
##             (Intercept)   x
## (Intercept)         NaN NaN
## x                   NaN NaN
\end{verbatim}
\begin{alltt}
\hlkwd{vcovHC}\hlstd{(model,} \hlkwc{type} \hlstd{=} \hlstr{"HC0"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##             (Intercept)    x
## (Intercept)         0.5 -0.5
## x                  -0.5  0.5
\end{verbatim}
\begin{alltt}
\hlcom{# help(vcovHC)}
\end{alltt}
\end{kframe}
\end{knitrout}

Решение с ручным подсчётом матриц
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{y} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{3}\hlstd{,} \hlnum{3}\hlstd{)}
\hlstd{X} \hlkwb{<-} \hlkwd{cbind}\hlstd{(}\hlkwd{rep}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{3}\hlstd{),} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{))}

\hlstd{hat_beta} \hlkwb{<-} \hlkwd{solve}\hlstd{(}\hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{X)} \hlopt{%*%} \hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{y}
\hlstd{hat_beta}
\end{alltt}
\begin{verbatim}
##      [,1]
## [1,]    2
## [2,]    1
\end{verbatim}
\begin{alltt}
\hlcom{# by hand Var(hat_beta)}
\hlstd{y_hat} \hlkwb{<-} \hlstd{X} \hlopt{%*%} \hlstd{hat_beta}
\hlstd{e_hat} \hlkwb{<-} \hlstd{y} \hlopt{-} \hlstd{y_hat}
\hlstd{RSS} \hlkwb{<-} \hlkwd{sum}\hlstd{((y} \hlopt{-} \hlstd{y_hat)}\hlopt{^}\hlnum{2}\hlstd{)}
\hlstd{vcov_ols} \hlkwb{<-} \hlstd{RSS} \hlopt{/} \hlstd{(}\hlnum{3} \hlopt{-} \hlnum{2}\hlstd{)} \hlopt{*} \hlkwd{solve}\hlstd{(}\hlkwd{t}\hlstd{(X)} \hlopt{%*%} \hlstd{X)}
\hlstd{vcov_ols}
\end{alltt}
\begin{verbatim}
##      [,1] [,2]
## [1,]    1   -1
## [2,]   -1    3
\end{verbatim}
\begin{alltt}
\hlcom{# crossprod(X) is just synonym for t(X) %*% X}

\hlstd{H} \hlkwb{<-} \hlstd{X} \hlopt{%*%} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(X))} \hlopt{%*%} \hlkwd{t}\hlstd{(X)}
\hlstd{H}
\end{alltt}
\begin{verbatim}
##              [,1]         [,2]         [,3]
## [1,] 5.000000e-01 5.000000e-01 5.551115e-17
## [2,] 5.000000e-01 5.000000e-01 5.551115e-17
## [3,] 5.551115e-17 5.551115e-17 1.000000e+00
\end{verbatim}
\begin{alltt}
\hlkwd{diag}\hlstd{(H)}
\end{alltt}
\begin{verbatim}
## [1] 0.5 0.5 1.0
\end{verbatim}
\begin{alltt}
\hlstd{S_hat_white} \hlkwb{<-} \hlkwd{diag}\hlstd{(}\hlkwd{as.vector}\hlstd{(e_hat}\hlopt{^}\hlnum{2}\hlstd{))}
\hlstd{S_hat_HC3} \hlkwb{<-} \hlkwd{diag}\hlstd{(}\hlkwd{as.vector}\hlstd{(e_hat}\hlopt{^}\hlnum{2}\hlstd{)}\hlopt{/}\hlstd{(}\hlnum{1} \hlopt{-} \hlkwd{diag}\hlstd{(H))}\hlopt{^}\hlnum{2}\hlstd{)}
\hlstd{S_hat_HC3} \hlcom{# look at the problem}
\end{alltt}
\begin{verbatim}
##      [,1] [,2] [,3]
## [1,]    4    0    0
## [2,]    0    4    0
## [3,]    0    0   16
\end{verbatim}
\begin{alltt}
\hlcom{# vcov White}
\hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(X))} \hlopt{%*%} \hlkwd{t}\hlstd{(X)}  \hlopt{%*%} \hlstd{S_hat_white} \hlopt{%*%} \hlstd{X} \hlopt{%*%} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(X))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2]
## [1,]  0.5 -0.5
## [2,] -0.5  0.5
\end{verbatim}
\begin{alltt}
\hlcom{# vcov HC3}
\hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(X))} \hlopt{%*%} \hlkwd{t}\hlstd{(X)}  \hlopt{%*%} \hlstd{S_hat_HC3} \hlopt{%*%} \hlstd{X} \hlopt{%*%} \hlkwd{solve}\hlstd{(}\hlkwd{crossprod}\hlstd{(X))}
\end{alltt}
\begin{verbatim}
##      [,1] [,2]
## [1,]    2   -2
## [2,]   -2   18
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{solution}
\protect \hypertarget {soln:8.32}{}
\begin{solution}{{8.32}}

\end{solution}
\protect \hypertarget {soln:8.33}{}
\begin{solution}{{8.33}}
при гомоскедастичности $\hat{\mu}=\bar{y}$, при гетероскедастичности
\[
\hat{\mu}=\frac{\sum \tilde{x}_i \tilde{y}_i}{\sum \tilde{x}_i^2}=\frac{\sum i^2\cdot y_i}{\sum i^2}
\]
\end{solution}
\protect \hypertarget {soln:8.34}{}
\begin{solution}{{8.34}}
$\E(\hb|X)=\b$
\end{solution}
\protect \hypertarget {soln:8.35}{}
\begin{solution}{{8.35}}

\end{solution}
\protect \hypertarget {soln:8.36}{}
\begin{solution}{{8.36}}
Одинаковые.
\end{solution}
\protect \hypertarget {soln:8.37}{}
\begin{solution}{{8.37}}
\[
\hVar(\hb_1) = TSS \frac{1}{n_0}\frac{1}{n-2}
\]
\[
\hVar(\hb_1) = TSS_0\frac{n}{n_0} \frac{1}{n_0}\frac{1}{n-2}
\]
\end{solution}
