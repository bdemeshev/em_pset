\protect \hypertarget {soln:1.1}{}
\begin{solution}{{1.1}}
\begin{enumerate}
\item Верно: \[\sum_{i=1}^{n}(a_i-\bar{a}) = \sum_{i=1}^{n}a_i - n\cdot\bar{a} = \sum_{i=1}^{n}a_i - \sum_{i=1}^{n}a_i = 0\]
\item Верно: \[\sum_{i=1}^{n}(a_i-\bar{a})^2 = \sum_{i=1}^{n}(a_i-\bar{a})(a_i+\bar{a}) = \sum_{i=1}^{n}(a_i-\bar{a})a_i + \bar{a}\underbrace{\sum_{i=1}^{n}(a_i-\bar{a})}_{=0} = \sum_{i=1}^{n}(a_i-\bar{a})a_i \]

\item Верно: \[\sum_{i=1}^{n}(a_i-\bar{a})(b_i-\bar{b}) = \sum_{i=1}^{n}(a_i-\bar{a})b_i - \bar{b}\underbrace{\sum_{i=1}^{n}(a_i-\bar{a})}_{=0} = \sum_{i=1}^{n}(a_i-\bar{a})b_i \]
\item А вот это неверно! (следует из предыдущего пункта)
\item Верно
\item Верно
\end{enumerate}
\end{solution}
\protect \hypertarget {soln:1.2}{}
\begin{solution}{{1.2}}
\begin{enumerate}
\item \(\htheta = \sum y_i (1 + x_i) / \sum (1 + x_i)^2\)

Стандартная процедура МНК:
\[RSS = \sum \e_i^2 = \sum \left(y_i - \theta - \theta x_i\right)^2 \rightarrow \min \limits_\theta\]
\[\frac{\partial RSS}{\partial \theta} = 2 \sum \left(y_i - \theta - \theta x_i\right)(-1 - x_i) \]
\[\sum \left(y_i - \htheta - \htheta x_i\right)(-1 - x_i) = 0\]
\[\sum y_i (-1 - x_i) + \htheta \sum (-1 - x_i)^2 = 0 \]
\[\htheta = \frac{\sum y_i (1 + x_i)}{\sum (1 + x_i)^2} \]

\item \(\htheta = \sum \left(y_i (1 - x_i)\right) / \sum (1 - x_i)^2\)

\item \(\htheta = \left( \sum \ln (y_i / x_i) \right) / n \)

\item \(\htheta = \left( \sum (y_i - x_i) \right) / n \)

\item \(\htheta = \sum \left((y_i - 1) x_i\right) / \sum x_i^2\)

\item \(\htheta = \sum (y_i / x_i^2) / \sum (1 /x^3)\)

\item \(\htheta = \sum \left((y_i - z_i)(x_i - z_i) \right) / \sum \left(x_i - z_i\right)^2 \)

\end{enumerate}
\end{solution}
\protect \hypertarget {soln:1.3}{}
\begin{solution}{{1.3}}
Заметим, что $y_i + z_i = \underbrace{(\alpha + \gamma)}_{\mu} + \underbrace{(\beta+\delta)}_{\lambda}x_i + u_i$.

Если оценить данную модель при помощи МНК, получим как раз то, что нужно доказать.
\end{solution}
\protect \hypertarget {soln:1.4}{}
\begin{solution}{{1.4}}
\(\hat{\alpha} = 0, \ \hb = 1 \)
\end{solution}
\protect \hypertarget {soln:1.5}{}
\begin{solution}{{1.5}}
 % 1.5.
Рассмотрим регрессию суммы $(y_i + z_i)$ на саму себя. Естественно, в ней
\[
\widehat{y_i + z_i} = 0 + 1 \cdot (y_i + z_i).
\]

Отсюда получаем, что $\hat{\alpha} + \hat{\gamma} = 0$ и $\hb + \hat{\delta} = 1$.
\end{solution}
\protect \hypertarget {soln:1.6}{}
\begin{solution}{{1.6}}

Исходя из условия, нужно оценить методом МНК коэффициенты двух следующих моделей:
\[y_i = \alpha + \beta x_i + \e_i \]
\[y_i = \frac{\gamma}{2} + \frac{\delta}{2} x_i + \frac{1}{2} v_i \]

Заметим, что на минимизацию суммы квадратов остатков коэффициент \(1/2\) не влияет, следовательно:
\[\hat{\gamma} = 2\hat{\alpha}, \ \hat{\delta} = 2 \hb  \]

\end{solution}
\protect \hypertarget {soln:1.7}{}
\begin{solution}{{1.7}}
Выпишем задачу:
\[
\begin{cases}
RSS = \sum\limits_{i=1}^{n}(y_i - \beta_1x_i - \beta_2z_i)^2 \rightarrow \min\limits_{\beta_1, \beta_2}\\
\beta_1 + \beta_2 = 1
\end{cases}
\]

Можем превратить ее в задачу минимизации функции одного аргумента:
\[
RSS =  \sum\limits_{i=1}^{n}(y_i - x_i - \beta_2(z_i-x_i))^2 \rightarrow \min_{\beta_2}
\]

Выпишем условия первого порядка:
\[
\frac{\partial RSS}{\partial \beta_2} = \sum\limits_{i=1}^{n}2(y_i-x_i-\hb_2(z_i-x_i))(x_i-z_i)=0
\]

Отсюда:
\[
\sum\limits_{i=1}^{n}(y_i-x_i)(x_i-z_i) + \hb_2\sum\limits_{i=1}^{n}(z_i-x_i)^2 = 0 \Rightarrow \hb_2 = \frac{\sum\limits_{i=1}^n (y_i-x_i)(z_i-x_i)}{\sum\limits_{i=1}^n (z_i-x_i)^2}
\]

А $\beta_1$ найдется из соотношения $\hb_1+\hb_2 = 1$.

\end{solution}
\protect \hypertarget {soln:1.8}{}
\begin{solution}{{1.8}}
$\hb=\sum x_i y_i/\sum x_i^2$
\end{solution}
\protect \hypertarget {soln:1.9}{}
\begin{solution}{{1.9}}
Нужно решить задачу:
\[
RSS = \sum\limits_{i=1}^n(y_i-\beta)^2 \rightarrow \min\limits_{\beta}
\]

Условия первого порядка:
\[
\frac{\partial RSS}{\partial \beta} = \sum\limits_{i=1}^{n}2(y_i-\hb) = 0 \Rightarrow \sum\limits_{i=1}^n y_i-n\hb = 0
\]

Поэтому
\[
\hb = \frac{\sum_{i=1}^ny_i}{n} = \bar{y}
\]
\end{solution}
\protect \hypertarget {soln:1.10}{}
\begin{solution}{{1.10}}
$\hb_2=\sum (x_i-\bar{x})(y_i-\bar{y})/\sum(x_i-\bar{x})^2$, $\hb_1=\bar{y}-\hb_2\bar{x}$
\end{solution}
\protect \hypertarget {soln:1.11}{}
\begin{solution}{{1.11}}
Имеем следующую задачу:
\[
RSS = \sum\limits_i(y_i-1-\beta x_i)^2 \rightarrow \min\limits_{\beta}
\]

Откуда сразу все находим:
\[
\frac{\partial RSS}{\partial \beta} = \sum\limits_i2(y_i-1-\hb x_i)(-x_i) = 0 \Rightarrow \sum\limits_i (y_i-1-\hb x_i)x_i=0 \Rightarrow
\]
\[
\sum\limits_i x_iy_i-\sum\limits_ix_i - \hb\sum\limits_ix_i^2 = 0 \Rightarrow \hb = \frac{\sum_ix_i(y_i-1)}{\sum_ix_i^2}
\]
\end{solution}
\protect \hypertarget {soln:1.12}{}
\begin{solution}{{1.12}}
Обозначив вес первого слитка за \(\beta_1\), вес второго слитка за \(\beta_2\), а показания весов за \(y_i\), получим, что
\[y_1 = \beta_1 + \e_1, \ y_2 = \beta_2 + \e_2, \ y_3 = \beta_1 + \beta_2 + \e_3\]

Тогда
\[(300 - \beta_1)^2 + (200 - \beta_2)^2 + (400 - \beta_1 - \beta_2)^2 \rightarrow \min \limits_{\beta_1,\  \beta_2} \]
\[\hb_1 = \frac{800}{3}, \ \hb_2 = \frac{500}{3} \]
\end{solution}
\protect \hypertarget {soln:1.13}{}
\begin{solution}{{1.13}}
Можем воспользоваться готовой формулой для регрессии на константу:
\[
\hb = \bar{y} = \frac{10+10+3}{3} = \frac{23}{3}
\]

(можно решить задачу $2(10-\beta)^2 + (3-\beta)^2\rightarrow \min$)

\end{solution}
\protect \hypertarget {soln:1.14}{}
\begin{solution}{{1.14}}
Условие первого порядка $\int_0^1 -2x(f(x)-\hb x) \, dx =0$, получаем
\[
\hb = \frac{\int_0^1 x f(x)\, dx} {\int_0^1 x^2 \, dx}
\]

\(\hb = \left(\int \limits_0^1 f(x) x dx\right) / \left(\int \limits_0^1 x^2 dx\right)\)
\end{solution}
\protect \hypertarget {soln:1.15}{}
\begin{solution}{{1.15}}
\begin{enumerate}
\item Проявите воображение! Все зависит от данных. Например, может быть вот так:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{tikz}\hlstd{(}\hlstr{"../R_plots/aggregate_regression.tikz"}\hlstd{,} \hlkwc{standAlone} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlkwc{bareBones} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\hlstd{n} \hlkwb{<-} \hlnum{100}\hlstd{;}
\hlstd{s} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{4}\hlstd{),} \hlkwd{c}\hlstd{(n}\hlopt{/}\hlnum{2}\hlstd{, n}\hlopt{/}\hlnum{2}\hlstd{));}
\hlstd{x} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{1} \hlopt{+} \hlkwd{runif}\hlstd{(n}\hlopt{/}\hlnum{2}\hlstd{),} \hlkwd{runif}\hlstd{(n}\hlopt{/}\hlnum{2}\hlstd{));}
\hlstd{y} \hlkwb{<-} \hlnum{2} \hlopt{*} \hlstd{x} \hlopt{+} \hlstd{s} \hlopt{+} \hlkwd{rnorm}\hlstd{(n,} \hlkwc{sd} \hlstd{=} \hlnum{0.15}\hlstd{)}


\hlkwd{plot}\hlstd{(x, y,} \hlkwc{type} \hlstd{=} \hlstr{"n"}\hlstd{,} \hlkwc{frame} \hlstd{=} \hlstr{"FALSE"}\hlstd{)}
\hlkwd{points}\hlstd{(x[}\hlnum{1} \hlopt{:} \hlstd{(n}\hlopt{/}\hlnum{2}\hlstd{)], y[}\hlnum{1} \hlopt{:} \hlstd{(n}\hlopt{/}\hlnum{2}\hlstd{)],} \hlkwc{pch} \hlstd{=} \hlnum{21}\hlstd{,}
       \hlkwc{col} \hlstd{=} \hlstr{"black"}\hlstd{,} \hlkwc{bg} \hlstd{=} \hlstr{"ForestGreen"}\hlstd{,} \hlkwc{cex} \hlstd{=} \hlnum{2}\hlstd{)}
\hlkwd{points}\hlstd{(x[(n}\hlopt{/}\hlnum{2} \hlopt{+} \hlnum{1}\hlstd{)} \hlopt{:} \hlstd{n], y[(n}\hlopt{/}\hlnum{2} \hlopt{+} \hlnum{1}\hlstd{)} \hlopt{:} \hlstd{n],}
       \hlkwc{pch} \hlstd{=} \hlnum{21}\hlstd{,} \hlkwc{col} \hlstd{=} \hlstr{"black"}\hlstd{,} \hlkwc{bg} \hlstd{=} \hlstr{"SkyBlue"}\hlstd{,} \hlkwc{cex} \hlstd{=} \hlnum{2}\hlstd{)}

\hlstd{modelV1} \hlkwb{<-} \hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{x} \hlopt{+} \hlstd{s)}
\hlcom{# модели по 1:100 и 101:200 в отдельности}
\hlkwd{abline}\hlstd{(}\hlkwd{coef}\hlstd{(modelV1)[}\hlnum{1}\hlstd{],} \hlkwd{coef}\hlstd{(modelV1)[}\hlnum{2}\hlstd{],} \hlkwc{lwd} \hlstd{=} \hlnum{3}\hlstd{)}
\hlkwd{abline}\hlstd{(}\hlkwd{coef}\hlstd{(modelV1)[}\hlnum{1}\hlstd{]} \hlopt{+} \hlnum{4} \hlopt{*} \hlkwd{coef}\hlstd{(modelV1)[}\hlnum{3}\hlstd{],}
       \hlkwd{coef}\hlstd{(modelV1)[}\hlnum{2}\hlstd{],} \hlkwc{lwd} \hlstd{=} \hlnum{3}\hlstd{)}
\hlstd{modelV2} \hlkwb{<-} \hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{x)}
\hlcom{# общая модель}
\hlkwd{abline}\hlstd{(modelV2,} \hlkwc{lwd} \hlstd{=} \hlnum{2}\hlstd{,} \hlkwc{col} \hlstd{=} \hlstr{"red"}\hlstd{)}
\hlkwd{invisible}\hlstd{(}\hlkwd{dev.off}\hlstd{())}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{minipage}{0.6\textwidth}
\begin{center}
\begin{tikzpicture}[scale = 0.025]
\input{R_plots/aggregate_regression.tikz}
\end{tikzpicture}
\end{center}
\end{minipage}


\item Так тоже бывает:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{tikz}\hlstd{(}\hlstr{"../R_plots/aggregate_regression_b.tikz"}\hlstd{,} \hlkwc{standAlone} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlkwc{bareBones} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\hlstd{n} \hlkwb{<-} \hlnum{100}\hlstd{;}
\hlstd{s} \hlkwb{<-} \hlkwd{rep}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{4}\hlstd{),} \hlkwd{c}\hlstd{(n}\hlopt{/}\hlnum{2}\hlstd{, n}\hlopt{/}\hlnum{2}\hlstd{));}
\hlstd{x} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{runif}\hlstd{(n}\hlopt{/}\hlnum{2}\hlstd{),} \hlnum{1} \hlopt{+} \hlkwd{runif}\hlstd{(n}\hlopt{/}\hlnum{2}\hlstd{));}
\hlstd{y} \hlkwb{<-} \hlopt{-}\hlnum{2} \hlopt{*} \hlstd{x} \hlopt{+} \hlstd{s} \hlopt{+} \hlkwd{rnorm}\hlstd{(n,} \hlkwc{sd} \hlstd{=} \hlnum{0.15}\hlstd{)}

\hlkwd{plot}\hlstd{(x, y,} \hlkwc{type} \hlstd{=} \hlstr{"n"}\hlstd{,} \hlkwc{frame} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\hlkwd{points}\hlstd{(x[}\hlnum{1} \hlopt{:} \hlstd{(n}\hlopt{/}\hlnum{2}\hlstd{)], y[}\hlnum{1} \hlopt{:} \hlstd{(n}\hlopt{/}\hlnum{2}\hlstd{)],} \hlkwc{pch} \hlstd{=} \hlnum{21}\hlstd{,}
       \hlkwc{col} \hlstd{=} \hlstr{"black"}\hlstd{,} \hlkwc{bg} \hlstd{=} \hlstr{"ForestGreen"}\hlstd{,} \hlkwc{cex} \hlstd{=} \hlnum{2}\hlstd{)}
\hlkwd{points}\hlstd{(x[(n}\hlopt{/}\hlnum{2} \hlopt{+} \hlnum{1}\hlstd{)} \hlopt{:} \hlstd{n], y[(n}\hlopt{/}\hlnum{2} \hlopt{+} \hlnum{1}\hlstd{)} \hlopt{:} \hlstd{n],} \hlkwc{pch} \hlstd{=} \hlnum{21}\hlstd{,}
       \hlkwc{col} \hlstd{=} \hlstr{"black"}\hlstd{,} \hlkwc{bg} \hlstd{=} \hlstr{"SkyBlue"}\hlstd{,} \hlkwc{cex} \hlstd{=} \hlnum{2}\hlstd{)}

\hlstd{modelV1} \hlkwb{<-} \hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{x} \hlopt{+} \hlstd{s)}
\hlcom{# модели по 1:100 и 101:200 в отдельности}
\hlkwd{abline}\hlstd{(}\hlkwd{coef}\hlstd{(modelV1)[}\hlnum{1}\hlstd{],} \hlkwd{coef}\hlstd{(modelV1)[}\hlnum{2}\hlstd{],} \hlkwc{lwd} \hlstd{=} \hlnum{3}\hlstd{)}
\hlkwd{abline}\hlstd{(}\hlkwd{coef}\hlstd{(modelV1)[}\hlnum{1}\hlstd{]} \hlopt{+} \hlnum{4} \hlopt{*} \hlkwd{coef}\hlstd{(modelV1)[}\hlnum{3}\hlstd{],} \hlkwd{coef}\hlstd{(modelV1)[}\hlnum{2}\hlstd{],} \hlkwc{lwd} \hlstd{=} \hlnum{3}\hlstd{)}
\hlstd{modelV2} \hlkwb{<-} \hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{x)}
\hlcom{# общая модель}
\hlkwd{abline}\hlstd{(modelV2,} \hlkwc{lwd} \hlstd{=} \hlnum{2}\hlstd{,} \hlkwc{col} \hlstd{=} \hlstr{"red"}\hlstd{)}
\hlkwd{invisible}\hlstd{(}\hlkwd{dev.off}\hlstd{())}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{minipage}{0.6\textwidth}
\begin{center}
\begin{tikzpicture}[scale = 0.025]
\input{R_plots/aggregate_regression_b.tikz}
\end{tikzpicture}
\end{center}
\end{minipage}

\item А вот так не бывает!
\end{enumerate}
\end{solution}
\protect \hypertarget {soln:1.16}{}
\begin{solution}{{1.16}}
Нет. Коэффициенты можно интерпретировать только «при прочих равных», т.е. при равных $x$. Из-за разных $x$ может оказаться, что у мужчин $\bar{y}$ меньше, чем $\bar{y}$ для женщин.
\end{solution}
\protect \hypertarget {soln:1.17}{}
\begin{solution}{{1.17}}
Модель можно представить в линейном виде, когда неизвестные параметры входят в нее линейно.
\begin{enumerate}
\item Обозначим $z_i = 1/x_i$, и готово.
\item Возьмем логарифм от обеих частей\ldots
\item Вычтем единицу из обеих частей и снова логарифм\ldots
\item Перевернем обе части уравнения, вычтем единицу и прологарифмируем\ldots
\item Вместо $x_i$ возьмем $e^{x_i}$ и прологарифмируем\ldots
\item Вместо $\beta_1$ возьмем $e^{\beta_1}$ и прологарифмируем\ldots
\end{enumerate}
\end{solution}
\protect \hypertarget {soln:1.18}{}
\begin{solution}{{1.18}}
Пусть \(\bar{y}_m\) — среднее значение \(y\) по выборке для мужчин, \(\bar{y}_f\) — среднее значение \(y\) по выборке для женщин. Тогда

\begin{enumerate}
\item \(\hy = \bar{y}_m + \left( \bar{y}_m -  \bar{y}_f \right) \cdot 1_f \)

Оцениваемая модель:
\[y_i = \beta_0 + \beta_1 1_{f, i} + \e_i \]

Стандартная процедура МНК:
\[RSS = \sum \e_i^2 = \sum \left(y_i - \beta_0 - \beta_1 \cdot 1_f \right)^2 \rightarrow \min \limits_{\beta_0, \beta_1}\]
\[\frac{\partial RSS}{\partial \beta_0} = 2 \sum \left(y_i - \beta_0 - \beta_1 x_i\right)(-1) \]
\[\frac{\partial RSS}{\partial \beta_1} = 2 \sum \left(y_i - \beta_0 - \beta_1 x_i\right)(-1_f) \]

Условия первого порядка:
\[\sum \left(y_i - \hb_0 - \hb_1 \cdot 1_f\right) = 0\]
\[\sum \left(\left(y_i - \hb_0 - \hb_1 \cdot 1_f\right)\cdot 1_f\right)= 0\]

Осталось немного поработать с оператором суммирования. Обозначим за \(k\) — число женщин в нашей выборке объемом \(n\), мужчин тогда будет \(n - k\). Тогда
\[\sum \left(y_i - \hb_0 - \hb_1 \cdot 1_f\right) = m \bar{y}_f + (n - m) \bar{y}_m - n \hb_0 - m \hb_1 = 0\]
\[\sum \left(\left(y_i - \hb_0 - \hb_1 \cdot 1_f\right)\cdot 1_f\right) = m    \bar{y}_f - m \hb_0 - m \hb_1 =0\]

Отсюда легко ищутся оценки коэффициентов:
\[\hb_0 = \bar{y}_m, \ \hb_1 = \bar{y}_f - \bar{y}_m  \]

Следовательно:
\[\hy = \bar{y}_m + \left( \bar{y}_f -  \bar{y}_m \right) \cdot 1_f \]

\item \(\hy = \bar{y}_f + \left( \bar{y}_m -  \bar{y}_f \right) \cdot 1_m \)

\item \(\hy = \bar{y}_m \cdot 1_m + \bar{y}_f \cdot 1_f \)

\item Условия первого порядка линейного зависимы — мультиколлинеарность. МНК здесь неприменим.
\end{enumerate}
\end{solution}
\protect \hypertarget {soln:1.19}{}
\begin{solution}{{1.19}}
Если сложить попарно $m_i$ и $f_i$, то в сумме всегда выйдет единица. А оценки, полученные при помощи метода наименьших квадратов линейны по объясняемой переменной, то есть оценки коэффициентов модели $m_i+f_i \sim \dots$ это суммы соответствующих оценок из двух разных моделей. Но они должны получиться равными 1 и 0 соответствнно (так как зависимая переменная — вектор из единиц). Поэтому $\hb_1 + \hat{\gamma}_1 = 1$, $\hb_2 + \hat{\gamma}_2 = 0$.
\end{solution}
\protect \hypertarget {soln:1.20}{}
\begin{solution}{{1.20}}
Все оценки коэффициентов увеличатся в 100 раз.
\end{solution}
\protect \hypertarget {soln:1.21}{}
\begin{solution}{{1.21}}
Да, возможно:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{tikz}\hlstd{(}\hlstr{"../R_plots/with_and_without_c.tikz"}\hlstd{,} \hlkwc{standAlone} \hlstd{=} \hlnum{FALSE}\hlstd{,} \hlkwc{bareBones} \hlstd{=} \hlnum{TRUE}\hlstd{)}
\hlstd{x} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlkwd{rnorm}\hlstd{(n,} \hlkwc{mean} \hlstd{=} \hlnum{4}\hlstd{,} \hlkwc{sd} \hlstd{=} \hlnum{2}\hlstd{))}
\hlstd{y} \hlkwb{<-} \hlstd{x} \hlopt{-} \hlnum{7} \hlopt{+} \hlkwd{runif}\hlstd{(n,} \hlkwc{min} \hlstd{=} \hlopt{-}\hlnum{1}\hlstd{,} \hlkwc{max} \hlstd{=} \hlnum{1}\hlstd{)}

\hlkwd{plot}\hlstd{(x,y,} \hlkwc{pch} \hlstd{=} \hlnum{21}\hlstd{,} \hlkwc{bg} \hlstd{=} \hlstr{"ForestGreen"}\hlstd{,}
     \hlkwc{col} \hlstd{=} \hlstr{"black"}\hlstd{,} \hlkwc{xlim} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,}\hlnum{10}\hlstd{),} \hlkwc{ylim} \hlstd{=} \hlkwd{c}\hlstd{(}\hlopt{-}\hlnum{5}\hlstd{,}\hlnum{3}\hlstd{))}
\hlkwd{abline}\hlstd{(}\hlkwd{coef}\hlstd{(}\hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{x))[}\hlnum{1}\hlstd{],} \hlkwd{coef}\hlstd{(}\hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{x))[}\hlnum{2}\hlstd{],} \hlkwc{lwd} \hlstd{=} \hlnum{2}\hlstd{)}
\hlkwd{abline}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwd{coef}\hlstd{(}\hlkwd{lm}\hlstd{(y} \hlopt{~} \hlnum{0} \hlopt{+} \hlstd{x))[}\hlnum{1}\hlstd{] ,} \hlkwc{lwd} \hlstd{=} \hlnum{2}\hlstd{)}
\hlstd{labels} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"With intercept"}\hlstd{,} \hlstr{"Without intercept"}\hlstd{)}
\hlkwd{text}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{7.5}\hlstd{,} \hlnum{1.5}\hlstd{),} \hlkwd{c}\hlstd{(}\hlnum{2}\hlstd{,} \hlnum{0.4}\hlstd{), labels)}
\hlkwd{coef}\hlstd{(}\hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{x))}
\end{alltt}
\begin{verbatim}
## (Intercept)           x
##  -6.8304265   0.9634434
\end{verbatim}
\begin{alltt}
\hlkwd{coef}\hlstd{(}\hlkwd{lm}\hlstd{(y} \hlopt{~} \hlnum{0} \hlopt{+} \hlstd{x))}
\end{alltt}
\begin{verbatim}
##          x
## -0.3487678
\end{verbatim}
\begin{alltt}
\hlkwd{invisible}\hlstd{(}\hlkwd{dev.off}\hlstd{())}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{minipage}{0.6\textwidth}
\begin{center}
\begin{tikzpicture}[scale = 0.025]
\input{R_plots/with_and_without_c.tikz}
\end{tikzpicture}
\end{center}
\end{minipage}
\end{solution}
\protect \hypertarget {soln:1.22}{}
\begin{solution}{{1.22}}
Так как \(\hy = \hb = \bar{y} \), то \(R^2 = 0\).
\end{solution}
\protect \hypertarget {soln:1.23}{}
\begin{solution}{{1.23}}
Вспомним формулу для  $TSS$:
\[
TSS = \sum\limits_{i=1}^n (y_i-\bar{y})^2
\]
Так как значения $y$ остались теми же, $TSS_1 = TSS_2$.

\[
RSS = \sum\limits_{i=1}^n (y_i-\hy_i)^2 \hspace{2cm} ESS = \sum\limits_{i=1}^n (\hy_i-\bar{y})^2
\]

Добавление еще одного регрессора не уменьшит точность оценки, то есть $RSS_2\leqslant RSS_1$, $ESS_2 \geqslant ESS_1$.

Соответственно, коэффициент $R^2 = ESS/TSS$ не уменьшится, то есть $R^2_2 \geqslant R^2_1$.

\end{solution}
\protect \hypertarget {soln:1.24}{}
\begin{solution}{{1.24}}
Интересный результат: смена знака коэффициента перед \(x\) при удалении переменной \(z\) не может произойти, если абсолютное значение \(t\)-статистики коэффициента перед \(z\) в регрессии с регрессорами \(x\) и \(z\) меньше абсолютного значения \(t\)-статистики коэффициента перед \(x\).\footnote{Leamer, E. E., 1975. A Result on the Sign of Restricted Least-Squares Estimates. Journal of Econometrics, 3, 387--390.}

Пример такого набора данных:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{y} \hlkwb{<-} \hlnum{1}\hlopt{:}\hlnum{50}
\hlstd{z} \hlkwb{<-} \hlstd{y} \hlopt{+} \hlkwd{rnorm}\hlstd{(}\hlkwd{length}\hlstd{(y),} \hlnum{0}\hlstd{,} \hlnum{0.01}\hlstd{)}
\hlstd{x} \hlkwb{<-} \hlstd{y} \hlopt{+} \hlkwd{rnorm}\hlstd{(}\hlkwd{length}\hlstd{(y),} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{)}
\hlkwd{coef}\hlstd{(}\hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{x))}
\end{alltt}
\begin{verbatim}
## (Intercept)           x
## -0.04267503  1.00640588
\end{verbatim}
\begin{alltt}
\hlkwd{coef}\hlstd{(}\hlkwd{lm}\hlstd{(y} \hlopt{~} \hlstd{x} \hlopt{+} \hlstd{z))}
\end{alltt}
\begin{verbatim}
##  (Intercept)            x            z
## -0.003206347 -0.001442806  1.001455355
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{solution}
\protect \hypertarget {soln:1.25}{}
\begin{solution}{{1.25}}
$y_i^*=7+3(y_i-\bar{y})/s_y$
% эта задача не использует понятия вероятностей, хотя близка. Пусть будет в невероятностной секции.
Нужно вспомнить свойства математического ожидания и дисперсии и провести следующие преобразования:
\[
\tilde{y}_i = \frac{y_i-\bar{y}}{s_y} \Rightarrow \E[\tilde{y}] = 0, \hspace{2mm} \Var(\tilde{y}) = 1
\]
\[
y^*_i = \tilde{y}_i\cdot3 + 7 \Rightarrow \E[y^*] = 7, \hspace{2mm} \Var(y^*) = 9
\]
\end{solution}
\protect \hypertarget {soln:1.26}{}
\begin{solution}{{1.26}}
  $R^2 = -3 \cdot \frac{-1}{12}=\frac{1}{4}$. Выборочная корреляция равна $-1/2$, так как коэффициенты отрицательные.
\end{solution}
